{
  "docusaurus-plugin-content-docs": {
    "default": {
      "loadedVersions": [
        {
          "versionName": "current",
          "versionLabel": "Next",
          "versionPath": "/docs",
          "tagsPath": "/docs/tags",
          "versionEditUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs",
          "versionEditUrlLocalized": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/i18n/en/docusaurus-plugin-content-docs/current",
          "versionBanner": null,
          "versionBadge": false,
          "versionClassName": "docs-version-current",
          "isLast": true,
          "routePriority": -1,
          "sidebarFilePath": "/Users/dmcknight/Sites/prestodb-v2/prestodb.io/sidebars.js",
          "contentPath": "/Users/dmcknight/Sites/prestodb-v2/prestodb.io/docs",
          "contentPathLocalized": "/Users/dmcknight/Sites/prestodb-v2/prestodb.io/i18n/en/docusaurus-plugin-content-docs/current",
          "docs": [
            {
              "unversionedId": "intro",
              "id": "intro",
              "title": "Tutorial Intro",
              "description": "Let's discover Docusaurus in less than 5 minutes.",
              "source": "@site/docs/intro.md",
              "sourceDirName": ".",
              "slug": "/intro",
              "permalink": "/docs/intro",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/intro.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "next": {
                "title": "Create a Page",
                "permalink": "/docs/tutorial-basics/create-a-page"
              }
            },
            {
              "unversionedId": "tutorial-basics/congratulations",
              "id": "tutorial-basics/congratulations",
              "title": "Congratulations!",
              "description": "You have just learned the basics of Docusaurus and made some changes to the initial template.",
              "source": "@site/docs/tutorial-basics/congratulations.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/congratulations",
              "permalink": "/docs/tutorial-basics/congratulations",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/congratulations.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 6,
              "frontMatter": {
                "sidebar_position": 6
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Deploy your site",
                "permalink": "/docs/tutorial-basics/deploy-your-site"
              },
              "next": {
                "title": "Manage Docs Versions",
                "permalink": "/docs/tutorial-extras/manage-docs-versions"
              }
            },
            {
              "unversionedId": "tutorial-basics/create-a-blog-post",
              "id": "tutorial-basics/create-a-blog-post",
              "title": "Create a Blog Post",
              "description": "Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed...",
              "source": "@site/docs/tutorial-basics/create-a-blog-post.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/create-a-blog-post",
              "permalink": "/docs/tutorial-basics/create-a-blog-post",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/create-a-blog-post.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 3,
              "frontMatter": {
                "sidebar_position": 3
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create a Document",
                "permalink": "/docs/tutorial-basics/create-a-document"
              },
              "next": {
                "title": "Markdown Features",
                "permalink": "/docs/tutorial-basics/markdown-features"
              }
            },
            {
              "unversionedId": "tutorial-basics/create-a-document",
              "id": "tutorial-basics/create-a-document",
              "title": "Create a Document",
              "description": "Documents are groups of pages connected through:",
              "source": "@site/docs/tutorial-basics/create-a-document.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/create-a-document",
              "permalink": "/docs/tutorial-basics/create-a-document",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/create-a-document.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create a Page",
                "permalink": "/docs/tutorial-basics/create-a-page"
              },
              "next": {
                "title": "Create a Blog Post",
                "permalink": "/docs/tutorial-basics/create-a-blog-post"
              }
            },
            {
              "unversionedId": "tutorial-basics/create-a-page",
              "id": "tutorial-basics/create-a-page",
              "title": "Create a Page",
              "description": "Add Markdown or React files to src/pages to create a standalone page:",
              "source": "@site/docs/tutorial-basics/create-a-page.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/create-a-page",
              "permalink": "/docs/tutorial-basics/create-a-page",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/create-a-page.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Tutorial Intro",
                "permalink": "/docs/intro"
              },
              "next": {
                "title": "Create a Document",
                "permalink": "/docs/tutorial-basics/create-a-document"
              }
            },
            {
              "unversionedId": "tutorial-basics/deploy-your-site",
              "id": "tutorial-basics/deploy-your-site",
              "title": "Deploy your site",
              "description": "Docusaurus is a static-site-generator (also called Jamstack).",
              "source": "@site/docs/tutorial-basics/deploy-your-site.md",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/deploy-your-site",
              "permalink": "/docs/tutorial-basics/deploy-your-site",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/deploy-your-site.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 5,
              "frontMatter": {
                "sidebar_position": 5
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Markdown Features",
                "permalink": "/docs/tutorial-basics/markdown-features"
              },
              "next": {
                "title": "Congratulations!",
                "permalink": "/docs/tutorial-basics/congratulations"
              }
            },
            {
              "unversionedId": "tutorial-basics/markdown-features",
              "id": "tutorial-basics/markdown-features",
              "title": "Markdown Features",
              "description": "Docusaurus supports Markdown and a few additional features.",
              "source": "@site/docs/tutorial-basics/markdown-features.mdx",
              "sourceDirName": "tutorial-basics",
              "slug": "/tutorial-basics/markdown-features",
              "permalink": "/docs/tutorial-basics/markdown-features",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-basics/markdown-features.mdx",
              "tags": [],
              "version": "current",
              "sidebarPosition": 4,
              "frontMatter": {
                "sidebar_position": 4
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Create a Blog Post",
                "permalink": "/docs/tutorial-basics/create-a-blog-post"
              },
              "next": {
                "title": "Deploy your site",
                "permalink": "/docs/tutorial-basics/deploy-your-site"
              }
            },
            {
              "unversionedId": "tutorial-extras/manage-docs-versions",
              "id": "tutorial-extras/manage-docs-versions",
              "title": "Manage Docs Versions",
              "description": "Docusaurus can manage multiple versions of your docs.",
              "source": "@site/docs/tutorial-extras/manage-docs-versions.md",
              "sourceDirName": "tutorial-extras",
              "slug": "/tutorial-extras/manage-docs-versions",
              "permalink": "/docs/tutorial-extras/manage-docs-versions",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-extras/manage-docs-versions.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 1,
              "frontMatter": {
                "sidebar_position": 1
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Congratulations!",
                "permalink": "/docs/tutorial-basics/congratulations"
              },
              "next": {
                "title": "Translate your site",
                "permalink": "/docs/tutorial-extras/translate-your-site"
              }
            },
            {
              "unversionedId": "tutorial-extras/translate-your-site",
              "id": "tutorial-extras/translate-your-site",
              "title": "Translate your site",
              "description": "Let's translate docs/intro.md to French.",
              "source": "@site/docs/tutorial-extras/translate-your-site.md",
              "sourceDirName": "tutorial-extras",
              "slug": "/tutorial-extras/translate-your-site",
              "permalink": "/docs/tutorial-extras/translate-your-site",
              "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/tutorial-extras/translate-your-site.md",
              "tags": [],
              "version": "current",
              "sidebarPosition": 2,
              "frontMatter": {
                "sidebar_position": 2
              },
              "sidebar": "tutorialSidebar",
              "previous": {
                "title": "Manage Docs Versions",
                "permalink": "/docs/tutorial-extras/manage-docs-versions"
              }
            }
          ],
          "sidebars": {
            "tutorialSidebar": [
              {
                "type": "doc",
                "id": "intro"
              },
              {
                "type": "category",
                "label": "Tutorial - Basics",
                "collapsible": true,
                "collapsed": true,
                "items": [
                  {
                    "type": "doc",
                    "id": "tutorial-basics/create-a-page"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/create-a-document"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/create-a-blog-post"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/markdown-features"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/deploy-your-site"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-basics/congratulations"
                  }
                ]
              },
              {
                "type": "category",
                "label": "Tutorial - Extras",
                "collapsible": true,
                "collapsed": true,
                "items": [
                  {
                    "type": "doc",
                    "id": "tutorial-extras/manage-docs-versions"
                  },
                  {
                    "type": "doc",
                    "id": "tutorial-extras/translate-your-site"
                  }
                ]
              }
            ]
          },
          "mainDocId": "intro",
          "categoryGeneratedIndices": []
        }
      ]
    }
  },
  "docusaurus-plugin-content-blog": {
    "default": {
      "blogSidebarTitle": "Recent posts",
      "blogPosts": [
        {
          "id": "/2022/04/15/disggregated-coordinator",
          "metadata": {
            "permalink": "/blog/2022/04/15/disggregated-coordinator",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-04-15-disggregated-coordinator.md",
            "source": "@site/blog/2022-04-15-disggregated-coordinator.md",
            "title": "Disaggregated Coordinator",
            "description": "Meta: Swapnil Tailor, Tim Meehan, Vaishnavi Batni, Abhisek Saikia, Neerad Somanchi",
            "date": "2022-04-15T00:00:00.000Z",
            "formattedDate": "April 15, 2022",
            "tags": [],
            "readingTime": 3.74,
            "truncated": true,
            "authors": [
              {
                "name": "Swapnil Tailor",
                "url": "https://www.linkedin.com/in/swapnil-tailor-23077b6/"
              }
            ],
            "frontMatter": {
              "title": "Disaggregated Coordinator",
              "author": "Swapnil Tailor",
              "authorURL": "https://www.linkedin.com/in/swapnil-tailor-23077b6/"
            },
            "nextItem": {
              "title": "Native Delta Lake Connector for Presto",
              "permalink": "/blog/2022/03/15/native-delta-lake-connector-for-presto"
            }
          },
          "content": "**Meta**: Swapnil Tailor, Tim Meehan, Vaishnavi Batni, Abhisek Saikia, Neerad Somanchi\n\n## Overview\nPresto's architecture originally only supported a single coordinator and a pool of workers. This has worked well for many years but created some challenges.\n\n- With a single coordinator, the cluster can scale up to a certain number of workers reliably. A large worker pool running complex, multi-stage queries can overwhelm an inadequately provisioned coordinator, requiring upgraded hardware to support the increase in worker load.\n- A single coordinator is a single point of failure for the Presto cluster.\n\nTo overcome these challenges, we came up with a new design with a disaggregated coordinator that allows the coordinator to be horizontally scaled out across a single pool of workers.\n\n<!--truncate-->\n\n## Architecture\n![Architecture](/img/blog/2022-04-15-disaggregated-coordinator/disaggregated-coordinator-architecture.png)\n\nA disaggregated coordinator setup supports a pool of coordinators with the help of a new component, the resource manager.\n\n\n#### Resource Manager\nThe resource manager aggregates data from all coordinators and workers, and constructs a global view of the cluster. Clusters support multiple resource managers, each acting as a primary. The discovery service runs on each resource manager.\nThe resource manager is not in the critical path for the query.  Rather, it is a complementary process that can survive momentary unavailability.\n\n#### Coordinator\nThe coordinator sends heartbeats at regular intervals to all the resource managers. These heartbeats contain information about the queries handled by the coordinator, which the resource managers use to refresh their global view of the cluster. The coordinator fetches aggregated resource group information periodically from the resource manager.\n\n#### Worker\nEach worker sends regular heartbeats with memory and cpu utilization to the resource managers. The resource managers track these metrics for the worker pool.\n\n## Query Execution Flow\n![QueryExecutionFlow](/img/blog/2022-04-15-disaggregated-coordinator/query-execution-flow.png)\n\nWith the introduction of a resource manager, the query execution flow looks slightly different.\n\n- A query is submitted to one of the coordinators in the cluster.\n- The coordinator prepares the query for execution by parsing, analyzing and assigning it to a given resource group. \n- A heartbeat is sent to each resource manager when the query is created by the coordinator.\n- The coordinator polls the resource manager at regular intervals to fetch cluster level resource group information.\n- The coordinator polls the resource manager to get active worker information. This information is used for query scheduling.\n- The rest of the query execution remains the same.\n\n\n## Memory Management\n![MemoryManagement](/img/blog/2022-04-15-disaggregated-coordinator/memory-management.png)\n\nThe resource manager needs up to date information about memory and cpu utilization of the worker pool for resource group queuing. Currently, this information is periodically collected by the coordinator. In the disaggregated coordinator setup, resource managers receive query-level statistics from coordinator heartbeats, and memory pool information from worker heartbeats. This information is periodically polled by the coordinator to help make local decisions (i.e. queue/run a query, kill a query when the cluster is low on memory).\n\n## Resource Management\n![ResourceManagement](/img/blog/2022-04-15-disaggregated-coordinator/resource-management.png)\n\nThe resource managers runs in multi-master mode. To support that, coordinators post query updates to all resource managers. The resource manager aggregates this information. The coordinator polls a resource manager to fetch up to date information about resource group usage in the cluster.\n\n## Resource Group Consistency Model\n\nResource groups in a disaggregated coordinator setup are eventually consistent.  While this may lead to over-admission in certain scenarios, in practice this is mitigated by gating the resource group to only allow queries to run when certain freshness guarantees have been met (as opposed to the previous logic of checking every millisecond).  This may mean if the cluster’s resource managers are down, then queries may be queued in the coordinator’s resource groups.  This is to ensure coordinators don’t over-admit queries in the face of resource manager unavailability.\n\nMore details about flags can be found [here](https://prestodb.io/docs/current/installation/deployment.html#config-properties) which can help tune the cluster’s resource groups to the desired consistency.\n\n## Discovery Service\n![discoveryservice](/img/blog/2022-04-15-disaggregated-coordinator/discovery-service.png)\n\nAn embedded version of the discovery server runs on resource managers in distributed mode. Discovery servers stay in sync by passing updates they receive to other discovery servers in the cluster.\n\n## Configuration\n\nMinimal configuration to enable a disaggregated coordinator cluster can be found in [here](https://prestodb.io/docs/current/installation/deployment.html#config-properties).\n\nNo changes needed in [jvm.config](https://prestodb.io/docs/current/installation/deployment.html#jvm-config) and [node.properties](https://prestodb.io/docs/current/installation/deployment.html#node-properties).\n\nRecommended release version to use disaggregated coordinator in production: [0.266](https://prestodb.io/docs/current/release/release-0.266.html#release-release-0-266--page-root)\n\n## Disaggregated Coordinator Talk\n\nThere were lightning talks about the Disaggregated Coordinator at past PrestoCons. Videos and slides can be accessed using the following links:\n\n1. Lightning Talk in 2020 Prestocon: [video](https://www.youtube.com/watch?v=slwPm-mROZ0)\n\n2. Lightning Talk in 2021 PrestoCon about Production Rollout: [video and slides](https://prestodb.io/prestocon_2021.html#Disaggregated_Coordinator_Production_Rollout)"
        },
        {
          "id": "/2022/03/15/native-delta-lake-connector-for-presto",
          "metadata": {
            "permalink": "/blog/2022/03/15/native-delta-lake-connector-for-presto",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-03-15-native-delta-lake-connector-for-presto.md",
            "source": "@site/blog/2022-03-15-native-delta-lake-connector-for-presto.md",
            "title": "Native Delta Lake Connector for Presto",
            "description": "Co-authors\\",
            "date": "2022-03-15T00:00:00.000Z",
            "formattedDate": "March 15, 2022",
            "tags": [],
            "readingTime": 7.415,
            "truncated": true,
            "authors": [
              {
                "name": "Rohan Pednekar",
                "url": "https://www.linkedin.com/in/pednero"
              }
            ],
            "frontMatter": {
              "author": "Rohan Pednekar",
              "authorURL": "https://www.linkedin.com/in/pednero",
              "title": "Native Delta Lake Connector for Presto"
            },
            "prevItem": {
              "title": "Disaggregated Coordinator",
              "permalink": "/blog/2022/04/15/disggregated-coordinator"
            },
            "nextItem": {
              "title": "Avoid Data Silos in Presto in Meta: the journey from Raptor to RaptorX",
              "permalink": "/blog/2022/01/28/avoid-data-silos-in-presto-in-meta"
            }
          },
          "content": "**Co-authors**\\\n[Denny Lee](https://www.linkedin.com/in/dennyglee/), Sr. Staff Developer Advocate at Databricks\n\nThis is a joint publication by the PrestoDB and Delta Lake communities\n\n![Native Delta Lake Connector for Presto](/img/blog/2022-03-15-native-delta-lake-connector-for-presto/banner.png)\n\nDue to the popularity of both the [PrestoDB](https://prestodb.io) and [Delta Lake](https://delta.io) projects (more on this below), in [early 2020](https://databricks.com/blog/2020/01/29/query-delta-lake-tables-presto-athena-improved-operations-concurrency-merge-performance.html) the Delta Lake community announced that one could query Delta tables from PrestoDB. While popular, this method entailed the use of a manifest file where a Delta table is registered in Hive metastore as symlink table type. While this approach may satisfy batch processing requirements, it did not satisfy frequent processing or streaming requirements.  Therefore, we are happy to announce the release of the native Delta Lake connector for PrestoDB ([source code](https://github.com/prestodb/presto/tree/master/presto-delta) | [docs](https://prestodb.io/docs/current/connector/deltalake.html)).\n\n<!--truncate-->\n\n[![PrestoCon2021 Seesion](/img/blog/2022-03-15-native-delta-lake-connector-for-presto/prestocon2021-session.png)](https://prestodb.io/prestocon_2021.html#Delta_Lake_Connector_for_Presto)\n\nBefore we dive into the connector, let’s provide some background for those not intimately  familiar with these projects.\n\n## Presto - Open Source SQL Engine for the Data Lakehouse\n\nPresto is an open source, MPP, distributed SQL engine widely recognized for its low latency queries, high concurrency and native ability to query data lakes and many other data sources. Presto is a compute layer of the disaggregated stack which allows users to scale storage and compute separately and as cloud adoption has increased exponentially, Presto has become the de facto query engine for interactive analytical queries.\n\n### Why Does It Matter?\n\nAs cloud data warehouses become more cost prohibitive, and data mesh approach is not performant enough, more and more workloads are migrating to the data lake. If all your data is going to end up in cloud native storage like Amazon S3, ADLS Gen2, GCS, etc. then the most optimized and efficient data strategy is to leverage [an open data lakehouse analytics stack](https://ahana.io/whitepaper-unlocking-the-business-value-of-the-data-lake/), which provides much more flexibility with no lock-in. \n\n## Delta Lake - Open Source Data Lake Storage Standards\n\nDelta Lake is an open-source project built for data [lakehouses](http://cidrdb.org/cidr2021/papers/cidr2021_paper17.pdf) with compute engines including Spark, PrestoDB, Flink, and Hive and APIs for Scala, Java, Rust, Ruby, and Python. Delta Lake is an ACID table storage layer over cloud object stores that Databricks started providing to customers in 2017 and open sourced in 2019. The core idea of Delta Lake is simple: it maintains information about which objects are part of a Delta table in an ACID manner, using a write-ahead log that is itself stored in the cloud object store. The objects themselves are encoded in Parquet, making it easy to write connectors from engines that can already process Parquet. This design allows clients to update multiple objects at once, replace a subset of the objects with another, etc., in a serializable manner while still achieving high parallel read and write performance from the objects themselves (similar to raw Parquet). The log also contains metadata such as min/max statistics for each data file, enabling an order of magnitude faster metadata searches than the “files in object store” approach.\n\n![Delta Lake](/img/blog/2022-03-15-native-delta-lake-connector-for-presto/delta-lake.png)\n\nDelta Lake is designed so that all the metadata is in the underlying object store, and transactions are achieved using optimistic concurrency protocols against the object store (with some details varying by cloud provider). This means that no servers need to be running to maintain state for a Delta table; users only need to launch servers when running queries, and enjoy the benefits of separately scaling compute and storage. For more information, refer to the VLDB whitepaper [Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores](https://databricks.com/wp-content/uploads/2020/08/p975-armbrust.pdf).\n\n## Native Delta Lake Connector\n\nWe - the Presto and Delta Lake communities - have come together to make it easier for Presto to leverage the reliability of data lakes by integrating with Delta Lake. \n\nWe're excited to announce the addition of the Native Delta Lake Connector feature which allows for the reading of Delta Lake tables natively in Presto instead of using a manifest file (symlink_format_manifest).  \n\nThis enhances the Open Data Lake Analytics Experience with Presto, offering:\n* Robust data consistency\n* Automatic schema evolution\n* More memory performance with an iterator \n* No additional manual intervention\n* Time travel query support\n* Data skipping (as part of the [Delta Lake 2022H1 roadmap](https://github.com/delta-io/delta/issues/920))\n* File statistics (as part of the [Delta Lake 2022H1 roadmap](https://github.com/delta-io/delta/issues/920)) \n\nThis Presto/Delta connector utilizes the Delta Standalone project to natively read the Delta transaction log without the need of a manifest file. The memory-optimized, lazy iterator included in [Delta Standalone](https://docs.delta.io/latest/delta-standalone.html) allows PrestoDB to efficiently iterate through the Delta transaction log metadata and avoids OOM issues when reading large Delta tables.\n\n## Architecture\n\n![Architecture](/img/blog/2022-03-15-native-delta-lake-connector-for-presto/Architecture.png)\n\nFigure 1: This diagram illustrates the flow of calls between Presto main, Delta connector, Delta Standalone Reader (DSR) library, Hive Metastore (HMS) and Storage at Presto Coordinator and Executors\n\nPresto consists of two types of processes, Coordinator and Worker.\n\n* **Presto Coordinator** is responsible for receiving the query from the client, parsing, planning, optimizing, scheduling tasks and managing workers to execute queries and sending query output back to the client. \n* **Presto Worker** is responsible for executing tasks and processing data.\n\nDelta Lake Connector consists of following main components:\n* **Metadata Provider**: As the name says Metadata Provider is responsible for discovery of Delta Lake tables either through Hive Metastore or table path in storage and loading Delta table metadata using Delta Standalone Reader library which stores in its own [transaction logs](https://books.japila.pl/delta-lake-internals/DeltaLog/#_delta_log-metadata-directory). \n* **Delta Standalone Reader**: This is a library used to get the list of files (that pass the predicate) from the snapshot. The predicate (if present) may contain filters on partition columns and/or regular columns. The DSR library takes care of filtering the files based on the predicate and partition columns (and in future can use file stats to prune even more files). Each file maps one split and in addition to the file path, the split also contains the predicate which is used in data skipping at the reader (eg. Parquet RowGroup skipping or Page skipping)\n* **Split Generator**: Split Generator splits the table into multiple input splits. Splits are generated in batches with help of DSR's lazy iterator. Once the first batch is received, the tasks are started by the Scheduler. As the tasks read the first batch of splits, the next set of splits is generated by the Split Generator.\n* **Page Source Provider**: In Delta connector implementation, existing Parquet reader implementation is used to create a Page Source. The Parquet PageSource takes the file path, list of columns to read and predicate from the Split generated by Delta Split Generator. The predicate is used to prune row groups based on the row group stats. As we add support for other file formats, there is a need to create PageSource for that format.\n\n## Implementation: Querying Delta Lake tables from Presto\n\nThis native connector is available by default with your Presto installation. Here “delta” is the Delta Lake catalog name.\n\n### Accessing Delta Table with table name from Presto:\n```\npresto> describe delta.default.nation_location;\n\npresto> describe delta.default.nation_location;\n  Column   |  Type   | Extra | Comment \n-----------+---------+-------+---------\n nationkey | bigint  |       |         \n name      | varchar |       |         \n regionkey | bigint  |       |         \n comment   | varchar |       |         \n(4 rows)\n\nQuery 20220204_073639_00009_edx3y, FINISHED, 3 nodes\nSplits: 68 total, 68 done (100.00%)\n0:24 [4 rows, 299B] [0 rows/s, 12B/s]\n```\n### Directly querying Delta Lake table path - access delta table with its S3 file path\n```\npresto> select * from delta.\"$path$\".\"s3://delta-glue/nation_data\" limit 2;\n\npresto> select * from delta.\"$path$\".\"s3://delta-glue/nation_data\" limit 2;\nnationkey |   name    | regionkey |                                   comment\n-----------+-----------+-----------+------------------------------------------------------------------------------\n0 | ALGERIA   |         0 |  haggle. carefully final deposits detect slyly agai\n1 | ARGENTINA |         1 | al foxes promise slyly according to the regular accounts. bold requests alon\n(2 rows)\n\nQuery 20220204_073347_00005_edx3y, FINISHED, 2 nodes\nSplits: 34 total, 34 done (100.00%)\n0:14 [3 rows, 1.68KB] [0 rows/s, 123B/s]\n```\n\n### Snapshot Access\n\n#### Query version 1 of this table\n```\npresto> WITH nyctaxi_2019_part AS (SELECT * FROM deltas3.\"$path$\".\"s3://weyland-yutani/delta/nyctaxi_2019_part@v1\") SELECT COUNT(1) FROM nyctaxi_2019_part;\n\n  _col0\n----------\n 59354546\n(1 row)\n```\n#### Query version 5 of this table\n```\npresto> WITH nyctaxi_2019_part AS (SELECT * FROM deltas3.\"$path$\".\"s3://weyland-yutani/delta/nyctaxi_2019_part@v5\") SELECT COUNT(1) FROM nyctaxi_2019_part;\n\n  _col0\n----------\n 78959576\n(1 row)\n```\n#### Query current version of this table\n```\npresto> WITH nyctaxi_2019_part AS (SELECT * FROM deltas3.\"$path$\".\"s3://weyland-yutani/delta/nyctaxi_2019_part\") SELECT COUNT(1) FROM nyctaxi_2019_part;\n\n  _col0\n----------\n 84397753\n(1 row)\n```\n\n## What’s next?\nTry out the PrestoDB and Delta Lake connector! Here are some more resources to help you get started: \n* [Source code](https://github.com/prestodb/presto/tree/master/presto-delta)\n* [Public documentation](https://prestodb.io/docs/current/connector/deltalake.html)\n* [Presto/Delta Lake Community Office hours](https://www.youtube.com/watch?v=8qZ1zXLlD08)\n* [Delta Standalone project](https://databricks.com/blog/2022/01/28/the-ubiquity-of-delta-standalone-java-scala-hive-presto-trino-power-bi-and-more.html)\n* [PrestoCon 2021 presentation](https://databricks.com/blog/2022/01/28/the-ubiquity-of-delta-standalone-java-scala-hive-presto-trino-power-bi-and-more.html)\n\nIf you find any issues with the connector, please open up an issue in the [PrestoDB GitHub](https://github.com/prestodb/presto) or [Delta Lake Connectors GitHub](https://github.com/delta-io/connectors).  If you have questions, please join the [presto-users](https://groups.google.com/g/presto-users) discussion list, [PrestoDB Slack channel](https://prestodb.slack.com/),  [Delta Lake discussion list](https://groups.google.com/forum/#!forum/delta-users), or [Delta Lake Slack channel](https://dbricks.co/delta-users-slack).\n \n## Credit\nWe would also like to thank the PrestoDB and Delta Lake communities with special call out to Venki Korukanti, Sajith Appukuttan, and George Chow."
        },
        {
          "id": "/2022/01/28/avoid-data-silos-in-presto-in-meta",
          "metadata": {
            "permalink": "/blog/2022/01/28/avoid-data-silos-in-presto-in-meta",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2022-01-28-avoid-data-silos-in-presto-in-meta.md",
            "source": "@site/blog/2022-01-28-avoid-data-silos-in-presto-in-meta.md",
            "title": "Avoid Data Silos in Presto in Meta: the journey from Raptor to RaptorX",
            "description": "Alluxio: Rongrong Zhong",
            "date": "2022-01-28T00:00:00.000Z",
            "formattedDate": "January 28, 2022",
            "tags": [],
            "readingTime": 10.6,
            "truncated": true,
            "authors": [
              {
                "name": "Rongrong Zhong",
                "url": "https://www.linkedin.com/in/rongrong-zhong-31aa8a8/"
              }
            ],
            "frontMatter": {
              "title": "Avoid Data Silos in Presto in Meta: the journey from Raptor to RaptorX",
              "author": "Rongrong Zhong",
              "authorURL": "https://www.linkedin.com/in/rongrong-zhong-31aa8a8/"
            },
            "prevItem": {
              "title": "Native Delta Lake Connector for Presto",
              "permalink": "/blog/2022/03/15/native-delta-lake-connector-for-presto"
            },
            "nextItem": {
              "title": "Common Sub-Expression optimization",
              "permalink": "/blog/2021/11/22/common-sub-expression-optimization"
            }
          },
          "content": "**Alluxio**: Rongrong Zhong\n**Meta**: James Sun, Ke Wang\n\n*Raptor* is a Presto connector ([presto-raptor](https://github.com/prestodb/presto/tree/master/presto-raptor)) that is used to power some critical\ninteractive query workloads in Meta (previously Facebook). Though referred to in the ICDE 2019 paper\n*[Presto: SQL on Everything](https://research.facebook.com/publications/presto-sql-on-everything/)*, it remains somewhat mysterious to many\nPresto users because there is no available documentation for this feature. This article will shed some light on the history of Raptor, and why\nMeta eventually replaced it in favor of a new architecture based on local caching, namely RaptorX.\n\n![Raptor Timeline](/img/blog/2022-01-28-avoid-data-silos-in-presto-in-meta/timeline.png)\n\n## The story of Raptor\n\nGenerally speaking, Presto as a query engine does not own storage. Instead, connectors were developed to query different external data sources.\nThis framework is very flexible, but in disaggregated compute and storage architectures it is hard to offer low latency guarantees. Network and\nstorage latency add difficult to avoid variability. To address this limitation, Raptor was designed as a shared-nothing storage engine for Presto.\n\n### Motivation – an initial use case in the AB testing framework\n\n<!--truncate-->\n\nIn Meta, new product features typically go through AB testing before they are released more broadly. The AB testing framework allows engineers to configure\nexperiments that roll out a new feature to a test group, and then monitors key metrics against a control group. The framework provides engineers\nwith a UI to analyze their experiment’s statistics, which converts the configurations to Presto queries. The query shapes are known and limited.\nQueries typically join multiple large data sets, which include user, device, test, event attributes, etc. The basic requirements for this use case are:\n1. **Accuracy**: data need to be complete, accurate, and can't be approximate\n2. **Flexibility**: users should be able to arbitrarily slice and dice their results\n3. **Freshness**: test results should be available within hours\n4. **Interactive latency**: Queries need to return results within seconds\n5. **High availability**: As a critical service for product development, there should be minimal downtime for the service.\n\nPresto in a typical warehouse setting (i.e., using Hive connector to query warehouse data directly) could easily meet the first two requirements but not\nthe rest. At that time there was no near-real-time data ingestion and most warehouse data was ingested daily, thus not satisfying the freshness requirement.\nMeta's data centers were already moving to a disaggregated compute / storage architecture that could not guarantee latency when scanning large tables at high\nQPS. A typical Presto deployment would stop the whole cluster, thus not satisfying HA requirements.\n\nTo support this critical use case, we began the journey of productionizing Raptor.\n\n### Raptor Architecture\nFollowing is the high level architecture of a Presto cluster with Raptor connector.\n\n![Raptor Architecture](/img/blog/2022-01-28-avoid-data-silos-in-presto-in-meta/raptor_architecture.png)\n\nThe Raptor connector uses MySQL as its metastore for storing table and file metadata. Table data is stored on flash disks on each worker node and periodically\nbacked up to an external storage system to enable recovery in case of a worker node crashing. Data is ingested into the Raptor cluster in small enough batches to provide\nminute level latency, providing freshness. A standby cluster is created to provide high availability (HA).\n\nFor more information on the Raptor storage engine, read [Appendix - Raptor Architecture Details](#raptor-architecture-details) or watch [Appendix - Raptor Talk](#raptor-talk). \n\n### Limitations\nHaving compute/storage collocated, Raptor clusters can support low-latency high-throughput query workloads. However, the flip side of collocation is also significant.\n\n#### Low cluster utilization\nThe size of a Raptor cluster is typically decided by how much data needs to be stored. As the tables grow, more worker nodes are needed due to the collocated\ncompute/storage, which also creates challenges to repurpose these machines for other uses even when the cluster is idle.\n#### Low tail performance\nBecause data is hard allocated to worker nodes, if a worker node is down or slow, it will inevitably affect query performance, making it hard to provide\nstable tail performance.\n#### High engineering overhead\nRaptor requires a lot of storage engine specific features and processes like data ingestion / eviction, data compaction, data backup / restoration, data\nsecurity, etc. For a disaggregated Presto cluster directly querying Meta’s data warehouse, all of these services are managed by dedicated teams and\nimprovements benefit all use cases. The same cannot be said for Raptor, which resulted in engineering overhead.\n#### High operational overhead\nThe additional storage aspects of Raptor clusters also require additional operational work. The different cluster configuration and behavior means separate\noncall processes need to be set up.\n#### Potential security and privacy vulnerabilities\nWith the increasing security and privacy demands, having a unified implementation of security and privacy policies becomes more important. Using separate\nstorage engines makes enforcing such policies extremely hard and fragile.\n\n## The Inception of RaptorX\nWith the pain points of Raptor, engineers at Meta started to rethink Raptor’s future in 2019. Is it possible to get the benefit from local flash storage\nwithout paying the cost of collocated storage / compute architecture? The direction that was decided on was to add a new local caching layer on top of a\nvanilla data warehouse. This project, as a replacement of the Presto Raptor connector use cases, is named RaptorX.\n\nTechnically, The RaptorX project is not related to Raptor. The intuition is that the same flash drive can be used to store Raptor tables as data cache,\nthus keeping hot data on the compute nodes. The advantages of using local flash as caching rather than storage engine are:\n* It is no longer necessary for Presto to manage data lifecycle\n* Query performance is less affected by data loss due to single worker failure\n* Caching as a feature in the filesystem layer, is part of the presto-hive connector, thus the architecture of a RaptorX cluster is similar to other warehouse\npresto clusters, reducing operational overhead.\n\n### RaptorX Architecture\nFollowing is the architecture of RaptorX:\n\n![RaptorX Architecture](/img/blog/2022-01-28-avoid-data-silos-in-presto-in-meta/raptorx_architecture.png)\n\nThe fundamental difference between Raptor and Raptor X is how the local SSDs on workers are used. In RaptorX, Presto workers use [Alluxio](www.alluxio.io) to cache\nfile data locally. It is well understood that access patterns for different table columns could be very different, and columnar file formats\nlike ORC and Parquet are commonly used for data files to increase data locality within files. By caching file fragments in small page size on top of columnar files,\nonly data that is frequently accessed will be kept close to compute. The Presto coordinator tries to schedule compute that processes the same data to the same worker node to\nincrease cache effectiveness. RaptorX also implements file footer and metadata caching, and other smart caching strategies that improve the performance further.\n\nFor more details about the RaptorX architecture, please read [RaptorX: Building a 10X Faster Presto](https://prestodb.io/blog/2021/02/04/raptorx).\n\n### RaptorX vs Raptor performance benchmark\nWe ran benchmarks to compare the performance of a RaptorX prototype against Raptor. The benchmark is run on a cluster with ~1000 worker nodes and a single coordinator.\nRaptor and RaptorX are using the same hardware, so the whole dataset fits in RaptorX local SSD cache, thus cache hit rate is close to 100%.\n\n![Benchmark](/img/blog/2022-01-28-avoid-data-silos-in-presto-in-meta/benchmark.png)\n\nAs you can see from the benchmark result, P90 latency has an almost 2x improvement for RaptorX compared to Raptor. The difference between average query latency\nand P90 query latency in RaptorX is much smaller compared to Raptor. This is because in Raptor, data is physically bound to the worker node hosting it, thus a\nslow node would inevitably affect query latency. In RaptorX, instead of hard affinity between worker and data, we use soft affinity when scheduling. Soft affinity\nwill select two worker nodes as candidates to process a split. If the first choice worker node is up and healthy, that node would be chosen, otherwise a secondary\nnode will be chosen. Data can potentially be cached at multiple nodes, and scheduling can optimize for better CPU load balancing for the overall workload.\n\n### Migration from Raptor to RaptorX\nAll previous Raptor use cases in Meta are migrated to RaptorX, which provides better user experience and is easy to scale.\n\n#### A/B Testing Framework\nIn the previous section we mentioned that the requirements for the A/B testing framework are: accuracy, flexibility, freshness, interactive latency and high availability.\nSince RaptorX is a caching layer on the original Hive data, accuracy is guaranteed by Hive. It enjoys all the query optimization from the core Presto engine, as well as many\nspecific optimizations in Hive connector. Benchmark shows that both average and P90 query latency is better than Raptor. For freshness requirements, we were\nable to benefit from Meta’s near real time warehouse data ingestion framework improvements, which improved data freshness for all Hive data. High availability was guaranteed with\na standby cluster, same as in Raptor.\n\nDuring the process of migration, traffic to the framework grew to 2X due to great user experience and organic growth. RaptorX clusters were able to support the extra traffic\nwith the same capacity as Raptor clusters pre-migration. The clusters’ CPU capacity were fully utilized without worrying about storage limitations.\n\n#### Dashboard\nAnother typical use case of Raptor in Meta is improving the dashboard experience. Presto is used to power many of the dashboarding use cases in Meta, and some data engineering\nteams choose to inject their pre-aggregated tables to dedicated Raptor clusters for better performance. By migrating to RaptorX, data engineers can remove the ingestion step\nand no longer need to worry about data consistency between base tables and the pre-aggregated tables, while also enjoying around 30% query latency reduction in most percentiles\nbeyond P50.\n\n### Beyond the scope of Raptor\nSince RaptorX is very easy to use as a booster on normal Hive connector workloads, we also enabled it for Meta's warehouse interactive workloads. These are multitenant clusters\nthat handle pretty much all non-ETL queries to Hive data through Presto, ranging from Tableau, internal dashboards, various auto-generated UI analytics queries, various in-house\ntooling generated workloads, pipeline prototyping, debugging, data exploration, etc. RaptorX is enabled for these clusters to provide an opportunistic boost to queries that hit the\nsame data set.\n\n## Appendix\n### Raptor Architecture Details\n#### Data Organization\n\n![Data Organization](/img/blog/2022-01-28-avoid-data-silos-in-presto-in-meta/raptor_storage.png)\n\nRaptor tables are hash bucketed. Data from the same bucket is stored on the same worker node. Multiple tables bucketed on the same columns are called a\n*distribution*. A table bucket can contain multiple shards. *Shard* is the basic immutable unit of Raptor data. A shard is stored as a file in ORC format.\nTables can also have sorting properties, which allows better query optimization.\n\n#### Execution optimizations\nRaptor as a native storage engine for Presto allows Presto to schedule computation onto data nodes, thus providing low-latency, high-throughput data\nprocessing capabilities. In addition to generic SQL optimizations, the Raptor data organization enables more execution optimizations.\n* **Collocated Join**: when joining tables in the same distribution on the bucketed columns, Raptor will do a collocated join since data with the same\njoin keys are on the same worker, avoiding shuffling.\n* **Data Pruning**: Raptor can do shard level pruning and ORC reader level pruning.\n  * Shard level pruning: column ranges of shards are stored in metadata, which can be used to skip shards based on predicates. If a table has sorting\nproperties, shards will be sorted within a worker, which can also be used for shard pruning.\n  * ORC reader level pruning: ORC reader uses stripe metadata to prune stripes and row groups based on predicate. If data is ordered, ordering property\nalso helps with pruning.\n\n#### Other features\n* **Temporal column**: a TIME or DATE type column can be specified as a temporal column. Raptor enforces daily boundaries on shards if a temporal column is\nspecified. This helps with detention performance for large tables due to retention policies.\n* **Background compaction**: data is normally ingested into Raptor in small time granularities for freshness, this can result in small files, which is not\ngood for query performance. Raptor workers periodically run background jobs to compact small shards into big ones, and perform external sorting to maintain\nsorting properties.\n* **Data recovery**: if a worker goes down, the coordinator will redistribute the dead worker’s data across the rest of the cluster. All workers will download\nnecessary data from backup storage. During recovery, if a query needs the missing data, it will block until data is downloaded / recovered.\n* **Data cleaning**: each worker has a background process to compare its assigned data with its local data. Missing data will be recovered and stale data fixed.\n* **Data rebalancing**: if the coordinator detects data imbalance (e.g., new worker nodes added), it would fix the uneven data distribution.\n\n### Raptor Talk\nThere was a public talk at 2016 Data@Scale conference, you can get more information:\n[Presto Raptor: MPP Shared-Nothing Database on Flash](https://engineering.fb.com/2016/06/16/core-data/data-scale-june-2016-recap/)"
        },
        {
          "id": "/2021/11/22/common-sub-expression-optimization",
          "metadata": {
            "permalink": "/blog/2021/11/22/common-sub-expression-optimization",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-11-22-common-sub-expression-optimization.md",
            "source": "@site/blog/2021-11-22-common-sub-expression-optimization.md",
            "title": "Common Sub-Expression optimization",
            "description": "The problem",
            "date": "2021-11-22T00:00:00.000Z",
            "formattedDate": "November 22, 2021",
            "tags": [],
            "readingTime": 3.8,
            "truncated": true,
            "authors": [
              {
                "name": "Rongrong Zhong",
                "url": "https://www.linkedin.com/in/rongrong-zhong-31aa8a8/"
              }
            ],
            "frontMatter": {
              "author": "Rongrong Zhong",
              "authorURL": "https://www.linkedin.com/in/rongrong-zhong-31aa8a8/",
              "title": "Common Sub-Expression optimization"
            },
            "prevItem": {
              "title": "Avoid Data Silos in Presto in Meta: the journey from Raptor to RaptorX",
              "permalink": "/blog/2022/01/28/avoid-data-silos-in-presto-in-meta"
            },
            "nextItem": {
              "title": "Scaling with Presto on Spark",
              "permalink": "/blog/2021/10/26/Scaling-with-Presto-on-Spark"
            }
          },
          "content": "## The problem\n\nOne common pattern we see in some analytical workloads is the repeated use of the same, often times expensive expression. Look at the following query plan for example:\n\n![Query Plan](/img/blog/2021-11-22-common-sub-expression-optimization/query-plan.png)\n\nThe expression `JSON_PARSE(features)` is used 6 times, and casted to different `ROW` structures for further processing. Traditionally, Presto would just execute the expression 6 times, in 6 separate projections. Since Presto would generate efficient bytecode for each projection, this would not be a problem as long as the expression itself is not expensive. For example, executing `x+y` 6 times in a cache efficient way would not necessarily incur a big performance overhead. However, running expensive string manipulations like `JSON_PARSE` or `REGEX` operations multiple times could quickly add up.\n<!--truncate-->\n\n## Common sub-expression optimization\n\n*Common Sub-Expression (CSE) optimization* is a common optimization technique in query execution optimization. The idea is to extract the sub-expressions that are commonly used in all expressions, compute them only once, and rewrite the projections to use these results instead.\n\n### How does it work?\n\nLet’s use projection as an example to explain how CSE optimization works. First let’s take a look at how projection worked before we introduced CSE optimization. In the query plan above, you can see that the project operator has 6 *assignments* (e.g., `expr := TRY_CAST(json_parse(features))` is one *assignment*). For each assignment in the project operator, Presto would generate a java class `PageProjection` for the projection expression. This happens on *presto worker* when worker plans local execution. The expression evaluation logic is wrapped in `PageProjection.project`, which takes in a page, and produces a block as the output. In this model, there is one `PageProjection` class generated for each assignment. Since they are separate classes, no intermediate results can be shared among different assignments.\n\n![Original PageProjection](/img/blog/2021-11-22-common-sub-expression-optimization/page-projection.png)\n\nOne way to allow different assignment to share intermediate result (CSE result) is to generate a single `PageProjection` class for assignments that are sharing CSEs, turning the above model to the one below:\n\n![New Grouped PageProjection](/img/blog/2021-11-22-common-sub-expression-optimization/new-page-projection.png)\n\nThe code that generates `PageProjection` is in `PageFunctionCompiler`. In this class, we first extract all CSEs, in the above example, there is only one CSE, which is `json_parse(features)`. We create a new assignment for the CSE, then rewrite each assignment using this newly introduced variable. So the assignments become:\n\n```\ncse := json_parse(features)\nexpr := TRY_CAST(cse)\nexpr_0 := TRY_CAST(cse)\n...\nexpr_4 := TRY_CAST(cse)\n```\n\nSince all assignments depend on this CSE, we will wrap them in a single `PageProjection` class. We generate a function for cse that computes its value on first invocation and stores it, and on later invocation will just return the result directly.\n\n![PageFunctionCompiler](/img/blog/2021-11-22-common-sub-expression-optimization/page-function-compiler.png)\n\nIn this way, the expensive `json_parse` will only need to be executed once.\n\nLet’s look at another example to get a better understanding of what counts as CSEs. In the following query:\n\n```\nSELECT\n    x + y + z,                        -- exp0\n    x + y * z,                        -- exp1\n    (x + y + z) * 2,                  -- exp2\n    cast(x + y + z as VARCHAR),       -- exp3\n    (x + y + z) * 2 * z               -- exp4\nFROM \n    (VALUES (1, 2, 4), (3, 5, 7), (2, 4, 5)) t(x, y, z);\n```\n\nThere are 5 assignments in the projection, with 2 CSEs: `x + y + z` and `(x + y + z) * 2` (`x + y` is also a CSE but all expressions using this also uses `x + y + z` so we do not generate a separate function for it). We can rewrite this projection into:\n\n```\ncse1 := x + y + z\ncse2 := cse1 * 2\nexpr0 := cse1\nexpr1 := x + y * z\nexpr2 := cse2\nexpr3 := CAST(cse1 as VARCHAR)\nexpr4 := cse2 * z\n```\n\nFrom the rewritten assignments, we can see that expr1 does not share any CSEs with other expressions, and the rest do. We will generate 2 `PageProjection` classes, one for expr1, another one for the rest of the assignments.\n\n### Performance benefit\n\nMicrobenchmark (`CommonSubExpessionBenchmark`) has shown that even for simple expressions like `x + y`, there could be a 10% performance improvement. For queries using complex expressions like `JSON_PARSE`, we’ve seen 3x performance improvements in production.\n\n![Benchmark](/img/blog/2021-11-22-common-sub-expression-optimization/benchmark.png)\n\n## How to use it?\n\nThis optimization was first introduced in Presto release 0.235. There were subsequent improvements and bug fixes in the releases after that, but it is considered stable since release 0.245. The optimization is on by default. You can turn it off by setting session property `SET SESSION optimize_common_sub_expressions = false` to see the performance difference."
        },
        {
          "id": "/2021/10/26/Scaling-with-Presto-on-Spark",
          "metadata": {
            "permalink": "/blog/2021/10/26/Scaling-with-Presto-on-Spark",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-10-26-Scaling-with-Presto-on-Spark.md",
            "source": "@site/blog/2021-10-26-Scaling-with-Presto-on-Spark.md",
            "title": "Scaling with Presto on Spark",
            "description": "Co-authors\\",
            "date": "2021-10-26T00:00:00.000Z",
            "formattedDate": "October 26, 2021",
            "tags": [],
            "readingTime": 8.165,
            "truncated": true,
            "authors": [
              {
                "name": "Rohan Pednekar",
                "url": "https://www.linkedin.com/in/pednero"
              }
            ],
            "frontMatter": {
              "author": "Rohan Pednekar",
              "authorURL": "https://www.linkedin.com/in/pednero",
              "title": "Scaling with Presto on Spark"
            },
            "prevItem": {
              "title": "Common Sub-Expression optimization",
              "permalink": "/blog/2021/11/22/common-sub-expression-optimization"
            },
            "nextItem": {
              "title": "Native Parquet Writer for Presto",
              "permalink": "/blog/2021/06/29/native-parquet-writer-for-presto"
            }
          },
          "content": "**Co-authors**\\\n[Shradha Ambekar](https://www.linkedin.com/in/shradha-ambekar-a0504714), Staff Software Engineer at [Intuit](https://www.intuit.com/)\\\n[Ariel Weisberg](https://www.linkedin.com/in/ariel-weisberg-a5b6899), Software Engineer at [Facebook](https://www.facebook.com/)\n\n## Overview\n\nPresto was originally designed to run interactive queries against data warehouses, but now it has evolved into a unified SQL engine on top of open data lake analytics for both interactive and batch workloads. Popular workloads on data lakes include: \n\n### 1. Reporting and dashboarding \n\nThis includes serving custom reporting for both internal and external developers for business insights and also many organizations using Presto for interactive A/B testing analytics. A defining characteristic of this use case is a requirement for low latency. It requires tens to hundreds of milliseconds at very high QPS, and not surprisingly this use case is almost exclusively using Presto and that's what Presto is designed for.  \n\n### 2. Data science with SQL notebooks \n\nThis use case is one of ad hoc analysis and typically needs moderate latency ranging from seconds to minutes. These are the queries of data scientist, and business analysts who want to perform compact ad hoc analysis to understand product usage, for example, user trends and how to improve the product. The QPS is relatively lower because users have to manually initiate these queries.\n\n### 3. Batch processing for large data pipelines\n\nThese are scheduled jobs that are running every day, hour, or whenever the data is ready. They often contain queries over very large volumes of data and the latency can be up to tens of hours and processing can range from CPU days to years and terabytes to petabytes of data.\n\nPresto works exceptionally effectively for ad-hoc or interactive queries today, and even some batch queries, with the constraint that the entire query must fit in memory and run quickly enough that fault tolerance is not required. Most ETL batch workloads that don’t fit in this box are running on “very big data” compute engines like Apache Spark. Having multiple compute engines with different SQL dialects and APIs makes managing and scaling these workloads complicated for data platform teams. Hence, Facebook decided to simplify and build Presto on Spark as the path to further scale Presto. Before we get into Presto on Spark, let me explain a bit more about the architecture of each of these two popular engines. \n\n<!--truncate-->\n\n## Presto’s Architecture\n\n![Presto Architecture](/img/blog/2021-10-26-Scaling-with-Presto-on-Spark/presto_arch.png)\n\nPresto is designed for low latency and follows the classic MPP architecture; it uses in-memory streaming shuffle to achieve low latency. Presto has a single shared coordinator per cluster with an associated pool of workers. Presto tries to schedule as many queries as possible on the same Presto worker (shared executor), in order to support multi-tenancy.\n\nThis architecture provides very low latency scheduling of tasks and allows concurrent processing of multiple stages of a query, but the tradeoff is that the coordinator is a SPOF and bottleneck, and queries are poorly isolated across the entire cluster.\n\nAdditionally streaming shuffle does not allow for much fault tolerance further impacting the reliability of long running queries.\n\n## Spark’s Architecture\n\n![Spark Architecture](/img/blog/2021-10-26-Scaling-with-Presto-on-Spark/spark_arch.png)\n\nOn other hand, Apache Spark is designed for scalability from the very beginning and it implements a Map-Reduce architecture. Shuffle is materialized to disk fully between stages of execution with the capability to preempt or restart any task. Spark maintains an isolated Driver to coordinate each query and runs tasks in isolated containers scheduled on demand. These differences improve reliability and reduce overall operational overhead.\n\n## Why Presto alone isn’t a good fit for batch workloads?\n\nScaling an MPP architecture database to batch data processing over Internet-scale datasets is known to be an extremely difficult problem [1]. To simplify this let's examine the below aggregation query. Essentially this query goes over the orders table in TPCH and does aggregation grouping on custom keys, and summing the total price. Presto leverages in-memory shuffle and executes shuffle on the custom key, after reading the data and doing aggregation for the same key, on each worker.\n\n![MPP](/img/blog/2021-10-26-Scaling-with-Presto-on-Spark/pos1.png)\n\nDoing in-memory shuffle means the producer will buffer data in memory and wait for the data to be fetched by the consumer as a result. We have to execute all the tasks, before and after the exchange at the same time. So thinking about in the mapreduce world all the mappers and the reducer have to be run concurrently. This makes in-memory shuffle an all-or-nothing exclusion model.\n\nThis causes inflexible scheduling and scaling query size becomes more difficult because everything is running concurrently. In the aggregation phase the query may exceed the memory limit because everything has to be held in the memory in hash tables in order to track each group (custkey).\n\nAdditionally we are limited by the size of a cluster in how many nodes we can hash partition the data across to avoid having to fit it all in memory. Using distributed disk (Presto-on-Spark, Presto Unlimited) we can partition the data further and are only limited by the number of open files and even that is a limit that can be scaled quite a bit by a shuffle service.\n\nFor that reason it makes Presto difficult to scale to very large and complex batch pipelines. Such pipelines remain running for hours, all to join and aggregate over a huge amount of data. This motivated the development of [Presto Unlimited](https://prestodb.io/blog/2019/08/05/presto-unlimited-mpp-database-at-scale) which adapts Presto’s MPP design to large ETL workloads, and improves user experience at scale.\n\n![Presto Unlimited](/img/blog/2021-10-26-Scaling-with-Presto-on-Spark/pos2.png)\n\nWhile Presto Unlimited solved part of the problem by allowing shuffle to be partitioned over distributed disk, it didn’t fully solve fault tolerance, and did nothing to improve isolation and resource management.\n\n## Presto on Spark\n\nPresto on Spark is an integration between Presto and Spark that leverages Presto’s compiler/evaluation as a library with Spark’s RDD API used to manage execution of Presto’s embedded evaluation. This is similar to how Google chose to [embed F1 Query inside their MapReduce framework](http://www.vldb.org/pvldb/vol11/p1835-samwel.pdf).\n\nThe high level goal is to bring a fully disaggregated shuffle to Presto’s MPP run time and we achieved this by adding a materialization step right after the shuffle. The materialized shuffle is modeled as a temporary partition table, which brings more flexible execution after shuffle and allows to partition level retries. With Presto on Spark, we can do a fully disaggregated shuffle on custom keys for the above query both on mapper and reducer side, this means all mappers and reducers can be independently scheduled and are independently retriable.\n\n![Presto Evaluation Library](/img/blog/2021-10-26-Scaling-with-Presto-on-Spark/pos3.png)\n\n## Presto On Spark at Intuit\n\n[Superglue](https://towardsdatascience.com/superglue-journey-of-lineage-data-observability-data-pipelines-23ffb2990b30) is a homegrown tool at Intuit that helps users build, manage and monitor data pipelines. Superglue was built to democratize data for analysts and data scientists. Superglue minimizes time spent developing and debugging data pipelines, and maximizes time spent on building business insights and AI/ML.\n \nMany analysts at Intuit use Presto (AWS Athena) to explore data in the Data Lake/S3. These analysts would spend several hours converting these exploration SQLs written for Presto to Spark SQL to operationalize/schedule them as data pipelines in Superglue. To minimize SQL dialect conversion issues and associated productivity loss for analysts, the Intuit team started to explore various options including query translation, query virtualization, and presto on spark. After a quick POC, Intuit decided to go with Presto on Spark as it leverages Presto’s compiler/evaluation as a library (no query conversion is required) and Spark’s scalable data processing capabilities.\n\nPresto on Spark is now in production at Intuit. In three months, there are hundreds of critical pipelines that have thousands of jobs running on Presto On Spark via Superglue. \n\nPresto on Spark runs as a library that is submitted with [spark-submit](https://spark.apache.org/docs/latest/submitting-applications.html) or [Jar Task](https://docs.databricks.com/dev-tools/api/latest/jobs.html#jobssparkjartask)  on the Spark cluster. Scheduled batch data pipelines are launched on ephemeral clusters to take advantage of resource isolation, manage cost, and minimize operational overhead. DDL statements are executed against Hive and DML statements are executed against Presto. This enables analysts to write Hive-compatible DDL and the user experience remains unchanged. \n\nThis solution helped enable a performant and scalable platform with **seamless end-to-end experience** for analysts to explore and process data. It thereby improved analysts' productivity and empowered them to **deliver insights at high speed**.\n\n## When To Use Spark’s Execution Engine With Presto\n\nSpark is the tool of choice across the industry for running large scale complex batch ETL pipelines. Presto on Spark heavily benefits pipelines written in Presto that operate on terabytes/petabytes of data, as it takes advantage of Spark’s large scale processing capabilities. The biggest win here is that no query conversion is required and you can leverage Spark for  \n* Scaling to larger data volumes\n* Scaling Presto’s resource management to larger clusters\n* Increase reliability and elasticity of Presto as a compute engine\n\n## Why ‘Presto on Spark’ matters\n\nWe tried to achieve the following to adapt ‘Presto on Spark’ to Internet-scale batch workloads [2]:\n* Fully disaggregated shuffles\n* Isolated executors\n* Presto resource management, Different Scheduler, Speculative Execution, etc. \n\nA unified option for batch data processing and ad hoc is very important for  creating the experience of queries that scale instead of fail without requiring rewrites between different SQL dialects.\nWe believe this is only **a first step towards more confluence between the Spark and the Presto communities, and a major step towards enabling unified SQL experience between interactive and batch use cases**.\nToday many internet giants like Facebook, etc. have moved over to Presto on Spark and we have seen many organizations including Intuit started running their complex data pipelines in production with Presto on Spark. \n\n\n“Presto on Spark” is one of the most active development areas in [Presto](https://github.com/prestodb/presto), feel free check it out and please give it a star! If you have any questions, feel free to ask in the [PrestoDB Slack Channel](https://prestodb.slack.com/). \n\n## Reference\n[1] [MapReduce: Simplified Data Processing on Large Clusters](https://cs.stanford.edu/~matei/courses/2015/6.S897/readings/mapreduce-cacm.pd)\n[2] [Presto-on-Spark: A Tale of Two Computation Engines](https://github.com/prestodb/presto/issues/13856)"
        },
        {
          "id": "/2021/06/29/native-parquet-writer-for-presto",
          "metadata": {
            "permalink": "/blog/2021/06/29/native-parquet-writer-for-presto",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-06-29-native-parquet-writer-for-presto.md",
            "source": "@site/blog/2021-06-29-native-parquet-writer-for-presto.md",
            "title": "Native Parquet Writer for Presto",
            "description": "Pinterest: Lu Niu",
            "date": "2021-06-29T00:00:00.000Z",
            "formattedDate": "June 29, 2021",
            "tags": [],
            "readingTime": 2.79,
            "truncated": true,
            "authors": [
              {
                "name": "Lu Niu",
                "url": "https://www.linkedin.com/in/luniu/"
              }
            ],
            "frontMatter": {
              "title": "Native Parquet Writer for Presto",
              "author": "Lu Niu",
              "authorURL": "https://www.linkedin.com/in/luniu/"
            },
            "prevItem": {
              "title": "Scaling with Presto on Spark",
              "permalink": "/blog/2021/10/26/Scaling-with-Presto-on-Spark"
            },
            "nextItem": {
              "title": "Presto Foundation and PrestoDB: Our Commitment to the Presto Open Source Community",
              "permalink": "/blog/2021/06/14/Commitment-to-Presto-Open-Source-Community"
            }
          },
          "content": "**Pinterest:** Lu Niu\n\n**Twitter:** Zhenxiao Luo\n\n## Overview\nWith the wide deployment of Presto in a growing number of companies, Presto is used not only for queries, but also for data ingestion and ETL jobs. There is a need to improve Presto’s file writer performance, especially for popular columnar file formats, e.g. Parquet, and ORC. In this article, we introduce the brand new native Parquet writer for Presto, which writes directly from Presto's columnar data structure to Parquet's columnar values, with up to 6X throughput improvement and less CPU and memory overhead.\n\n<!-- truncate -->\n\n## Presto’s Old Parquet Writer\n\nAs shown in Figure 1, Presto was using Hive’s old Parquet writer to write files: \nThis old approach first iterates each columnar block in a page and reconstructs every single record.\nThen Presto would call the Hive record writer to consume each individual record and write value bytes to Parquet pages.\nWhenever it exceeds the buffer size, the old writer will flush in memory data to the underlying file system. \n\n  \n![Presto Page to Parquet RowGroups](/img/blog/2021-05-19-native-parquet-writer/page_to_parquet_rowgroups.png)\n\n  \nPresto has vectorized execution for in-memory columnar data, and Parquet is a columnar file format. The old Parquet writer was adding unnecessary overhead to convert Presto’s columnar in-memory data into row based records, and then doing one more conversion to write row based records to Parquet’s columnar on disk file format. The unnecessary data transformation adds significant performance overhead, especially when complex data types are involved such as nested structs.\n\n## Brand New Native Parquet Writer\nTo improve file writing efficiency, and overcome drawbacks in the old Parquet writer, we are introducing a brand new native Parquet writer, which writes directly from Presto's in-memory data structure to Parquet's columnar file format, including data values, repetition values, and definition values. The native Parquet writer significantly reduces CPU and memory overhead for Presto.\n\n  \n![Native Parquet Writer](/img/blog/2021-05-19-native-parquet-writer/writer-arch.png)\n  \n  \nAs shown in figure 2, the native Parquet writer constructs Parquet schema based on column names and types. Primitive types and complex types(Struct, Array, Map) are converted to corresponding Parquet types. Column writers are created with schema information. For each Presto page, the native writer iterates each block, transforming the Presto block into Parquet values, definition levels, and repetition levels. The corresponding column writer will write byte streams into a Parquet file.\n  \n## Performance \nPresto has a Hive file format benchmark to test reader and writer performance. The test creates a list of pages containing millions of rows, writes them to a temp file using either the native writer or the old hive record writer and then compares the performance. The following graphs show the results with three compression schemes: gzip, snappy and no compression. X-axis are various types of data; Y-axis are writing throughput. Clearly, we could see the brand new native Parquet writer outperforms the old writer. It could consistently achieve > 20% throughput improvements. The native Parquet writer performs best for BIGINT_SEQUENTIAL and BIGINT_RANDOM with GZIP compression, with up to 650% throughput improvements. When writing all columns of TPCH LINEITEM, the throughput gain is around 50%.\n  \n  \n![Writer Throughput: No Compression](/img/blog/2021-05-19-native-parquet-writer/no_compression.png)\n  \n  \n![Writer Throughput: SNAPPY](/img/blog/2021-05-19-native-parquet-writer/snappy.png)\n  \n  \n![Writer Throughput: GZIP](/img/blog/2021-05-19-native-parquet-writer/gzip.png)\n  \n  ## Use Native Parquet Writer in Production\n\n  The native parquet writer is ready for production deployment. To turn it on:\n```text\n# in /catalog/hive.properties\nhive.parquet.optimized-writer.enabled=true\n```\nIf you have any questions, feel free to ask in the [PrestoDB Slack Channel](https://prestodb.slack.com/)."
        },
        {
          "id": "/2021/06/14/Commitment-to-Presto-Open-Source-Community",
          "metadata": {
            "permalink": "/blog/2021/06/14/Commitment-to-Presto-Open-Source-Community",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-06-14-Commitment-to-Presto-Open-Source-Community.md",
            "source": "@site/blog/2021-06-14-Commitment-to-Presto-Open-Source-Community.md",
            "title": "Presto Foundation and PrestoDB: Our Commitment to the Presto Open Source Community",
            "description": "Authors",
            "date": "2021-06-14T00:00:00.000Z",
            "formattedDate": "June 14, 2021",
            "tags": [],
            "readingTime": 2.795,
            "truncated": true,
            "authors": [
              {
                "name": "Girish Baliga, Tim Meehan, Dipti Borkar, Amit Chopra, Zhenxiao Luo, Arijit Bandyopadhyay, Steven Mih, Bin Fan"
              }
            ],
            "frontMatter": {
              "title": "Presto Foundation and PrestoDB: Our Commitment to the Presto Open Source Community",
              "author": "Girish Baliga, Tim Meehan, Dipti Borkar, Amit Chopra, Zhenxiao Luo, Arijit Bandyopadhyay, Steven Mih, Bin Fan"
            },
            "prevItem": {
              "title": "Native Parquet Writer for Presto",
              "permalink": "/blog/2021/06/29/native-parquet-writer-for-presto"
            },
            "nextItem": {
              "title": "RaptorX: Building a 10X Faster Presto",
              "permalink": "/blog/2021/02/04/raptorx"
            }
          },
          "content": "**Authors** \n\n* Girish Baliga, Chair, Presto Foundation, Presto Foundation Member: [Uber](https://www.uber.com/)\n* Tim Meehan, Chair, Presto Foundation, Technical Steering Committee, Presto Foundation Member: [Facebook](https://www.facebook.com/)\n* Dipti Borkar, Chair, Presto Foundation, Outreach, Presto Foundation Member: [Ahana](https://ahana.io/)\n* Amit Chopra, Board member, Presto Foundation Member: [Facebook](https://www.facebook.com/)\n* Zhenxiao Luo , Board member, Presto Foundation Member: [Twitter](https://twitter.com/)\n* Arijit Bandyopadhyay, Board member, Presto Foundation Member: [Intel](https://intel.com/)\n* Steven Mih, Board member, Presto Foundation Member: [Ahana](https://ahana.io/)\n* Bin Fan, Outreach team member, Presto Foundation Member: [Alluxio](https://www.alluxio.io/)\n\n\nWe recently wrapped up an amazing PrestoCon Day attended by over 600 people from across the globe. The technical discussions and the panel was a clear indication of the growing community. We showcased a number of features contributed by various companies that continue to advance the mission of Presto open source, reiterating our commitment to grow the Presto community and support the continued improvement of the core technology. \n\n\n<!-- truncate -->\n\n\n**Why companies are choosing Presto today**\n\n\nWhile the data infrastructure stacks of member companies and customers are built on many different  open source technologies, Presto stands out as the interactive analytics engine of choice.\n\nCompanies are choosing Presto for three main reasons:\n\n* It’s open source, which lends itself to being a platform for innovation for many developers,\n* It’s incredibly fast and feature rich, and its extensibility allows us to provide a uniform user interface for a variety of analytics use cases. \n* It was built, matured, and is constantly improved at Facebook scale, one of the biggest in the industry. Some of the recent major innovations include Project Aria for scan optimization, Project RaptorX for IO performance optimization and multiple coordinator support. \n\nPresto’s versatility gives users the ability to make smart, effective, and timely business decisions based on data, and run critical business operations at global scale. \n\n**Why companies are participating in the Presto Foundation**\n\nLinux Foundation has provided an open and neutral governance model to successfully run the Presto project in the open., Under these guiding principles, the Presto community works in a constructive and collaborative manner in the Presto Foundation based on [the principles of openness, neutrality, and transparency](https://prestodb.io/join.html).\n\nAs members of the Linux Foundation’s Presto Foundation, we believe that a community-driven open source approach is critical to the diversity of ideas and continued openness of a critical project like Presto. \n\n\n**What’s next for the Presto open source community**\n\nAt [PrestoCon Day](https://prestodb.io/prestoconday2021.html), we shared a number of features on the roadmap that are driving Presto towards 10x improvements in Scale, Performance and Reliability. All these sessions will shortly be available on [our Presto YouTube channel](https://www.youtube.com/playlist?list=PLJVeO1NMmyqUDkrabo6CRGQ7zNTOMvu2L).\n\nIf you’re looking to join an open source project that is community-driven, focused on reliability, and are strongly committed to continuing the cutting edge innovating at scale in the data analytics space, we invite you to join the Presto community! \n\nLet’s make Presto better together!\n\n**Signed:**\n\n* Girish Baliga, Chair, Presto Foundation, Presto Foundation Member: Uber\n* Tim Meehan, Chair, Presto Foundation, Technical Steering Committee, Presto Foundation Member: Facebook\n* Dipti Borkar, Chair, Presto Foundation, Outreach, Presto Foundation Member: Ahana\n* Amit Chopra, Board member, Presto Foundation Member: Facebook\n* Zhenxiao Luo , Board member, Presto Foundation Member: Twitter  \n* Arijit Bandyopadhyay, Board member, Presto Foundation Member: Intel\n* Steven Mih, Board member, Presto Foundation Member: Ahana \n* Bin Fan, Outreach team member, Presto Foundation Member: Alluxio"
        },
        {
          "id": "/2021/02/04/raptorx",
          "metadata": {
            "permalink": "/blog/2021/02/04/raptorx",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-02-04-raptorx.md",
            "source": "@site/blog/2021-02-04-raptorx.md",
            "title": "RaptorX: Building a 10X Faster Presto",
            "description": "Facebook: Abhinav Sharma, Amit Dutta, Baldeep Hira, Biswapesh Chattopadhyay, James Sun, Jialiang Tan, Ke Wang, Lin Liu, Naveen Cherukuri, Nikhil Collooru, Peter Na, Prashant Nema, Rohit Jain, Saksham Sachdev, Sergey Pershin, Shixuan Fan, Varun Gajjala",
            "date": "2021-02-04T00:00:00.000Z",
            "formattedDate": "February 4, 2021",
            "tags": [],
            "readingTime": 9.43,
            "truncated": true,
            "authors": [
              {
                "name": "James Sun",
                "url": "https://www.linkedin.com/in/yutiansun/"
              }
            ],
            "frontMatter": {
              "title": "RaptorX: Building a 10X Faster Presto",
              "author": "James Sun",
              "authorURL": "https://www.linkedin.com/in/yutiansun/",
              "authorFBID": 100001087056694
            },
            "prevItem": {
              "title": "Presto Foundation and PrestoDB: Our Commitment to the Presto Open Source Community",
              "permalink": "/blog/2021/06/14/Commitment-to-Presto-Open-Source-Community"
            },
            "nextItem": {
              "title": "2020 Recap - A Year with Presto",
              "permalink": "/blog/2021/01/12/2020-recap-year-with-presto"
            }
          },
          "content": "**Facebook:** Abhinav Sharma, Amit Dutta, Baldeep Hira, Biswapesh Chattopadhyay, James Sun, Jialiang Tan, Ke Wang, Lin Liu, Naveen Cherukuri, Nikhil Collooru, Peter Na, Prashant Nema, Rohit Jain, Saksham Sachdev, Sergey Pershin, Shixuan Fan, Varun Gajjala\n\n**Alluxio:** Bin Fan, Calvin Jia, Haoyuan Li\n\n**Twitter:** Zhenxiao Luo\n\n**Pinterest:** Lu Niu\n\n*RaptorX is an internal project name aiming to boost query latency significantly beyond what vanilla Presto is capable of. This blog post introduces the hierarchical cache work, which is the key building block for RaptorX. With the support of the cache, we are able to boost query performance by 10X. This new architecture can beat performance oriented connectors like Raptor with the added benefit of continuing to work with disaggregated storage.*\n\n\n<!--truncate-->\n\n\n## Problems with Disaggregated Storage\n\n[Disaggregated storage](https://en.wikipedia.org/wiki/Disaggregated_storage) is the industry wide trend towards scaling storage and compute independently. It helps cloud providers reduce costs. Presto by nature supports such architecture. Data can be streamed from remote storage nodes outside of Presto servers.\n\n\n\nHowever, storage-compute disaggregation also provides new challenges for query latency as scanning huge amounts of data over the wire is going to be IO bound when the network is saturated. Moreover, the metadata paths will also go through the wire to retrieve the location of the data; a few roundtrips of metadata RPCs can easily bump up the latency to more than a second. The following figure shows the IO paths for Hive connectors in orange color, each of which could be a bottleneck on query performance.\n\n\n\n![Presto Architecture](/img/blog/2021-02-04-raptorx/presto-arch.png)\n\n\n\n\n## RaptorX: Build a Hierarchical Caching Solution\n\nHistorically, to solve the network saturation issue, Presto has a built-in Raptor connector to load data from remote storage to local SSD for fast access. However, this solution is no different from having a shared compute/storage node, which is runs counter to the idea of storage-compute disaggregation. The downside is obvious: either we waste CPU because the SSD in a worker is full or we waste SSD capacity if we are CPU bound. Thus, we started project RaptorX.\n\n\n\nRaptorX is an internal project aiming to boost query performance for Presto by at least 10X. Hierarchical caching is the key to RaptorX's success. Cache is particularly useful when storage nodes are disaggregated from compute nodes. The goal of RaptorX is not to create a new connector or product, but a built-in solution so that existing workloads can seamlessly benefit from it without migration. We are specifically targeting the existing Hive connector which is used by many workloads.\n\n\n\nThe following figure shows the architecture of the caching solution. We have hierarchical layers of cache, which will be introduced in details in the remainder of this post:\n\n* Metastore versioned cache: We cache table/partition information in the coordinator. Given metadata is mutable, like Iceberg or Delta Lake, the information is versioned. We only sync versions with metastore and fetch updated metadata when a version is out of date.\n* File list cache: Cache file list from remote storage partition directory.\n* Fragment result cache: Cache partially computed result on leaf worker's local SSDs. Pruning techniques are required to simplify query plans as they change all the time.\n* File handle and footer cache: Cache open file descriptors and stripe/file footer information in leaf worker memory. These pieces of data are mostly frequently accessed when reading files.\n* Alluxio data cache: Cache file segments with 1MB aligned chunks on leaf worker's local SSDs. The library is built from Alluxio's cache service.\n* Affinity scheduler: A scheduler that sends sticky requests based on file paths to particular workers to maximize cache hit rate.\n\n\n\n![RaptorX Architecture](/img/blog/2021-02-04-raptorx/raptorx-arch.png)\n\n\n## Metastore Versioned Cache\n\nA Presto coordinator caches table metadata (schema, partition list, and partition info) to avoid long `getPartitions` calls to Hive Metastore. However, Hive table metadata is mutable. Versioning is needed to determine if the cached metadata is valid or not. To achieve that, the coordinator attaches a version number to each cache key-value pair. When a read request comes, the coordinator asks the Hive Metastore to get the partition info (if it's not cached at all) or checks with Hive Metastore to confirm the cached info is up to date. Though the roundtrip to Hive Metastore cannot be avoided, the version matching is relatively cheap compared with fetching the entire partition info.\n\n\n\n## File List cache\n\nA Presto coordinator caches file lists in memory to avoid long `listFile` calls to remote storage. This can only be applied to sealed directories. For open partitions, Presto will skip caching those directories to guarantee data freshness. One major use case for open partitions is to support the need of near-realtime ingestion and serving. In such cases, the ingestion engines (e.g., micro batch) will keep writing new files to the open partitions so that Presto can read near-realtime data. Further details like compaction, metastore update, or replication for near-realtime ingestion will be out of the scope of this note.\n\n\n\n## Fragment Result Cache\n\nA Presto worker that is running a leaf stage can decide to cache the partially computed results on local SSD. This is to prevent duplicated computation upon multiple queries. The most typical use case is to cache the plan fragments on leaf stage with one level of scan, filter, project, and/or aggregation.\n\nFor example, suppose a user sends the following query, where `ds` is a partition column:\n\n```sql\nSELECT SUM(col) FROM T WHERE ds BETWEEN '2021-01-01' AND '2021-01-03'\n```\n\nThe partially computed sum for each of `2021-01-01`, `2021-01-02`, and `2021-01-03` partitions (or more precisely the corresponding files) will be cached on leaf workers forming a \"fragment result\". Suppose the user sends another query:\n\n```sql\nSELECT sum(col) FROM T WHERE ds BETWEEN '2021-01-01' AND '2021-01-05'\n```\n\nThen, the leaf worker will directly fetch the fragment result for `2021-01-01`, `2021-01-02`, and `2021-01-03` from cache and just compute the partial sum for `2021-01-04` and `2021-01-05`.\n\nNote that the fragment result is based on the leaf query fragment, which could be highly flexible as users can add or remove filters or projections. The above example shows we can easily handle filters with only partition columns. In order to avoid the cache miss caused by frequently changing non-partition column filters, we introduced partition statistics based pruning. Consider the following query, where time is a non-partition column:\n\n```sql\nSELECT SUM(col) FROM T\nWHERE ds BETWEEN '2021-01-01' AND '2021-01-05'\nAND time > now() - INTERVAL '3' DAY\n```\n\nNote that `now()` is a function that has values changing all the time. If a leaf worker caches the plan fragment based on `now()`’s absolute value, there is almost no chance to have a cache hit. However, if predicate `time > now() - INTERVAL '3' DAY` is a \"loose\" condition that is going to be true for most of the partitions, we can strip out the predicate from the plan during scheduling time. For example, if today was `2021-01-04`, we know for partition `ds = 2021-01-04`, predicate `time > now() - INTERVAL '3' DAY` is always true.\n\nMore generically, consider the following figure that contains a predicate and 3 partitions (`A`, `B`, `C`) with stats showing min and max. When the partition stats domain does not have any overlap with the predicate domain (e.g. partition `A`), we could directly prune this partition without sending splits to workers. If the partition stats is completely contained by the predicate domain (e.g. Partition `C`), then we don’t need this predicate because it would always hold true for this specific partition, and we could strip the predicate when doing plan comparison. For other partitions that have some overlapping with the predicate, we have to scan the partition with the given filter.\n\n\n\n![Predicate Pruning](/img/blog/2021-02-04-raptorx/frc.png)\n\n## File Descriptor and Footer Cache\n\nA Presto worker caches the file descriptors in memory to avoid long `openFile` calls to remote storage. Also, a worker caches common columnar file and stripe footers in memory. The current supported file formats are ORC, DWRF, and Parquet. The reason to cache such information in memory is due to the high hit rate of footers as they are the indexes to the data itself.\n\n\n\n## Alluxio Data Cache\n\nAlluxio data cache has been introduced in [an earlier post](https://prestodb.io/blog/2020/06/16/alluxio-datacaching). It is the main feature to deprecate the Raptor connector. A Presto worker caches remote storage data in its original form (compressed and possibly encrypted) on local SSD upon read. If, in the future, there is a read request covering the range that can be found on the local SSD, the request will return the result directly from the local SSD. The caching library was built as a joint effort with [Alluxio](https://www.alluxio.io/) and the Presto open source community.\n\nThe caching mechanism aligns each read into 1MB chunks, where 1MB is configurable to be adapted to different storage capability. For example, suppose Presto issues a read with 3MB in length starting with offset 0, then Alluxio cache checks if 0 - 1MB, 1 - 2MB, and 2 - 3MB chunks are already on disk and only fetch those that are not cached. The purging policy is based on LRU. It removes chunks from a disk that has not been accessed for the longest time. The Alluxio data cache exposes a standard Hadoop File System interface to the Hive connector, transparently storing requested chunks in a high-performance, highly concurrent, and fault-tolerant storage engine which is designed to serve workloads at Facebook scale.\n\n\n\n![Alluxio Data Cache](/img/blog/2021-02-04-raptorx/alluxio.png)\n\n\n\n\n## Soft Affinity Scheduling\n\nTo maximize the cache hit rate on workers, the coordinator needs to schedule the requests of the same file to the same worker. Because there is a high chance part of the file has already been cached on that particular worker. The scheduling policy is \"soft\", meaning that if the destination worker is too busy or unavailable, the scheduler will fallback to its secondary pick worker for caching or just skip the cache when necessary. The same [earlier post](https://prestodb.io/blog/2020/06/16/alluxio-datacaching) has a detailed explanation of the scheduling policy. The scheduling policy guarantees that cache is not on the critical path, but still can boost performance.\n\n\n\n## Performance\n\nRaptorX cache has been fully deployed and battle tested within Facebook. To compare the performance with vanilla Presto, we ran TPC-H benchmark on a 114-node cluster. Each worker has a 1TB local SSD with 4 threads configured per task. We prepared TPC-H tables with a scale factor of 100 in remote storage. The following chart shows the comparison between Presto and Presto with the hierarchical cache.\n\n\n\n![Alluxio Data Cache](/img/blog/2021-02-04-raptorx/tpch.png)\n\n\n\n\nFrom the benchmark, scan-heavy or aggregation-heavy queries like Q1, Q6, Q12 - Q16, Q19, and Q22 all have more than 10X latency improvement. Even join-heavy queries like Q2, Q5, Q10, or Q17 have 3X - 5X latency improvements.\n\n\n\n## User Guide\nIt is required to have local SSDs for workers in order to fully enable this feature. To turn on various layers of the caches in this post, tune the following configs accordingly.\n\nScheduling (`/catalog/hive.properties`):\n```text\nhive.node-selection-strategy=SOFT_AFFINITY\n```\n\n\nMetastore versioned cache (`/catalog/hive.properties`):\n```\nhive.partition-versioning-enabled=true\nhive.metastore-cache-scope=PARTITION\nhive.metastore-cache-ttl=2d\nhive.metastore-refresh-interval=3d\nhive.metastore-cache-maximum-size=10000000\n```\n\n\nList files cache (`/catalog/hive.properties`):\n```\nhive.file-status-cache-expire-time=24h\nhive.file-status-cache-size=100000000\nhive.file-status-cache-tables=*\n```\n\nData cache (`/catalog/hive.properties`):\n```\ncache.enabled=true\ncache.base-directory=file:///mnt/flash/data\ncache.type=ALLUXIO\ncache.alluxio.max-cache-size=1600GB\n```\n\nFragment result cache (`/config.properties` and `/catalog/hive.properties`):\n```\nfragment-result-cache.enabled=true\nfragment-result-cache.max-cached-entries=1000000\nfragment-result-cache.base-directory=file:///mnt/flash/fragment\nfragment-result-cache.cache-ttl=24h\nhive.partition-statistics-based-optimization-enabled=true\n```\n\nFile and stripe footer cache (`/catalog/hive.properties`):\n\n- For ORC or DWRF:\n```\nhive.orc.file-tail-cache-enabled=true\nhive.orc.file-tail-cache-size=100MB\nhive.orc.file-tail-cache-ttl-since-last-access=6h\nhive.orc.stripe-metadata-cache-enabled=true\nhive.orc.stripe-footer-cache-size=100MB\nhive.orc.stripe-footer-cache-ttl-since-last-access=6h\nhive.orc.stripe-stream-cache-size=300MB\nhive.orc.stripe-stream-cache-ttl-since-last-access=6h\n```\n\n- For Parquet:\n```\nhive.parquet.metadata-cache-enabled=true\nhive.parquet.metadata-cache-size=100MB\nhive.parquet.metadata-cache-ttl-since-last-access=6h\n```"
        },
        {
          "id": "/2021/01/12/2020-recap-year-with-presto",
          "metadata": {
            "permalink": "/blog/2021/01/12/2020-recap-year-with-presto",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2021-01-12-2020-recap-year-with-presto.md",
            "source": "@site/blog/2021-01-12-2020-recap-year-with-presto.md",
            "title": "2020 Recap - A Year with Presto",
            "description": "Tl;dr: 2020 was a huge year for the Presto community. We held our first major conference, PrestoCon, the biggest Presto event ever. We had a massive expansion of our meetup groups with more than 20 sessions held throughout the year, and significant innovations were contributed to Presto!",
            "date": "2021-01-12T00:00:00.000Z",
            "formattedDate": "January 12, 2021",
            "tags": [],
            "readingTime": 3.715,
            "truncated": true,
            "authors": [
              {
                "name": "Dipti Borkar",
                "url": "https://www.linkedin.com/in/diptiborkar/"
              }
            ],
            "frontMatter": {
              "title": "2020 Recap - A Year with Presto",
              "author": "Dipti Borkar",
              "authorURL": "https://www.linkedin.com/in/diptiborkar/"
            },
            "prevItem": {
              "title": "RaptorX: Building a 10X Faster Presto",
              "permalink": "/blog/2021/02/04/raptorx"
            },
            "nextItem": {
              "title": "Using OptimizedTypedSet to Improve Map and Array Functions",
              "permalink": "/blog/2020/12/04/typedset"
            }
          },
          "content": "Tl;dr: 2020 was a huge year for the Presto community. We held our first major conference, PrestoCon, the biggest Presto event ever. We had a massive expansion of our meetup groups with more than 20 sessions held throughout the year, and significant innovations were contributed to Presto! \n\nThis year has certainly been unique, to say the least. As chairperson of the Presto Foundation Outreach Committee, the term “outreach” took on a whole new meaning this year. But through the challenges of 2020, we adopted new ways to connect. We continued to build and engage with the Presto community in a new “virtual” way, and I couldn’t be more proud of all we’ve accomplished as a community in 2020. \n\nSo what did the Presto Foundation do in 2020? \n\n<!-- truncate -->\n\nHere are some of the things we accomplished:\n\n**Presto Meetups**\n\nIn April, we kicked off our [monthly virtual meetup series](https://www.meetup.com/prestodb/) and grew our meetup group to almost 1,000 people. We had talks from Presto users like LinkedIn, Jampp, Walmart, Apache Hudi, AWS, Ahana, Upsolver, Alluxio, and Amundsen and learned how these companies are solving real problems with Presto. We also got an inside look at some of the key Presto technological innovations from Presto engineers at Facebook, Uber, and Twitter.\n\nAdditionally, we organized Presto meetup groups in [New York City](https://www.meetup.com/presto-meetup-new-york/), [London](https://www.meetup.com/presto-meetup-london/), and [Sydney](https://www.meetup.com/presto-meetup-sydney/) (if you’re local to those areas, we encourage you to join!).\n\nI’m excited to connect with even more Presto users in 2021 through our meetup groups. Hopefully, we’ll be able to meet in person next year :)\n\n**PrestoCon 2020**\n\nWe hosted a [virtual PrestoCon](https://prestodb.io/prestocon.html) in September, the largest ever Presto event with nearly 600 registrations. Similar to KubeCon, PrestoCon will be the marquee event for Presto every year. There were 20 sessions on topics like the [Presto technology roadmap](https://static.sched.com/hosted_files/prestocon2020/66/prestocon-keynote-yigitbasi.pdf), the [Presto ecosystem](https://www.youtube.com/watch?v=lsFSM2Z4kPs), and how users like Uber, Alibaba, Pinterest, Twitter, Agari, and JD.com use Presto in production. The event was a true testament to how strong the Presto community is and how much it is growing. \n\nI can’t wait to see what will be showcased at the next one. Keep an eye out for our next event in early 2021!\n\n**Major innovations in PrestoDB**\n\nSome of the major innovations merged in and coming up were presented at PrestoCon. [Project Aria](https://engineering.fb.com/data-infrastructure/aria-presto/) dramatically improved table scan performance by 2-3x. One of the biggest challenges in Presto - the single coordinator is yet another challenge that is being solved by separating the resource manager and adding multiple coordinators. Another popular feature that’s seen a lot of interest in the community is the [support for UDFs](https://prestodb.io/docs/current/admin/function-namespace-managers.html). \n\nAnd the biggest innovation yet that will take Presto performance to the next level is the rewrite of the Presto worker from Java to C++. Anyone who has been in the database world and understands performance implications of the language that a query engine is written in will appreciate the order of magnitude performance improvement that this switch will bring to Presto. \n\n**More user contributions to prestodb.io**\n\nWe improved the user experience for prestodb.io with new [Introduction to Presto](https://prestodb.io/overview.html) and [Getting Started pages](https://prestodb.io/getting-started.html). We also had the most user-contributed blogs ever. [EA](https://prestodb.io/blog/2020/08/06/presto-in-ea), [Alibaba](https://prestodb.io/blog/2020/06/30/data-lake-analytics-blog), [Drift and Vistaprint]((https://prestodb.io/blog/2020/10/29/presto-at-drift)) shared their Presto use cases. Other users contributed how-to blogs like [Getting Started with Presto and Aria Scan Optimizations](https://prestodb.io/blog/2020/08/14/getting-started-and-aria), [Even Faster Unnest](https://prestodb.io/blog/2020/08/20/unnest), [PrestoDB and Apache Hudi](https://prestodb.io/blog/2020/08/04/prestodb-and-hudi), and many more.\n\n**Presto Foundation**\n\nPresto Foundation has now become an industry consortium. Similar to CNCF that hosts Kubernetes, we hope to see Presto Foundation with Presto grow with more end-users, vendors, and innovators joining the foundation! We had 3 new companies join the Presto Foundation earlier this year: Ahana, Upsolver, and Intel. We also look forward to working with Starburst, who has recently joined Presto Foundation as well. As the interest in and usage of Presto continues to grow, the companies that make up the foundation will help to drive even more developer awareness and engagement of the Presto technology. \n\nSo as you can see, we’ve been busy! I couldn’t be happier with the progress we made as a community in 2020, and I’m looking forward to all we have planned in 2021. By the way, if you want to stay connected with the Presto community, [join our mailing list](https://lists.prestodb.io/g/presto-events), where we’ll be sending out updates on new content, meetups, conferences, and more. \n\n\nCheers to a new year and the next stage of Presto!\n\n*_Dipti Borkar_*\nChairperson \n\nOn behalf of the\nPresto Foundation Outreach Committee"
        },
        {
          "id": "/2020/12/04/typedset",
          "metadata": {
            "permalink": "/blog/2020/12/04/typedset",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-12-04-typedset.md",
            "source": "@site/blog/2020-12-04-typedset.md",
            "title": "Using OptimizedTypedSet to Improve Map and Array Functions",
            "description": "Function evaluation is a big part of projection CPU cost. Recently we optimized a set of functions that use TypedSet, e.g.  mapconcat, arrayunion, arrayintersect, and arrayexcept. By introducing a new OptimizedTypeSet, the above functions saw improvements in several dimensions:",
            "date": "2020-12-04T00:00:00.000Z",
            "formattedDate": "December 4, 2020",
            "tags": [],
            "readingTime": 3.65,
            "truncated": true,
            "authors": [
              {
                "name": "Ying Su",
                "url": "https://www.linkedin.com/in/ying-su-b00b81107/"
              }
            ],
            "frontMatter": {
              "title": "Using OptimizedTypedSet to Improve Map and Array Functions",
              "author": "Ying Su",
              "authorURL": "https://www.linkedin.com/in/ying-su-b00b81107/",
              "authorFBID": 656599427
            },
            "prevItem": {
              "title": "2020 Recap - A Year with Presto",
              "permalink": "/blog/2021/01/12/2020-recap-year-with-presto"
            },
            "nextItem": {
              "title": "PrestoCon and Growing Industry Consortium - Intel and Upsolver Join Presto Foundation",
              "permalink": "/blog/2020/11/20/prestocon-and-foundation-update"
            }
          },
          "content": "Function evaluation is a big part of projection CPU cost. Recently we optimized a set of functions that use `TypedSet`, e.g.  `map_concat`, `array_union`, `array_intersect`, and `array_except`. By introducing a new `OptimizedTypeSet`, the above functions saw improvements in several dimensions:\n\n * Up to 80% reduction in wall time and CPU time in JMH benchmarks\n * Reserved memory reduced by 5%\n * Allocation rate reduced by 80%\n\nFurthermore, OptimizedTypeSet resolves the long standing issue of throwing `EXCEEDED_FUNCTION_MEMORY_LIMIT` for large incoming blocks: \"The input to function_name is too large. More than 4MB of memory is needed to hold the intermediate hash set.” \n\nThe `OptimizedTypeSet` and improvements to the above mentioned functions are merged to master, and will be available from Presto 0.244.\n\n<!--truncate-->\n\nIn this post we will look at the methods used in this improvement.\n\n## Avoid Using Internal BlockBuilder\n\n`TypedSet` has an internal `BlockBuilder` and appends each `Block` position being added to it, as well as an external `BlockBuilder` to construct the results. Block building using `BlockBuilder` is very costly and inefficient, especially when memory growth is not handled properly. The `BlockBuilder` was needed to keep track of the elements being added, so that it can:\n\n1. Resolve hash table probing collision\n2. Rehash\n\nHowever, `BlockBuilder` is not necessary for either of these issues. In all use cases, whole blocks (instead of several positions of a block) are added to the `TypedSet`, and the problem of resolving collisions can be resolved by keeping track of the blocks being added. Rehashing is not needed in most cases because we can calculate the maximum number of entries before creating the `TypedSet`. \n\nIn the new design, we provided methods that add a whole `Block` with different set operations: \n* union\n* intersect\n* except\n\nThese methods take in a `Block` and remember the selected positions. The operations can be applied on the same set multiple times, i.e. you can union two Block A and B, then intersect with Block C, then minus(except) Block D. This way the internal operations on the Block elements can be streamlined to more efficient loops. The memory consumption and allocation were also reduced because now we only need to remember the selected positions instead of building a whole new Block internally.\n\n## Avoid Recomputing Hash Positions\n\nCalculating hash positions took over 60% of original function costs. For some operations like set intersection, the previous implementation requires creating multiple `TypedSet`s and calculating the hash position multiple times. For example, the `array_intersect` function calculates the distinct common elements from two incoming `Block`s, one from the right side and the other one from the left side. It builds a `TypedSet` R which contains a hash table `R` for the right side `Block` first. When inserting a new element `X` from the left side `Block`, it first checks if hash table `R` contains the element. If not, the new element will be added to the other `TypedSet` L, which has another hash table `L` and represents the final results. \n\n![Remote Exchange](/img/blog/2020-12-04-OptimizedTypedSet/TypedSet.png)\n\nAs shown in the above picture, the `contains` and `add` operation would both calculate the hash positions for hash table `R` and `L` respectively. This hash position indicates the starting position of the linear probing for both hash tables. But actually the hashPosition calculated in hash table `R` can be reused by hash table `L` if the hash table sizes and hash functions are the same. \n\n![Remote Exchange](/img/blog/2020-12-04-OptimizedTypedSet/OptimizedTypedSet.png)\n\nIn the new `OptimizedTypedSet`, the first step is the same as the original implementation, in which the right side `Block` is added to the set first, and hash table `R` is constructed. Intersecting the set with the left side `Block` was done by creating a new hash table `L` of the same size (size precalculated) internally: it checks if the new elements are contained in hash table `R`, and if not, add them to hash table `L`. Unlike the original implementation, the `contains` and `add` operation share the same hash position calculation. When the new hash table `L` is constructed, the hash table `R` will be discarded. \n\n## Next Steps\n\nThe goal of the above PR was to demonstrate the benefit using the OptimizedTypedSet on 4 functions. There are a few other usages, e.g. `MultimapAggregationFunction`, `MapFromEntriesFunction`, etc. After we change them to use the new `OptimizedTypedSet`, the legacy `TypedSet` implementation can be removed.\n\n## Further reading\n\nFor more information please refer to the [PR#15362](https://github.com/prestodb/presto/pull/15362)."
        },
        {
          "id": "/2020/11/20/prestocon-and-foundation-update",
          "metadata": {
            "permalink": "/blog/2020/11/20/prestocon-and-foundation-update",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-11-20-prestocon-and-foundation-update.md",
            "source": "@site/blog/2020-11-20-prestocon-and-foundation-update.md",
            "title": "PrestoCon and Growing Industry Consortium - Intel and Upsolver Join Presto Foundation",
            "description": "Presto Foundation joined the Linux Foundation over a year ago, and has been focused on growing the Presto open source project and community. We encourage industry involvement with an open charter, clear guiding principles, and community-oriented goals. We recently hosted PrestoCon 2020, our first annual community conference, which was widely attended and well represented by Presto community members. We also warmly welcome Intel and Upsolver who recently joined the Presto Foundation.",
            "date": "2020-11-20T00:00:00.000Z",
            "formattedDate": "November 20, 2020",
            "tags": [],
            "readingTime": 3.315,
            "truncated": true,
            "authors": [
              {
                "name": "Girish Baliga",
                "url": "https://github.com/gbbaliga"
              }
            ],
            "frontMatter": {
              "title": "PrestoCon and Growing Industry Consortium - Intel and Upsolver Join Presto Foundation",
              "author": "Girish Baliga",
              "authorURL": "https://github.com/gbbaliga"
            },
            "prevItem": {
              "title": "Using OptimizedTypedSet to Improve Map and Array Functions",
              "permalink": "/blog/2020/12/04/typedset"
            },
            "nextItem": {
              "title": "Presto Enables Internal Log Data Analysis at Drift",
              "permalink": "/blog/2020/10/29/presto-at-drift"
            }
          },
          "content": "Presto Foundation joined the Linux Foundation over a year ago, and has been focused on growing the [Presto open source project](http://prestodb.io) and community. We encourage industry involvement with an [open charter](https://github.com/prestodb/foundation#presto-foundation-related-documents), [clear guiding principles](https://github.com/prestodb/foundation/blob/master/PRINCIPLES.md#presto-foundation-principles), and [community-oriented goals](https://github.com/prestodb/foundation/blob/master/GOALS.md#presto-foundation-pf-strategic-goals). We recently hosted [PrestoCon 2020](https://prestodb.io/prestocon.html), our first annual community conference, which was widely attended and well represented by Presto community members. We also warmly welcome Intel and Upsolver who recently joined the Presto Foundation. \n\n<!-- truncate -->\n\nBy way of quick introduction, I'm Girish Baliga, the newly elected Presto Foundation Governing Board Chair. I manage the Presto team at Uber and have been a contributor and observer at the governing board since the beginning of the foundation. After our successful PrestoCon event - the biggest of its kind - our previous chairperson **Brian Hsieh**, Head of Open Source at Uber, decided it was time for him to shift his focus on helping ramp up other great open source projects that Uber contributes to the community. I'd like to thank Brian for all his efforts in helping build the Presto community, and I'll continue to work closely with him.\n\n### Presto Foundation\nThe Linux Foundation's Presto Foundation encourages industry involvement and experience to oversee the Foundation itself and help it grow into an important consortium of organizations over time. Activities include outreach programs like PrestoCon and our monthly virtual meetup series. That said, the Governing Board generally stays clear of the technical aspects of the Presto project, which is handled by the Technical Steering Committee and the Project Committers. Presto Foundation membership is open to all interested organizations. To become a Premier Member, it takes a desire to see the Presto technology and community grow, based on the **[Foundation charter](https://github.com/prestodb/foundation#presto-foundation-related-documents)**, which includes **[these principles](https://github.com/prestodb/foundation/blob/master/PRINCIPLES.md#presto-foundation-principles)**:\n\n* One open, neutral and united Presto community\n* Open, transparent technical leadership and direction\n* No one individual or company is greater than the project and its community\n\nThe Presto Foundation has also posted its **[goals](https://github.com/prestodb/foundation/blob/master/GOALS.md#presto-foundation-pf-strategic-goals)** for the organization.\n\n### PrestoCon 2020\nTwo months ago in September, we had a fantastic first-ever [PrestoCon 2020](https://prestodb.io/prestocon.html) with a record number of attendees. The Linux Foundation did an amazing job managing the virtual conference, at a level you'd expect from an organization that puts on other big shows like KubeCon. To highlight a few happenings of the day, we had TSC Chair **Nezih Yigitbasi** open the show about all the [development in the PrestoDB community](https://static.sched.com/hosted_files/prestocon2020/66/prestocon-keynote-yigitbasi.pdf) and **Biswa Chattopadhyaya**, Chief Architect for Data at Facebook, close the day with an inspiring view of the [Presto roadmap moving forward](https://static.sched.com/hosted_files/prestocon2020/50/Presto%20%40%20Facebook%20Today%20and%20Tomorrow%20%28fburl.com_presto-vision-ext%29.pdf). Along with great community talks from Twitter, Alibaba, Pinterest, and JD.com, we saw great sponsor talks from Intel, Ahana, Alluxio, Facebook, Uber, and Starburst. We also saw a community launch announcement by premier member [Ahana](https://ahana.io/), with their [managed service for Presto](https://ahana.io/ahana-cloud/), making it easier to run Presto in the Cloud. Thanks to everyone who participated to make PrestoCon the biggest Presto event ever!\n\n### Welcome Intel and Upsolver\nI'm excited to welcome two new Premier Members to the Presto Foundation: Intel and Upsolver. Both are great organizations in their own ways. Everyone knows Intel of course, but some of you may not know that they have a deep Analytics and AI focus. **Arijit Bandyopadhyay**, CTO of Enterprise Analytics and AI in their Datacenter Group, will be joining as a Governing Board member. \n\nUpsolver is powering cloud lake houses by simplifying and automating data management. The company specializes in Presto data preparation and is the only officially recommended partner for AWS Athena.  **Ori Rafael**, the co-founder and CEO, will be joining as Governing Board member.\n\n![Presto Foundation Members](/img/blog/2020-11-20-prestocon-and-foundation-update/PF-members.png)\n\nWe started with four founding members: Facebook, Uber, Twitter, and Alibaba. Now, we have doubled the membership to eight and I'm looking forward to seeing the consortium grow even more over the coming years.\n\nI'm excited about all the plans we have moving forward. Stay tuned for our next big community event in March!\n\nGirish Baliga,  \nPresto Foundation Governing Board Chairperson"
        },
        {
          "id": "/2020/10/29/presto-at-drift",
          "metadata": {
            "permalink": "/blog/2020/10/29/presto-at-drift",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-10-29-presto-at-drift.md",
            "source": "@site/blog/2020-10-29-presto-at-drift.md",
            "title": "Presto Enables Internal Log Data Analysis at Drift",
            "description": "I’m a Senior Software Engineer in the data group at Drift, a conversational marketing platform that is used for qualifying leads faster, automatically booking meetings and connecting customers to the right business solutions more efficiently. I’ve used Presto quite a bit throughout my career, and I want to first give readers a quick overview of how Presto has enabled my team at Drift to quickly and cost-effectively analyze distributed logs at scale. Then I will share how we used and benefited from Presto at Vistaprint, where I worked previously.",
            "date": "2020-10-29T00:00:00.000Z",
            "formattedDate": "October 29, 2020",
            "tags": [],
            "readingTime": 3.47,
            "truncated": true,
            "authors": [
              {
                "name": "Arun Venkateswaran",
                "url": "https://github.com/venkaa6"
              }
            ],
            "frontMatter": {
              "title": "Presto Enables Internal Log Data Analysis at Drift",
              "author": "Arun Venkateswaran",
              "authorURL": "https://github.com/venkaa6"
            },
            "prevItem": {
              "title": "PrestoCon and Growing Industry Consortium - Intel and Upsolver Join Presto Foundation",
              "permalink": "/blog/2020/11/20/prestocon-and-foundation-update"
            },
            "nextItem": {
              "title": "Even Faster Unnest",
              "permalink": "/blog/2020/08/20/unnest"
            }
          },
          "content": "I’m a Senior Software Engineer in the data group at Drift, a conversational marketing platform that is used for qualifying leads faster, automatically booking meetings and connecting customers to the right business solutions more efficiently. I’ve used Presto quite a bit throughout my career, and I want to first give readers a quick overview of how Presto has enabled my team at Drift to quickly and cost-effectively analyze distributed logs at scale. Then I will share how we used and benefited from Presto at Vistaprint, where I worked previously.\n<!--truncate-->\n## How we use Presto at Drift\n\nTo provide data engineers in my group with the ability to mine log files, we use Amazon Athena which has Presto as its distributed SQL query engine for running analytic queries against data of any size.\nWe also use AWS Glue, an ETL service that makes it easy to categorize data, clean it, and move it between different data stores and data streams. Glue basically provides a unified metadata repository across a variety of data sources and formats, and it integrates easily with Athena.\n\n\n![](/img/blog/2020-10-29-presto-at-drift/architecture.png)\n\n\n\n\nSpecifically, Presto plays a key role in enabling internal analysis here at Drift. We use Presto to query both semi-structured and freeform log data such as JSON messages. With Presto, data engineers can easily drop the data into Amazon S3 without needing to perform too much data cleaning, so that Athena can read the data easily. We use that data to generate graphs that show how many log reports we’re running, or how many bad instances per hour are occurring.\n\n### Advantages of Presto\n\nOne of the main advantages of Presto in this case is that my engineering team can get to the data quickly and start querying the data immediately.\n\nThe data in Presto is completely cloud-based; it basically functions as an on-demand service query search. If the engineers need to run a massive load of data, it will automatically spin as many nodes as needed. \n\nBusiness users at Drift could use Presto by querying Salesforce data using Looker, our BI and analytics platform. They would be able to easily process petabytes of data to find answers to their questions, such as how many sales opportunities were created, how many accounts they have, and the progression of the sales pipeline from qualified lead to a closed sale.\nThe main data warehouse at Drift is Snowflake. The data loading does take a lot of development time, as it would require a lot of “scaffolding work” in order to run it. With Presto, I can drop the data in and parse it immediately. \n\n## How we used Presto at Vistaprint\n\nI also had the opportunity to use Presto earlier in my career, while working at Vistaprint, an ecommerce platform that combined in-house software and production technology to mass-customize personalized products.\n\nAt Vistaprint, Presto was used for flow log reporting to read order data. Every five minutes, we would do a count in AWS Athena to look at orders coming through the system. If it showed that no orders came through in a 15-minute timeframe, I would get an automatic alert via Slack or text, and I would know that there was a problem that needed human intervention. Presto provided quick visibility into the logs so that we could act fast. When our Kafka ingestion cluster went down, the Lambda code that I wrote triggered a Slack alert so we could respond quickly.\n\n### The advantages of using Presto at Vistaprint\n\nIn the case with Vistaprint, Presto’s ability to efficiently and quickly query large amounts of data from multiple sources was critical. I also appreciated how easy it was to add AWS Athena/Presto to our existing system. I didn’t need to worry about node provisioning, cluster setup, Presto configuration, or cluster tuning.\n\n## Conclusion\n\nPresto is designed and tuned for real-world workloads, and I can query data where it is stored without needing to move it into a separate analytics system. This is a huge resource and cost saver. I’m looking forward to seeing how Presto evolves over the next few years, and I’m excited to continue to use it for interactive analytic queries."
        },
        {
          "id": "/2020/08/20/unnest",
          "metadata": {
            "permalink": "/blog/2020/08/20/unnest",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-08-20-unnest.md",
            "source": "@site/blog/2020-08-20-unnest.md",
            "title": "Even Faster Unnest",
            "description": "Ying Su, Masha Basmanova, Orri Erling",
            "date": "2020-08-20T00:00:00.000Z",
            "formattedDate": "August 20, 2020",
            "tags": [],
            "readingTime": 8.91,
            "truncated": true,
            "authors": [
              {
                "name": "Ying Su",
                "url": "https://www.linkedin.com/in/ying-su-b00b81107/"
              }
            ],
            "frontMatter": {
              "title": "Even Faster Unnest",
              "author": "Ying Su",
              "authorURL": "https://www.linkedin.com/in/ying-su-b00b81107/",
              "authorFBID": 656599427
            },
            "prevItem": {
              "title": "Presto Enables Internal Log Data Analysis at Drift",
              "permalink": "/blog/2020/10/29/presto-at-drift"
            },
            "nextItem": {
              "title": "Getting Started with PrestoDB and Aria Scan Optimizations",
              "permalink": "/blog/2020/08/14/getting-started-and-aria"
            }
          },
          "content": "Ying Su, Masha Basmanova, Orri Erling\n\nUnnest is a common operation in Facebook’s daily Presto workload. It converts an `ARRAY`, `MAP`, or `ROW` into a flat relation. Its original implementation used deep copy all the time and was very inefficient. In [Unnest Operator Performance Enhancement with Dictionary Blocks](https://prestosql.io/blog/2019/08/23/unnest-operator-performance-enhancements.html), the author improved the Unnest operator by up to 10x in CPU and elapsed times by using `DictionaryBlock` when possible. We went one step further and improved it for another 5-10x.\n\n<!--truncate-->\n\n In this post, we will refer to the PrestoSQL implementation as \"baseline\". The JMH Benchmark results for different cases are shown below (\"before\" is the baseline implementation, and \"after\" is our new implementation):\n\n\n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestOneColumnWithoutOrdinality.png)\n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestTwoColumns.png)\n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestRowType.png)\n\nThe optimized Unnest implementation is available in Presto 0.235 and onward and is enabled by default. The JMH benchmark was also expanded to have better coverage and can be found in [BenchmarkUnnestOperator.java](https://github.com/prestodb/presto/blob/29acdbaff128fff2de185c28fb26031be1d88019/presto-main/src/test/java/com/facebook/presto/operator/BenchmarkUnnestOperator.java).\n\nIn Facebook's production workload, we observed over a 6x CPU reduction on this operator. The following chart shows the pairwise comparison for all unnest operators in one of our test runs. Every dot below the diagonal is a win. About 25% of operators show over a 5x CPU reduction, and some of them have even over a 20x reduction.\n\n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestCPUPairWiseComparison.png)\n\nThe histogram of the after vs. before CPU ratio shows most operators has a ratio less than 1, meaning most of them were more efficient. \n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestCPUDistribution.png)\n\nThe following chart shows the operator's CPU percentage was reduced from about 2% to 0.1% out of all operators after the roll-out happened on May 28. The amount of CPU, though small, still matters at Facebook's scale.\n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestPercentageTimeline.png)\n\n\n## What is the Unnest Operator?\n\nWhen users have data structured as `ARRAY`, `MAP`, or `ROW` they sometimes need to flatten them so that the nested structure can be regarded as top level citizen and sent to downstream operators for easier arithmetic or aggregation processing. An example query is as follows:\n\n```sql\nSELECT\n    zoo, animal\nFROM (\n    VALUES\n        ('OaklandZoo', ARRAY['dog', 'cat', 'tiger']),\n        ('SanFranciscoZoo', ARRAY['dog', 'cat'])\n) AS x (zoo, animals)\nCROSS JOIN UNNEST(animals) AS t (animal);\n```\n\nIn this example, the `zoo` column, which is a `VARCHAR` column, is being replicated for the same row and thus being called the \"replicated column\"; and the `animals` column, which is an `ARRAY(VARCHAR)` column, is unnested into a `VARCHAR` column, named as `animal` and is a \"unnest column\":\n\n```text\n| zoo             | animal |\n| ----------------| ------ |\n| OaklandZoo      | dog    |\n| OaklandZoo      | cat    |\n| OaklandZoo      | tiger  |\n| SanFranciscoZoo | dog    |\n| SanFranciscoZoo | cat    |\n```\n\nIn Presto relational data is represented as a series of `Page`s internally. `Page`s are composed of `Block`s, one for each column. For replicated columns, the Unnest operator needs to create new `Block`s from the original `Block`s with the data being replicated within each original row. The baseline implementation achieved this by creating a `DictionaryBlock` on top of the original `Block`, and thus avoided expensive deep copies. The same thing was done for the unnest columns if there is no need to insert nulls, where `DictionaryBlocks` were created, and the array offsets were translated to indices of the underlying block.\n\nWhat happens when there are multiple unnest columns and their cardinalities do not match? Take a look at this example:\n\n```sql\nSELECT\n    zoo,\n    animal,\n    employee\nFROM (\n    VALUES\n        ('OaklandZoo', ARRAY['dog', 'cat', 'tiger'], ARRAY['Alice', 'Bob']),\n        ('SanFranciscoZoo', ARRAY['dog', 'cat'], ARRAY['Clara', 'Danny'])\n) AS x (zoo, animals, employees)\nCROSS JOIN UNNEST(animals, employees) AS t (animal, employee);\n```\n\nThe result is\n\n```text\n| zoo             | animal | employee |\n| --------------- | ------ | -------- |\n| OaklandZoo      | dog    | Alice    |\n| OaklandZoo      | cat    | Bob      |\n| OaklandZoo      | tiger  | NULL     |\n| SanFranciscoZoo | dog    | Clara    |\n| SanFranciscoZoo | cat    | Danny    |\n```\n\nNote that a `NULL` is inserted in the last column on the third row. In this case, can we still create a `DictionaryBlock` on the original block? The original nested block does not have a `NULL`! So it’s impossible to find an index that points to a `NULL` value and build a `DictionaryBlock` easily. The baseline implementation just switches back to deep copying for this case. In the sections below, we are going to explain how we tackled this problem.\n\n## Opportunities for Improvements? Yes!\n\nThe baseline implementation did a number of things well. It avoided deep copy by using DictionaryBlock which improved the original improvement by an order of magnitude for cases without NULL insertions. Can we improve it additionally? The answer is yes.\n\n### Process data column-by-column, not row-by-row\nIf you read our blog post [5 design choices—and 1 weird trick — to get 2x efficiency gains in Presto repartitioning](https://prestodb.io/blog/2019/12/20/repartition), you may remember the design choice “Process data column by column, not row by row”. The same thing applies to `UnnestOperator` as well. The baseline implementation constructs the output blocks in the row-by-row manner:\n\n```text\nfor each incoming row\n    for each replicated column\n        append the repeated values by adding ids to the DictionaryBlock\n    for each unnest column\n        append the unnested  values by adding ids to the DictionaryBlock if possible.\n        Otherwise append the value to BlockBuilder.\n    if there is ordinality column\n        append the oridinality to the BlockBuilder\n```\n\nA key principle of vectorized execution is to use tight loops which allow the JVM to vectorize the compiled binary, and allow the CPU to parallelize the work as much as possible. Processing the data in a column by column manner makes this possible and an example of such a loop for building a replicated block looks like this:\n\n```java\npublic Block buildOutputBlock(int[] maxEntries, int offset, int length, int totalEntries)\n{\n    int[] ids = new int[totalEntries];\n    int fromPosition = 0;\n    for (int i = 0; i < length; i++) {\n        int toPosition = fromPosition + maxEntries[offset + i];\n        Arrays.fill(ids, fromPosition, toPosition, sourcePosition++);\n        fromPosition = toPosition;\n    }\n\n    return new DictionaryBlock(totalEntries, source, ids);\n}\n```\n\nThis simple change improved the performance of the NO-NULL case by at least 3x as shown by the JMH benchmarks.\n\n### Computation of maxEntries array and batch size\n\nTo make the above loop possible, we pre-compute the cardinalities for each row for all unnest columns and use them to get the max cardinalities (we call it the maxEntries array) in the operator. This way the memory can be accessed sequentially and the computation can be reused. The size of these arrays should require very little memory overhead, since they’re only for top level rows and should always be less than 1024 per block.\n\nWe then compute the batch size, ie. top level row count that can fit into the next output page. Right now we limit the nested row count for each batch to 1000 with a minimum batch size of 1. This means the unnested row count for each block might be over 1000. For example, if a row contains 10000 array entries, then the unnested row count would be 10000. Although this is over the 1000 rows we still have to output the whole top level row which translates into 10000 unnested rows.\n\n### Bonus: No need to create DictionaryBlock at all for some cases!\n\nThere are several cases that we can tell no `NULL`s need to be inserted:\n\n1. When there is only one unnest column (except `ARRAY` of `ROW`s case).\n2. When there are multiple columns, but the cardinalities for them are the same for the current batch.\n3. For a single Array of Row unnest column case, if the row block doesn't contain any nulls.\n\nFor these cases, the result can simply be acquired by using `getRegion()` to get a view on top of the original leaf block:\n\n```java\npublic Block buildOutputBlockWithoutNulls(int outputPositionCount)\n{\n    Block output = source.getRegion(sourcePosition, outputPositionCount);\n    sourcePosition += outputPositionCount;\n    return output;\n}\n```\n\nThis doesn’t copy the data, nor does it construct the `ids` mapping and `DictionaryBlock`. Nice!\n\n### A better way to copy Blocks\n\nHow do we handle the cases where `NULL`s need to be inserted? We can formulate the problem as a simple question: given a `Block` without `NULL`, how do we insert a `NULL` efficiently?\n\nThere are two ways of doing this:\n\n1. Use special id, e.g. `-1` to identify null in `DictionaryBlock`\n2. Copy the blocks and insert a NULL.\n\nCurrently `DictionaryBlock` doesn’t support the logic in 1). To make it happen, we need to change all `getId()` and `getIds()` call to handle the special values. While the size of the callsites is manageable (about 50), the performance implication need to be studied carefully.\n\nThe baseline implementation used the second way and used `PageBuilder` and `BlockBuilder` to copy the values for this case. However, the JMH benchmarks shows that they are not super efficient for this purpose. To compare the cost of copying blocks using\n`PageBuilder` and `BlockBuilder` versus doing bulk copy using tight loops, we measured the following two implementations:\n\n1. Using `PageBuilder` and `BlockBuilder`\n\n```java\n@Benchmark\npublic void copyBlockByAppend(BenchmarkData data)\n{\n    LongArrayBlockBuilder longArrayBlockBuilder = new LongArrayBlockBuilder(null, POSITIONS_PER_PAGE);\n    for (int i = 0; i < BLOCK_COUNT; i++) {\n        Block block = data.blocks.get(i);\n        int positionCount = block.getPositionCount();\n\n        for (int j = 0; j < positionCount; j++) {\n            BIGINT.appendTo(block, j, longArrayBlockBuilder);\n        }\n\n        Block outputBlock = longArrayBlockBuilder.build();\n    }\n}\n```\n\n2. Using bulk copy\n\n```java\n@Benchmark\npublic void copyBlockByLoop(BenchmarkData data)\n{\n    for (int i = 0; i < BLOCK_COUNT; i++) {\n        Block block = data.blocks.get(i);\n        int positionCount = block.getPositionCount();\n\n        long[] values = new long[positionCount];\n        for (int j = 0; j < positionCount; j++) {\n            values[j] = block.getLong(j);\n        }\n\n        boolean[] valueIsNull = copyIsNulls(block);\n\n        Block outputBlock = new LongArrayBlock(positionCount, Optional.of(valueIsNull), values);\n    }\n}\n```\n\nEven though the second code snippet is copying through the Block interface, it was over 4x faster over the first one:\n\n![Remote Exchange](/img/blog/2020-08-20-unnest.md/UnnestCopyBlocks.png)\n\n\nWe thus thought copying the Blocks this way can give us acceptable performance. To make the code clean and better encapsulated, we added a new method `appendNull()` to the `Block` interface. For all primitive types like `BOOLEAN`, `SMALLINT`, `INTEGER`, `BIGINT`, etc., we create a new `Block` with all the positions copied and then a `NULL` appended at the end of the block. For `VariableWidthBlock`, `ArrayBlock`, `MapBlock` and `RowBlock`, we can get hold of the underlying elements block or the raw slice, and then construct the offsets array. It doesn’t need to do a deep copy of the nested structure. To do this, we need to convert the `ArrayBlock`, `MapBlock` and `RowBlock` into the `ColumnarXXX` objects but the cost of conversion is very worthwhile.\n\n## Further reading\n* Here is the original issue explaining the design https://github.com/prestodb/presto/issues/13751\n* Here is the main pull request that made it all happen https://github.com/prestodb/presto/pull/13746"
        },
        {
          "id": "/2020/08/14/getting-started-and-aria",
          "metadata": {
            "permalink": "/blog/2020/08/14/getting-started-and-aria",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-08-14-getting-started-and-aria.md",
            "source": "@site/blog/2020-08-14-getting-started-and-aria.md",
            "title": "Getting Started with PrestoDB and Aria Scan Optimizations",
            "description": "This article was originally published by Adam on June 15th, 2020 over at his blog at datacatessen.com.",
            "date": "2020-08-14T00:00:00.000Z",
            "formattedDate": "August 14, 2020",
            "tags": [],
            "readingTime": 6.62,
            "truncated": true,
            "authors": [
              {
                "name": "Adam Shook",
                "url": "https://www.linkedin.com/in/adamjshook"
              }
            ],
            "frontMatter": {
              "title": "Getting Started with PrestoDB and Aria Scan Optimizations",
              "author": "Adam Shook",
              "authorURL": "https://www.linkedin.com/in/adamjshook"
            },
            "prevItem": {
              "title": "Even Faster Unnest",
              "permalink": "/blog/2020/08/20/unnest"
            },
            "nextItem": {
              "title": "Building a high-performance platform on AWS to support real-time gaming services using Presto and Alluxio",
              "permalink": "/blog/2020/08/06/presto-in-ea"
            }
          },
          "content": "This article was originally published by Adam on June 15th, 2020 over at his blog at [datacatessen.com](https://datacatessen.com/blog/prestodb-aria/).\n\n---\n\n[PrestoDB](https://prestodb.io) recently released a set of experimental features under their Aria project in order to increase table scan performance of data stored in ORC files via the Hive Connector.  In this post, we'll check out these new features at a very basic level using a test environment of PrestoDB on Docker.  To find out more about the Aria features, you can check out the [Facebook Engineering](https://engineering.fb.com/data-infrastructure/aria-presto/) blog post which was published June 2019.\n\n<!--truncate-->\n\nPresto is a massively parallel processing (MPP) SQL execution engine.  The execution engine is decoupled from data storage, and the project contains numerous plugins, called _Connectors_, that provide the Presto engine with data for query execution.  Data is read from the data store, then handed to Presto where it takes over to perform the operations of the query, such as joining data and performing aggregations.  This decoupling of data storage and execution allows for a single Presto instance to query various data sources, providing a very powerful federated query layer.  There are many connectors available for Presto, and the community regularly provides additional connectors for data stores.\n\nThe Hive Connector is often considered the standard connector for Presto.  This connector is configured to connect to a Hive Metastore, which exposes metadata about the tables defined in the Metastore.  Data is typically stored in HDFS or S3, and the Metastore provides information about where the files are stored and in what format; typically ORC but other supported formats like Avro and Parquet are supported.  The Hive connector allows the Presto engine to scan data from HDFS/S3 in parallel into the engine to execute your query.  [ORC (Optimized Row Columnar)](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+ORC) format is a very standard and common format for storing data, as it provides good compression and performance.\n\nPresto has two core services for executing queries. A _Coordinator_, which is responsible for query parsing and scheduling (among other things), and many _Workers_ which execute the queries in parallel.  The Coordinator can also act as a Worker, though it is not used for production environments.  Since we're playing with Presto here, we'll just use one node to act as both a Coordinator and Worker.  More detailed documentation, including installation details, can be found [here](https://prestodb.io/docs/current/).\n\nWe'll use a single Docker container in order to play around with Presto.  An example deployment of a single-node Presto deployment can be found towards the bottom of the deployment documentation [here](https://prestodb.io/docs/0.239/installation/deployment.html).\n\nLet's talk a bit about how Presto executes a query.  The Presto Coordinator parses the query to build a plan (which we will see examples of below).  Once the plan is made, it is broken into several stages (or fragments) which execute a series of operators.  Operators are a particular function that the engine performs to execute your query.  This typically begins with scanning data via a Connector, then performing operations such as filtering data, partial aggregations, and commonly exchanging data between Presto workers to perform joins and final aggregations.  All of these stages are broken into _splits_, which is a unit of parallelism in Presto.  Workers execute a configurable number of splits in parallel to get your desired results.  All data in the engine is kept in-memory (as long as you don't go past the thresholds of the cluster; another topic for another time).\n\nThe Hive connector (and all connectors for that matter) are responsible for breaking the input data set into splits for Presto to read in parallel.  As an optimization, the Presto engine will tell connectors the predicates used in a query and what columns are being selected, called _predicate pushdown_, which enables connectors to drop data before even handing it to the engine (what this blog post is all about!).\n\nTo demonstrate the predicate pushdown, let's take a look at a basic query -- counting rows of a table within a range.  The TPC-H `lineitem` table has about 600 million rows with a `shipdate` between 1992 and 1998.  Let's start without enabling the session properties to enable the Aria enhancements, running an `EXPLAIN` command to take a look at the query plan.\n\n```SQL\npresto:tpch> EXPLAIN (TYPE DISTRIBUTED) SELECT COUNT(shipdate) FROM lineitem WHERE shipdate BETWEEN DATE '1992-01-01' AND DATE '1992-12-31';\n\nFragment 0 [SINGLE]\n    Output layout: [count]\n    Output partitioning: SINGLE []\n    Stage Execution Strategy: UNGROUPED_EXECUTION\n    - Output[_col0] => [count:bigint]\n            _col0 := count\n        - Aggregate(FINAL) => [count:bigint]\n                count := \"\"presto.default.count\"\"((count_4))\n            - LocalExchange[SINGLE] () => [count_4:bigint]\n                - RemoteSource[1] => [count_4:bigint]\n\nFragment 1 [SOURCE]\n    Output layout: [count_4]\n    Output partitioning: SINGLE []\n    Stage Execution Strategy: UNGROUPED_EXECUTION\n    - Aggregate(PARTIAL) => [count_4:bigint]\n            count_4 := \"\"presto.default.count\"\"((shipdate))\n        - ScanFilter[table = TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{domains={shipdate=[ [[1992-01-01, 1992-12-31]] ]}}]'}, grouped = false, filterPredicate = shipdate BETWEEN (DATE 1992-01-01) AND (DATE 1992-12-31)] => [shipdate:date]\n                Estimates: {rows: 600037902 (2.79GB), cpu: 3000189510.00, memory: 0.00, network: 0.00}/{rows: ? (?), cpu: 6000379020.00, memory: 0.00, network: 0.00}\n                LAYOUT: tpch.lineitem{domains={shipdate=[ [[1992-01-01, 1992-12-31]] ]}}\n                shipdate := shipdate:date:10:REGULAR\n\n```\n\nQuery plans are read bottom-up, starting with `Fragment 1` that will scan the `lineitem` table in parallel, performing the filter on the `shipdate` column to apply the predicate.  It will then perform a partial aggregation for each split, and exchange that partial result to the next stage `Fragment 0` to perform the final aggregation before delivering the results to the client. In an effort to visualize the plan, see below. Note the horizontal line towards the bottom of the diagram, indicating which code executes in the Hive Connector and which code executes in the Presto engine.\n\n![Query Plan 1](/img/blog/2020-08-14-getting-started-and-aria/query-plan-1.png)\n\nWe'll now execute this query!\n\n```SQL\npresto:tpch> SELECT COUNT(shipdate) FROM lineitem WHERE shipdate BETWEEN DATE '1992-01-01' AND DATE '1992-12-31';\n  _col0   \n----------\n 76036301\n(1 row)\n\nQuery 20200609_154258_00019_ug2v4, FINISHED, 1 node\nSplits: 367 total, 367 done (100.00%)\n0:09 [600M rows, 928MB] [63.2M rows/s, 97.7MB/s]\n```\n\nWe see there are a little over 76 million rows `lineitem` table in the year 1992.  It took about 9 seconds to execute this query, processing 600 million rows.\n\nNow let's set the session properties `pushdown_subfields_enabled` and `hive.pushdown_filter_enabled` to enable the Aria features and take a look at the same explain plan.\n\n```SQL\npresto:tpch> SET SESSION pushdown_subfields_enabled=true;\nSET SESSION\npresto:tpch> SET SESSION hive.pushdown_filter_enabled=true;\nSET SESSION\npresto:tpch> EXPLAIN (TYPE DISTRIBUTED) SELECT COUNT(shipdate) FROM lineitem WHERE shipdate BETWEEN DATE '1992-01-01' AND DATE '1992-12-31';\nFragment 0 [SINGLE]\n    Output layout: [count]\n    Output partitioning: SINGLE []\n    Stage Execution Strategy: UNGROUPED_EXECUTION\n    - Output[_col0] => [count:bigint]\n            _col0 := count\n        - Aggregate(FINAL) => [count:bigint]\n                count := \"\"presto.default.count\"\"((count_4))\n            - LocalExchange[SINGLE] () => [count_4:bigint]\n                - RemoteSource[1] => [count_4:bigint]\n\nFragment 1 [SOURCE]\n    Output layout: [count_4]\n    Output partitioning: SINGLE []\n    Stage Execution Strategy: UNGROUPED_EXECUTION\n    - Aggregate(PARTIAL) => [count_4:bigint]\n            count_4 := \"\"presto.default.count\"\"((shipdate))\n        - TableScan[TableHandle {connectorId='hive', connectorHandle='HiveTableHandle{schemaName=tpch, tableName=lineitem, analyzePartitionValues=Optional.empty}', layout='Optional[tpch.lineitem{domains={shipdate=[ [[1992-01-01, 1992-12-31]] ]}}]'}, grouped = false] => [shipdate:date]\n                Estimates: {rows: 540034112 (2.51GB), cpu: 2700170559.00, memory: 0.00, network: 0.00}\n                LAYOUT: tpch.lineitem{domains={shipdate=[ [[1992-01-01, 1992-12-31]] ]}}\n                shipdate := shipdate:date:10:REGULAR\n                    :: [[1992-01-01, 1992-12-31]]\n```\n\nNote the major difference in the query plan at the very bottom, the inclusion of `shipdate` column in the `TableScan` operation. We see here that the connector now notices the predicate on the `shipdate` column of `1992-01-01` to `1992-12-31`.  To visualize, we see this predicate is pushed down to the connector, removing the necessity of the engine to filter this data.\n\n![Query Plan 2](/img/blog/2020-08-14-getting-started-and-aria/query-plan-2.png)\n\nWe'll run this query again!\n\n```SQL\npresto:tpch> SELECT COUNT(shipdate) FROM lineitem WHERE shipdate BETWEEN DATE '1992-01-01' AND DATE '1992-12-31';\n  _col0   \n----------\n 76036301\n(1 row)\n\nQuery 20200609_154413_00023_ug2v4, FINISHED, 1 node\nSplits: 367 total, 367 done (100.00%)\n0:05 [76M rows, 928MB] [15.5M rows/s, 189MB/s]\n```\n\nWe get the same result running the query, but the query time took almost half as long and, more importantly, we see only 76 million rows were scanned!  The connector has applied the predicate on the `shipdate` column, rather than having the engine process the predicate.  This saves some CPU cycles, resulting in faster query results. YMMV for your own queries and data sets, but if you're using the Hive connector with ORC files, it is definitely worth a look."
        },
        {
          "id": "/2020/08/06/presto-in-ea",
          "metadata": {
            "permalink": "/blog/2020/08/06/presto-in-ea",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-08-06-presto-in-ea.md",
            "source": "@site/blog/2020-08-06-presto-in-ea.md",
            "title": "Building a high-performance platform on AWS to support real-time gaming services using Presto and Alluxio",
            "description": "Authors: Teng Wang, Du Li, Yu Jin and Sundeep Narravula",
            "date": "2020-08-06T00:00:00.000Z",
            "formattedDate": "August 6, 2020",
            "tags": [],
            "readingTime": 5.685,
            "truncated": true,
            "authors": [
              {
                "name": "Teng Wang",
                "url": "https://www.linkedin.com/in/teng-wang-9b8732194/"
              }
            ],
            "frontMatter": {
              "title": "Building a high-performance platform on AWS to support real-time gaming services using Presto and Alluxio",
              "author": "Teng Wang",
              "authorURL": "https://www.linkedin.com/in/teng-wang-9b8732194/"
            },
            "prevItem": {
              "title": "Getting Started with PrestoDB and Aria Scan Optimizations",
              "permalink": "/blog/2020/08/14/getting-started-and-aria"
            },
            "nextItem": {
              "title": "PrestoDB and Apache Hudi",
              "permalink": "/blog/2020/08/04/prestodb-and-hudi"
            }
          },
          "content": "**Authors:** Teng Wang, Du Li, Yu Jin and Sundeep Narravula\n\nElectronic Arts (EA) is a leading company in the gaming industry, providing dozens of games to serve billions of users worldwide each year.\nMaking near real-time decisions for EA’s online services is critical for our business.\nThis blog describes a data platform on AWS based on Presto and Alluxio to support online services with instantaneous response within the gaming industry.\n\n<!--truncate-->\n\n![](/img/blog/2020-08-06-presto-in-ea/EAGames.png)\n\nThe EA Data & AI Department builds hundreds of platforms to manage petabytes of data generated by games and users everyday. These platforms consist of a wide range of data analytics, from real-time data ingestion to ETL pipelines. Formatted data produced by our department is widely adopted by executives, producers, product managers, game engineers, and designers for marketing and monetization, game design, customer engagement, player retention, and end-user experience.\n\n## Use Cases\n\nNear real-time information for EA’s online services is critical for making business decisions, such as campaigns and troubleshooting. These services include, but are not limited to, real-time data visualization, dashboarding, and conversational analytics. Our team is actively seeking a framework that can support these use cases.\n\nAt EA, we have adopted numerous data visualization tools, such as Tableau and Dundas, to support data insight analytics. These tools are usually connected with multiple data sources, such as MySQL DB, AWS S3, or HDFS. Users can load data from multiple endpoints simultaneously to run computationally heavy algorithms. One severe performance bottleneck is loading data as it is I/O heavy. This could be exacerbated more if the same data needs to be loaded multiple times. Thus we need a solution to reduce this data retrieving overhead by caching the data locally.\n\nDashboarding is another common use case, used to keep tracking user engagements, customer satisfaction, or system status in real-time. In these cases, the data volume is usually on the order of gigabytes, but frequent refreshes require instantaneous processing. Currently, we use commercial databases such as Redshift to serve time-sensitive data, and we are seeking an alternative to slash costs without losing performance.\n\nWe recently developed a reporting chatbot to provide immediate game related insights, such as live user satisfaction and real-time profit analysis. The backend of this system runs Presto with petabytes of data stored on S3. This chatbot converts user's questions into ANSI SQL and runs these queries on a Presto cluster. The queries usually conduct complex computations, such as prediction and merging after searching across datasets. We are eager to find a solution that compliments our S3 based datasets to improve the performance without introducing extra cost.\n\n## Architecture\n\nTo serve these different use cases with near-realtime requirements, we built and evaluated Presto as our data query platform with data in S3, and the working set cached in Alluxio.\nIn this blog we compare a mock of the aforementioned production setup of Presto on S3 against a similar stack with Alluxio. The architecture is shown below:\n\n![](/img/blog/2020-08-06-presto-in-ea/architecture.png)\n\nHere are some more details about our setup:\n\n- Each instance launched Presto and Alluxio, co-locating the two services.\n- For hardware, we used three h1.8xlarge AWS instances, each with 8TB ephemeral disks mounted for use by Alluxio to cache data local to Presto.\n- S3 was mounted to Alluxio as the underlying persisting file system.\n- Two catalogs were configured for Presto; one connected to our existing Hive metastore, referencing the benchmark datasets externally stored on S3, and the other connected to a separate - Hive metastore with the benchmark tables created in Alluxio.\n- We used the same datasets on S3 for performance comparison and pre-loaded the data into Alluxio with `alluxio fs distributedLoad /testDB`\n- To optimize the query performance when  processing large amounts of small files, we enabled metadata caching in `alluxio-site.properties` to tune the performance\n\n```\nalluxio.user.metadata.cache.enabled=true\nalluxio.user.metadata.cache.max.size=100000\nalluxio.user.metadata.cache.expiration.time=10min\n```\n\n## Benchmark Result\n\nFour independent benchmarks representing a wide range of different workloads were selected.\nThe baseline is the performance when Presto was querying S3 directly.\n\n**Benchmark 1** is our internal synthesis snapshots of player in-game events, representing **I/O heavy use cases**.\nThree datasets were tested with a total size of 1GB, 10GB, and 100GB and files in ORC format.\nEach dataset is created with the same DDL, containing 49 cols, 40 varchar, 5 booleans, and 4 maps.\nThe benchmark queries select all columns with one varchar field filter condition.\n\n**Result**: With caching data in Alluxio, Presto performs 2x to 7x faster compared to the baseline when querying S3 directly.\n\n**Benchmark 2** simulates data visualization with game metadata and user engagement records.\nwhich is a typical query that **stresses both CPU and I/O**.\nWith two commonly used datasets and queries frequently used in Tableau and Dundas respectively,\nthe queries select all columns with a date filtering condition, followed by `GROUP BY` and `ORDER BY` of the date.\nIn this test, we intentionally disabled Alluxio metadata caching which already shows significant improvement, to understand how data caching helps here.\n\n**Result**: Without metadata caching, Presto with Alluxio is 2.75x faster than S3 with the Dundas dataset and 5.1x faster with the Tableau dataset.\n\n**Benchmark 3** simulates our dashboarding use case using a dataset with **a large number of small files**. The datasets are batches of 2MB files, totalling to 50, 500, and 5000 files. The query used is a select query aggregating the number of entries for each date.\n\n**Result**: Alluxio with metadata caching is 1.2x to 5.9x faster than S3. Without metadata caching, Alluxio is only 1x to 1.35x faster. Enabling metadata caching significantly reduces the execution time by memorizing metadata, recognizing hot data and increasing replicas.\n\n**Benchmark 4** simulates the conversational bot. The dataset used was a snapshot of daily game performance.\nThe query contains **multiple stages of calculation to simulate a CPU intensive query**.\nIt converts an integer field into HyperLogLog, merges it, and selects the cardinality.\nThe results are filtered by an integer and varchar field.\n\n**Result**: Alluxio without metadata caching shortens the timespan from 85.2 seconds to 3 seconds, which improves the performance as much as 27x.\n\n## Conclusion\n\nThis blog explores an innovative platform with Presto as the computing engine and Alluxio as a data orchestration layer between Presto and S3 storage, to support online services with instantaneous response within the gaming industry. We evaluated this platform with real industrial examples of data visualization, dashboarding, and a conversational chatbot. Our preliminary results show that Presto with Alluxio outperforms S3 significantly in all cases. In particular, Alluxio with metadata caching shows up to 5.9x performance gain when handling large numbers of small files. Alluxio enables the separation of storage and compute by managing the allocated ephemeral disks to cache data from S3 local to Presto. Advanced cache management with an asymmetric number of replicas for hot vs. cold data accounts for performance gains in each scenario we tested."
        },
        {
          "id": "/2020/08/04/prestodb-and-hudi",
          "metadata": {
            "permalink": "/blog/2020/08/04/prestodb-and-hudi",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-08-04-prestodb-and-hudi.md",
            "source": "@site/blog/2020-08-04-prestodb-and-hudi.md",
            "title": "PrestoDB and Apache Hudi",
            "description": "Co-author: Brandon Scheller",
            "date": "2020-08-04T00:00:00.000Z",
            "formattedDate": "August 4, 2020",
            "tags": [],
            "readingTime": 15.46,
            "truncated": true,
            "authors": [
              {
                "name": "Bhavani Sudha Saktheeswaran",
                "url": "https://www.linkedin.com/in/bhasudha"
              }
            ],
            "frontMatter": {
              "author": "Bhavani Sudha Saktheeswaran",
              "authorURL": "https://www.linkedin.com/in/bhasudha",
              "title": "PrestoDB and Apache Hudi"
            },
            "prevItem": {
              "title": "Building a high-performance platform on AWS to support real-time gaming services using Presto and Alluxio",
              "permalink": "/blog/2020/08/06/presto-in-ea"
            },
            "nextItem": {
              "title": "Running Presto in a Hybrid Cloud Architecture",
              "permalink": "/blog/2020/07/17/alluxio-hybrid-cloud"
            }
          },
          "content": "**Co-author:** [Brandon Scheller](https://www.linkedin.com/in/brandon-scheller-a00851ab)\n\n[Apache Hudi](https://hudi.apache.org) is a fast growing data lake storage system that helps organizations build and manage petabyte-scale data lakes. Hudi brings stream style processing to batch-like big data by introducing primitives such as upserts, deletes and incremental queries. These features help surface faster, fresher data on a unified serving layer. Hudi tables can be stored on the Hadoop Distributed File System (HDFS) or cloud stores and integrates well with popular query engines such as [Presto](https://prestodb.io), [Apache Hive](https://hive.apache.org), [Apache Spark](https://spark.apache.org) and [Apache Impala](https://impala.apache.org). Given Hudi pioneered a new model that moved beyond just writing files to a more managed storage layer that interops with all major query engines, there were interesting learnings on how integration points evolved.\n\nIn this blog we are going to discuss how the Presto-Hudi integration has evolved over time and also discuss upcoming file listing and query planning improvements to Presto-Hudi queries.\n\n<!--truncate-->\n\n\n## Apache Hudi overview\nApache Hudi (Hudi for short, here on) enables storing vast amounts of data on top of existing DFS compatible storage while also enabling stream processing in addition to typical batch-processing. This is made possible by providing two new primitives.\nSpecifically,\n- **Update/Delete Records**: Hudi provides support for updating/deleting records, using fine grained file/record level indexes, while providing transactional guarantees for the write operation. Queries process the last such committed snapshot, to produce results.\n- **Change Streams**: Hudi also provides first-class support for obtaining an incremental stream of all the records that were updated/inserted/deleted in a given table, from a given point-in-time, and unlocks a new incremental-query category.\n\n![](/img/blog/2020-08-04-prestodb-and-hudi/HudiPrimitives.png)\nThe above diagram illustrates Hudi's primitives.\n\nThese primitives work closely hand-in-glove and unlock stream/incremental processing capabilities directly on top of DFS-abstractions. This is very similar to consuming events from a kafka-topic and then using a state-store to accumulate intermediate results incrementally.\nIt has several architectural advantages.\n- **Increased Efficiency**: Ingesting data often needs to deal with updates (resulting from database-change-capture), deletions (due to data-privacy-regulations) and enforcing unique-key-constraints (to ensure data-quality of event streams/analytics). However, due to lack of standardized support for such functionality, data engineers often resort to big batch jobs that reprocess entire day's events or reload the entire upstream database every run, leading to massive waste of computational-resources. Since Hudi supports record level updates, it brings an order of magnitude improvement to these operations, by only reprocessing changed records and rewriting only the part of the table that was updated/deleted, as opposed to rewriting entire table-partitions or even the entire table.\n- **Faster ETL/Derived Pipelines**: A ubiquitous next step, once the data has been ingested from external sources is to build derived data pipelines using Apache Spark/Apache Hive, or any other data processing framework, to ETL the ingested data for a variety of use-cases like data-warehousing, machine-learning-feature-extraction, or even just analytics. Typically, such processes again rely on batch-processing jobs, expressed in code or SQL, that process all input data in bulk and recompute all the output results. Such data pipelines can be sped up dramatically, by querying one or more input tables using an incremental-query instead of a regular snapshot-query, resulting in only processing the incremental changes from upstream tables, and then upsert or delete on the target derived table, as depicted in the first diagram.\n- **Access to fresh data**:  It's not everyday we will find that reduced resource usage also results in improved performance, since typically we add more resources (e.g memory) to improve performance metric (e.g query latency). By fundamentally shifting away from how datasets have been traditionally managed, for what may be the first time since the dawn of the big data era, Hudi realizes this rare combination. A sweet side-effect of incrementalizing batch-processing is that the pipelines also take a much smaller amount of time to run. This puts data into hands of organizations much more quickly than has been possible with data-lakes before.\n- **Unified Storage**: Building upon all the three benefits above, faster and lighter processing right on top of existing data-lakes mean lesser need for specialized storage or data-marts, simply for purposes of gaining access to near real-time data.\n\n## Types of Hudi tables and queries\n\n### Table Types\nHudi supports the following table types.\n\n**Copy On Write (COW)**: Stores data using exclusively columnar file formats (e.g parquet). Updates version & rewrites the files by performing a synchronous merge during write.\n\n**Merge On Read (MOR)**: Stores data using file versions with combination of columnar (e.g parquet) + row based (e.g avro) file formats. Updates are logged to delta files & later compacted to produce new versions of columnar files synchronously or asynchronously.\n\nThe following table summarizes the trade-offs between these two table types.\n\n\n| Trade-off           | CopyOnWrite                     | MergeOnRead     |\n| ---------           | -----------                     | -----------     |\n| Data Latency        | Higher                          | Lower           |\n| Update cost (I/O)   | Higher (rewrite entire parquet) | Lower (append to delta log) |\n| Parquet File Size   | Smaller (high update (I/0) cost) | Larger (low update cost) |\n| Write Amplification | Higher                          | Lower (depending on compaction strategy) |\n\n### Query types\nHudi supports the following query types.\n\n***Snapshot Queries***: Queries see the latest snapshot of the table as of a given commit or compaction action. In case of merge-on-read table, it exposes near-real time data (few mins) by merging the base and delta files of the latest file version on-the-fly. For copy-on-write tables, it provides a drop-in replacement for existing parquet tables, while providing upsert/delete and other write side features.\n\n***Incremental Queries***: Queries only see new data written to the table since a given commit/compaction. This effectively provides change streams to enable incremental data pipelines.\n\n***Read Optimized Queries***: Queries see the latest snapshot of a table as of a given commit/compaction action. Exposes only the base/columnar files in latest file versions and guarantees the same columnar query performance compared to a non-hudi columnar table.\n\nThe following table summarizes the trade-offs between the different query types.\n\n| Trade-off     | Snapshot | Read Optimized |\n| ---------     | -------- | -------------- |\n| Data Latency  | Lower | Higher |\n| Query Latency | *COW*: Same as query engine on plain parquet.  *MOR*: Higher (merge base / columnar file + row based delta / log files) | Same columnar query performance as COW Snapshot queries | \n\nThe following animations illustrate a simplified version of how inserts/updates are stored in a COW and a MOR table along with query results along the timeline. Note that X axis indicates the timeline and query results for each query type.\n\n\n![](/img/blog/2020-08-04-prestodb-and-hudi/HudiCOW.gif)\n\nNote that the table’s commits are fully merged into the table as part of the write operation. For updates, the file containing the record is re-written with new values for all records that are changed. For inserts, the records are first packed onto the smallest file in each partition path, until it reaches the configured maximum size. Any remaining records after that, are again packed into new file id groups, again meeting the size requirements.\n\n![](/img/blog/2020-08-04-prestodb-and-hudi/HudiMOR.gif)\n\nAt a high level, MOR writer goes through the same stages as COW writer in ingesting data. The updates are appended to the latest log (delta) file belonging to the latest file version without merging. For inserts, Hudi supports 2 modes:\n- Inserts to log files - This is done for tables that have an indexable log files (for e.g. hbase index or the upcoming record level index)\n- Inserts to parquet files - This is done for tables that do not have indexable log files, for example bloom index\n\nAt a later time, the log files are merged with the base parquet file by compaction action in the timeline. This table type is the most versatile, highly advanced and offers much flexibility for writing (ability to specify different compaction policies, absorb bursty write traffic etc) and querying (e.g: tradeoff data freshness and query performance). At the same time, it can involve a learning curve for mastering it operationally. \n\n## Early Presto integration\n\nHudi was designed in mid to late 2016. At that time, we were looking to integrate with query engines in the Hadoop ecosystem. To achieve this in Presto, we introduced a custom annotation - `@UseFileSplitsFromInputFormat`, as suggested by the community. Any Hive registered table if it has this annotation would fetch splits by invoking the corresponding inputformat’s `getSplits()` method instead of Presto Hive’s native split loading logic. For Hudi tables queried via Presto this would be a simple call to `HoodieParquetInputFormat.getSplits()`. This was a straightforward and simple integration. All one had to do was drop in the corresponding Hudi jars under `<presto_install>/plugin/hive-hadoop2/` directory. This supported querying COW Hudi tables and read optimized querying of MOR Hudi tables (only fetch data from compacted base parquet files). At Uber, this simple integration already supported over 100,000 Presto queries per day from 100s of petabytes of data (raw data and modeled tables) sitting in HDFS ingested using Hudi. \n\n## Moving away from InputFormat.getSplits()\n\nWhile invoking `inputformat.getSplits()` was a simple integration, we noticed that this could cause a lot of RPC calls to namenode. There were several disadvantages to the previous approach. \n\n1. The `InputSplit`s returned from Hudi are not enough. Presto needs to know the file status and block locations for each of the `InputSplit` returned. So this added 2 extra RPCs to the namenode for every split times the number of partitions loaded. Occasionally, backpressure can be observed if the namenode is under a lot of pressure.\n2. Furthermore, for every partition loaded (per `loadPartition()` call) in Presto split calculation, `HoodieParquetInputFormat.getSplits()` would be invoked. That caused redundant Hoodie table metadata listing, which otherwise can be reused for all partitions belonging to a table scanned from a query. \n\nThis led us to rethink the Presto-Hudi integration. At Uber, we changed this implementation by adding a compile time dependency on Hudi and instantiated the `HoodieTableMetadata` once in the `BackgroundHiveSplitLoader` constructor. We then leveraged Hudi Library APIs to filter the partition files instead of calling `HoodieParquetInputFormat.getSplits()`. This gave a significant reduction in the number of namenode calls in this path. \n\nTowards generalizing this approach and making it available for the Presto-Hudi community, we added a new API in Presto’s `DirectoryLister` interface that would take in a `PathFilter` object. For Hudi tables, we supplied this PathFilter object - [HoodieROTablePathFilter](https://github.com/apache/incubator-hudi/blob/master/hudi-hadoop-mr/src/main/java/org/apache/hudi/hadoop/HoodieROTablePathFilter.java), which would take care of filtering the files that Presto lists for querying Hudi tables and achieve the same results as Uber’s internal solution.\n\nThis change is available since the 0.233 version of Presto and depends on the 0.5.1-incubating Hudi version. Since Hudi is now a compile time dependency it is no longer necessary to provide Hudi jar files in the plugin directory.\n\n## Presto support for querying MOR tables in Hudi\nWe are starting to see more interest among the community to add support for snapshot querying of Hudi MOR tables from Presto. So far, from Presto, only read optimized queries (pure columnar data) are supported on Hudi tables. With this PR in progress - [https://github.com/prestodb/presto/pull/14795](https://github.com/prestodb/presto/pull/14795) we are excited that snapshot querying (aka real time querying) of Hudi MOR tables will be available soon. This would make fresher data available for querying by merging base file (Parquet data) and log files (Avro data) at read time.\n\nIn Hive, this can be made available by introducing a separate `InputFormat` class that provides ways to deal with splits and a new `RecordReader` class that can scan the split to fetch records. For Hive to query MOR Hudi tables there is already such classes available in Hudi:\n- `InputFormat` - `org.apache.hudi.hadoop.realtime.HoodieParquetRealtimeInputFormat`\n- `InputSplit` - `org.apache.hudi.hadoop.realtime.HoodieRealtimeFileSplit`\n- `RecordReader` - `org.apache.hudi.hadoop.realtime.HoodieRealtimeRecordReader`\nSupporting this in Presto involves understanding how Presto fetches records from Hive tables and making necessary changes in that layer. Because Presto uses its native `ParquetPageSource` rather than the record reader from the input format, Presto would only show the base parquet files, and not show the real time upserts from Hudi's log files which are avro data (essentially the same as a normal read-optimized Hudi query).\n\nTo allow Hudi real time queries to work, we identified and made the following separate necessary changes:\n1. Add extra metadata field to serializable `HiveSplit` to store Hudi split information.\nPresto-hive converts its splits into the serializable `HiveSplit` to pass around. Because it expects standard splits, it will lose the context of any extra information contained in complex splits extended from `FileSplit`. Our first thought was to simply add the entire complex split as an extra field of `HiveSplit`. This didn't work however as the complex splits were not serializable and this would also duplicate the base split data.\n\n    Instead we added a `CustomSplitConverter` interface. This accepts a custom split and returns an easily serializable String->String map containing the extra data from the custom split. To complement this, we also added this Map as an additional field to Presto's `HiveSplit`. We created the `HudiRealtimeSplitConverter` to implement `CustomSplitConverter` interface for Hudi real time queries.\n\n2. Recreate Hudi split from `HiveSplit`'s extra metadata.\nNow that we have the full information of the custom split contained in `HiveSplit`, we need to identify and recreate the `HoodieRealtimeFileSplit` just before reading the split. The same `CustomSplitConverter` interface has another method that takes a normal `FileSplit` + extra split information map and returns the actual complex FileSplit, in this case the `HudiRealtimeFileSplit`. This leads to our last and final change.\n\n3. Use `HoodieRealtimeRecordReader` from `HoodieParquetRealtimeInputFormat` to read recreated `HoodieRealtimeFileSplit`.\nPresto needs to use the new record reader to properly handle the extra information in the `HudiRealtimeFileSplit`. To do this we introduced another annotation `@UseRecordReaderFromInputFormat` similar to the first annotation. This signals Presto to use the Hive record cursor (which uses the record reader from the input format) instead of the page source. The Hive record cursor sees the recreated custom split and sets additional information/configs based on the custom split.\n\nWith these changes, Presto users should be able to access more real time data on Hudi MOR tables.\n\n## What’s next?\nFollowing are some interesting efforts (we call them [RFCs](https://cwiki.apache.org/confluence/display/HUDI/RFC+Process)) we are looking into that may need support in Presto. \n\n**[RFC-12: Bootstrapping Hudi tables efficiently](https://cwiki.apache.org/confluence/display/HUDI/RFC+-+12+%3A+Efficient+Migration+of+Large+Parquet+Tables+to+Apache+Hudi)**\n\nApache Hudi maintains per record metadata that enables us to provide record level upserts, unique key semantics and database-like change streams. However, this meant that, to take advantage of Hudi’s upsert and incremental processing support, users would need to rewrite their whole dataset to make it an Apache Hudi table. This [RFC](https://cwiki.apache.org/confluence/display/HUDI/RFC+-+12+%3A+Efficient+Migration+of+Large+Parquet+Tables+to+Apache+Hudi) provides a mechanism to efficiently migrate their datasets without the need to rewrite the entire dataset, while also providing the full capabilities of Hudi. \n\nThis will be achieved by having mechanisms to refer to the external data files (from the source table) from within the new bootstrapped Hudi table. With the possibility of data residing in an external location (bootstrapped data) or under Hudi table’s basepath (recent data),   `FileSplit`s would require to store more metadata on these. This work would also leverage and build upon the Presto MOR query support we are adding currently.\n\n**[Support Incremental and point in time time-travel queries on Hudi tables](https://issues.apache.org/jira/browse/HUDI-887)**\n\nIncremental queries allow us to extract change logs from a source Hudi table. Point in time queries allows for getting the state of a Hudi table between time T1 and T2. These are supported in Hive and Spark already. We are looking into supporting this feature in Presto as well. \n\nIn Hive incremental queries are supported by setting few configs in `JobConf` like for example - query mode to `INCREMENTAL`, starting commit time and max number of commits to consume. In Spark, there is a specific implementation to support incremental querying - `IncrementalRelation`. To support this in Presto, we need a way to identify the incremental query. Given Presto does not pass arbitrary session configs to the hadoop configuration object, an initial idea is to register the same table in the metastore as an incremental table. And then use query predicates to get other details such as starting commit time, max commits etc.\n\n**[RFC-15: Query planning and listing improvements](https://cwiki.apache.org/confluence/display/HUDI/RFC+-+15%3A+HUDI+File+Listing+and+Query+Planning+Improvements)**\n \nHudi  write client and Hudi queries need to perform `listStatus` operation on the file system to get a current view of the file system. While at Uber, the HDFS infrastructure was [heavily optimized](https://eng.uber.com/scaling-hdfs/) for listing, this can be an expensive op for large datasets containing thousands of partitions and each partition having thousands of files on cloud/object storage. The above RFC work aims at eliminating list operation and providing better query performance and faster lookups, by simply incrementally compacting Hudi’s timeline metadata into a snapshot of a table’s state at that instant.\n\nThe solutions here aim at \n- Ways for storing and maintaining metadata of the latest list of files. \n- Maintaining stats on all columns of a table to aid effective pruning of files before scanning. This can be leveraged in the query planning phase of the engine.\n\n\nTowards this, Presto would need some changes too. We are actively exploring ways to leverage such metadata in the query planning phase. This would be a significant addition to Presto-Hudi integration and would push the query latencies further down.\n\n**[Record Level Indexes](https://cwiki.apache.org/confluence/display/HUDI/RFC+-+08+%3A+Record+level+indexing+mechanisms+for+Hudi+datasets)**\n\nUpsert is a popular write operation on Hudi tables that relies on indexing to tag incoming records as upserts. HoodieIndex provides a mapping of a record id to a file id in both a partitioned or a non-partitioned dataset, with implementations backed by BloomFilters/ Key ranges (for temporal data), and also Apache HBase (for random updates). Many users find Apache HBase (or any such key-value store backed index) expensive and adding to operational overhead. \nThis effort tries to come up with a new index format for indexing at record level, natively within Hudi. Hudi would store and maintain the record level index (backed by pluggable storage implementations such as HFile, RocksDB). This would be used by both the writer (ingestion) and readers (ingestion/queries) and would significantly improve upsert performance over join based approaches or even bloom index for random update workloads. This is another area where query engines could leverage this information when pruning files before listing them. We are also looking at a way to leverage this metadata from Presto when querying. \n\n## Moving forward\n\nQuery engines like Presto are the gateways for users to reap the strength of Hudi. With an ever growing community and active development roadmap there are many interesting projects in Hudi. As Hudi invests heavily into the projects above, there is only greater need to deeply integrate with systems like Presto. Towards that, we look forward to collaborating with the Presto community. We welcome suggestions, feedback and encourage you to make [contributions](https://github.com/apache/hudi/issues) and connect with us [here](https://hudi.apache.org/community.html)."
        },
        {
          "id": "/2020/07/17/alluxio-hybrid-cloud",
          "metadata": {
            "permalink": "/blog/2020/07/17/alluxio-hybrid-cloud",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-07-17-alluxio-hybrid-cloud.md",
            "source": "@site/blog/2020-07-17-alluxio-hybrid-cloud.md",
            "title": "Running Presto in a Hybrid Cloud Architecture",
            "description": "Migrating SQL workloads from a fully on-premise environment to cloud infrastructure has numerous benefits, including alleviating resource contention and reducing costs by paying for computation resources on an on-demand basis. In the case of Presto running on data stored in HDFS, the separation of compute in the cloud and storage on-premises is apparent since Presto’s architecture enables the storage and compute components to operate independently. The critical issue in this hybrid environment of Presto in the cloud retrieving HDFS data from an on-premise environment is the network latency between the two clusters.",
            "date": "2020-07-17T00:00:00.000Z",
            "formattedDate": "July 17, 2020",
            "tags": [],
            "readingTime": 5.54,
            "truncated": true,
            "authors": [
              {
                "name": "Adit Madan",
                "url": "https://www.linkedin.com/in/aditm/"
              }
            ],
            "frontMatter": {
              "title": "Running Presto in a Hybrid Cloud Architecture",
              "author": "Adit Madan",
              "authorURL": "https://www.linkedin.com/in/aditm/"
            },
            "prevItem": {
              "title": "PrestoDB and Apache Hudi",
              "permalink": "/blog/2020/08/04/prestodb-and-hudi"
            },
            "nextItem": {
              "title": "Data Lake Analytics: Alibaba's Federated Cloud Strategy",
              "permalink": "/blog/2020/06/30/data-lake-analytics-blog"
            }
          },
          "content": "Migrating SQL workloads from a fully on-premise environment to cloud infrastructure has numerous benefits, including alleviating resource contention and reducing costs by paying for computation resources on an on-demand basis. In the case of Presto running on data stored in HDFS, the separation of compute in the cloud and storage on-premises is apparent since Presto’s architecture enables the storage and compute components to operate independently. The critical issue in this hybrid environment of Presto in the cloud retrieving HDFS data from an on-premise environment is the network latency between the two clusters.\n\nThis crucial bottleneck severely limits performance of any workload since a significant portion of its time is spent transferring the requested data between networks that could be residing in geographically disparate locations. As a result, most companies copy their data into a cloud environment and maintain that duplicate data, also known as Lift and Shift. Companies with compliance and data sovereignty requirements may even prevent organizations from copying data into the cloud. This approach is not scalable and requires introducing a lot of manual effort to achieve reasonable results. This article introduces [Alluxio](https://www.alluxio.io/) to serve as a [data orchestration](https://www.alluxio.io/data-orchestration/) layer to help serve data to Presto efficiently, as opposed to either directly querying the distant HDFS cluster or manually providing a localized copy of the data to Presto in a cloud cluster.\n\n<!--truncate-->\n\n## Hybrid Cloud Architecture with Alluxio and Presto\n\nIn the following architecture diagram, both Presto and Alluxio processes are co-located in the cloud cluster. As far as Presto is concerned, it is querying for and writing data to Alluxio as if it were a co-located HDFS cluster. When Alluxio receives a request for data, it fetches the data from the remote HDFS cluster initially, but subsequent requests will be served directly from its cache. When Presto sends data to be persisted into storage, Alluxio asynchronously writes data to HDFS, freeing the Presto workload from needing to wait for the remote write to complete. In both read and write scenarios, with the exception of the initial read, a Presto workload is able to run at the same, if not faster, performance as if it were in the same network as the HDFS cluster. Note that besides the deployment and configuration of Alluxio and establishing the connection between Presto and Alluxio, there is no additional configuration or other manual efforts needed to maintain the hybrid environment.\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/PrestoAlluxioHadoop.png)\n\n## Benchmarking Performance\n\nFor benchmarking, we run SQL queries on data in a geographically separated Hive and HDFS cluster.\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/VPCPeering.png)\n\nThe hybrid cloud environment used for experimentation in this section includes two Amazon EMR clusters in different AWS regions. Because the two clusters are geographically dispersed, there is noticeable network latency between the clusters. [VPC peering](https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html) is used to create VPC connections to allow traffic between the two VPCs over the global AWS backbone with no bandwidth bottleneck. Readers can follow the [tutorial in the whitepaper](https://www.alluxio.io/resources/whitepapers/zero-copy-hybrid-cloud-for-data-analytics-strategy-architecture-and-benchmark-report/) to reproduce the benchmark results if using AWS as the cloud provider.\n\nWe used data and queries from the industry standard [TPC-DS](http://www.tpc.org/tpcds/) benchmark for decision support systems that examines large amounts of data and answers business questions. The queries can be categorized into the following classes (according to visualizations in this [repository](https://github.com/databricks/spark-sql-perf/blob/e9ef9788c2094aeb40c0f7d883b8c1cb0f852b74/src/main/notebooks/performance.dashboard.scala)): Reporting, Interactive, and Deep Analytics.\n\nWith Alluxio, we collected two numbers for all TPC-DS queries; denoted by **Cold** and **Warm**. \n- **Cold** is the case where data is not loaded in Alluxio storage before the query is run. In this case Alluxio fetches data from HDFS on-demand during query execution.\n- **Warm** is the case where data is loaded into Alluxio storage after the Cold run. Subsequent queries accessing the same data do not communicate with HDFS. \n\nWith HDFS, we collected two numbers as well; **Local** and **Remote**.\n- **Local** is the case where Presto and HDFS are co-located in the same region. This number shows us the performance of running the compute on-premises when data is local without bursting into the cloud.\n- **Remote** is the case where Presto reads from storage in another region.\n\n### TPC-DS Data Specificiations\n\n| Scale Factor | Format  | Compression | Data Size | Number of Files |\n| ------------ | ------- | ----------- | --------- | --------------- |\n| 1000         | Parquet | Snappy      | 463.5 GB  | 234.2 K         |\n\n### EMR Instance Specificiations\n\n| Instance Type | Master Instance Count | Worker Instance Count | Alluxio Storage Volume (us-west-1) | HDFS Storage Volume (ap-southeast-1) |\n| ------------- | --------------------- | --------------------- | ---------------------------------- | ------------------------------------ |\n| r5.4xlarge    | 1 each                | 10 each               | NVMe SSD                           | EBS                                  |\n\nWe compared the performance of Presto with Alluxio (Cold and Warm) with Presto directly on HDFS (Local and Remote). Benchmarking shows an average of **3x improvement** in performance with Alluxio when the cache is warm over accessing HDFS data remotely.\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/AlluxioWarmVsHdfsRemote.png)\n\nThe following table summarizes the results by class. Overall the maximum improvement seen with Alluxio was for q9 (7.1x) and the minimum was for q39a (1x - no difference).\n\nQuery Class: Reporting\nMax Improvement: q27 (3.1x)\nMin Improvement:  q43 (2.7x)\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/TpcdsReporting.png)\n\nQuery Class: Interactive\nMax Improvement: q73 (3.9x)\nMin Improvement:  q98 (2.2x)\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/TpcdsInteractive.png)\n\nQuery Class: Deep Analytics\nMax Improvement: q34 (4.2x)\nMin Improvement:  q59 (1.9x)\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/TpcdsDeepAnalytics.png)\n\nWith a 10 node compute cluster, the peak bandwidth utilization throughout running all the queries remained under 2Gbps when accessing data from the geographically separated cluster. Bandwidth was not the bottleneck with the AWS backbone network. As the utilization scales with the size of the compute cluster, a bandwidth bottleneck could be expected for larger clusters when not using Alluxio since the bandwidth available with Direct Connect may be limited.\n\nMost of the performance gain seen with Alluxio is explained by the latency difference for both metadata and data, when cached seamlessly into the localized Alluxio cluster.\n\n\n![](/img/blog/2020-07-17-alluxio-hybrid-cloud/TpcdsAll.png)\n\n## Conclusion\n\nA hybrid cloud architecture allows cloud computing resources to be used for data analytics, even if the data resides in a completely different network. In addition to achieving significantly better performance, the execution plan outlined does not require any significant reconfiguration of the on-premise infrastructure. Since users can harness the compute power of a public cloud, this opens up more opportunities for Presto to be utilized as a scalable and performant compute framework for analytics using data stored on-premises.\n\nAn in-depth whitepaper, [“Zero-Copy” Hybrid Cloud for Data Analytics - Strategy, Architecture, and Benchmark Report](https://www.alluxio.io/resources/whitepapers/zero-copy-hybrid-cloud-for-data-analytics-strategy-architecture-and-benchmark-report/), was originally published by Alluxio on the Alluxio Engineering Blog on April 6, 2020. Check out the [blogs](https://www.alluxio.io/blog/) for more articles about Alluxio’s engineering work and join Alluxio Open Source community on [Slack](http://alluxio-community.slack.com) for any questions you might have."
        },
        {
          "id": "/2020/06/30/data-lake-analytics-blog",
          "metadata": {
            "permalink": "/blog/2020/06/30/data-lake-analytics-blog",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-06-30-data-lake-analytics-blog.md",
            "source": "@site/blog/2020-06-30-data-lake-analytics-blog.md",
            "title": "Data Lake Analytics: Alibaba's Federated Cloud Strategy",
            "description": "Presto is known to be a high-performance, distributed SQL query engine for Big Data. It offers large-scale data analytics with multiple connectors for accessing various data sources. This capability enables the Presto users to further extend some features to build a large-scale data federation service on cloud.",
            "date": "2020-06-30T00:00:00.000Z",
            "formattedDate": "June 30, 2020",
            "tags": [],
            "readingTime": 6.255,
            "truncated": true,
            "authors": [
              {
                "name": "George Wang",
                "url": "https://www.linkedin.com/in/george-wang-9a5a46190/"
              }
            ],
            "frontMatter": {
              "title": "Data Lake Analytics: Alibaba's Federated Cloud Strategy",
              "author": "George Wang",
              "authorURL": "https://www.linkedin.com/in/george-wang-9a5a46190/"
            },
            "prevItem": {
              "title": "Running Presto in a Hybrid Cloud Architecture",
              "permalink": "/blog/2020/07/17/alluxio-hybrid-cloud"
            },
            "nextItem": {
              "title": "Improving Presto Latencies with Alluxio Data Caching",
              "permalink": "/blog/2020/06/16/alluxio-datacaching"
            }
          },
          "content": "Presto is known to be a high-performance, distributed SQL query engine for Big Data. It offers large-scale data analytics with multiple connectors for accessing various data sources. This capability enables the Presto users to further extend some features to build a large-scale data federation service on cloud. \n\nAlibaba Data Lake Analytics embraces Presto’s federated query engine capability and has accumulated a number of successful business use cases that signify the power of Presto's analytics capability.\n\n\n\n<!-- truncate -->\n\n## Data Federation Service\n\nToday’s cloud databases empower cloud users to easily and directly interact with other cloud products and services, available to be built for their own cloud data lake analytic scenarios and solutions. The architecture to pile up a cloud-based data lake analytic scenario is divided into three layers:\n\n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/threeLayerCloud_v1.png)\n\n**First Layer**: A variety of selected cloud native data storage and database services. \n\nUsers store and place their data on a series of cloud-native data storage services and cloud databases systems. Cloud data sources vary depending on different business use cases and business scenarios, but speaking in general, there are two cloud data storage approaches: NoSQL and relational databases. Various choices of storage service may include: cost-effective object cloud storage which may store files, structured, semi-structured, unstructured raw data, multimedia files; NoSQL, RDS, and other cloud database services;\n\n\n**Second Layer**: Cloud native data lake analysis service layer.\n\nThis layer emphasizes a very important feature in Cloud Native computing, that is, Serverless is server-free and is the basic service for building SaaS on the cloud, except for Data Lake analysis scenarios, there is also a more ubiquitous serverless PaaS and Function-computing as-a-Service (FaaS) such as Alibaba Cloud Function Calculation. This article focuses on the serverless analysis capability of Data Lake analytic scenarios.\n\n1. Elasticity - This allows an on-demand flexibility capability, which means being flexible in time, predictable and intelligent mixed load handling ability\n\n2. Resiliency -  This is a typical feature for High Availability systems. When failures occurs, the failover process moves processing performed by the failed component to the backup component within a matter of microseconds. This mechanism becomes transparent to users for a better user experience in a HA system in both Rolling Upgrade and Disaster Recovery across AZ (availability zones) \n\n3. Federated and Analytical - Multi-mode federated analysis capabilities, includes analysis and integration capabilities for multiple formats and systems, comprehensive analysis functions and excellent interactive analysis performance and experience, parallel computing processing capabilities, compatible interface capability and so on.\n\n\n\n**Third layer**: Data analysis application and visualization application layer\n\nBusiness logic based upon Data Lake analysis service layer may require an integration of data analysis tools on the cloud. All kinds of visualized data analysis products and tools are available to be interacted with users.\n\nUpon the basis of the above cloud data lake analysis architecture, the data federation ecosystem is divided into three layers accordingly, for which Data Lake Analytics plays a key role for serverless cloud-native computing framework:\n\n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/dla_three_tier.png)\n\n\n\n\n\n## Data Lake Analytics\n### Introducing DLA \n\nData Lake Analytics, known as DLA, is a large scale serverless data federation service on Alibaba Cloud. It is one of the most popular serverless SQL engines based on a well-customized computing engine from PrestoDB. DLA integrates with mainstream data sources and provides easy-to-use MySQL JDBC/ODBC connection protocol to allow users to interact with.\n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/DLA_architecture.png)\n\nFrontNode is the SQL access layer which provides MySQL protocol to let users interact with. MySQL is a flexible and compatible connection protocol service, frequently used by DLA users. Unified Meta Layer is a metadata repository which holds all metadata for all data sources DLA can support. DLA is running as an independent project which was forked from PrestoDB(0.227) and is now integrated with mainstream data sources in Alibaba Cloud. It is currently working closely with the PrestoDB community to track ongoing projects and updates. It is mission critical to continue the close partnership with PrestoDB for business success with latest updates.\n\n\n### DLA Product Key Features:\n\n1. Serverless\na. No infrastructure and management cost\nb. Zero start-time\nc. Transparent upgrade\nd. QoS Elastic Service\n2. Standard SQL Connections\na. Compatible with standard SQL Compliance\nb. Rich built-in function support\nc. JDBC/ODBC support\nd. Compatible with BI tools\n3. Heterogeneous Data Sources\na. Enable OSS Data analysis requirements\nb. Enable Table Store Data for SQL querying service\nc. Federated data analysis across multiple database instances\nd. Ease of interconnection analysis for multiple data sources\ne. Any forms of analytics in “Data Lake”\n4. Optimized Compute Engine\na. Support for unstructured data\nb. Vectorized execution and optimization\nc. Operator pipeline optimization\nd. Resource isolation and prioritization\n\n\n\n## DLA Use Cases\n\n\n### Typical Data Flow Scenarios:\n\nGenerally DLA empowers cloud users analytic capability in 3 typical user data flow scenarios:\n1. The cloud user uploads the business generated data such as Log, CSV, JSON and other files, directly to OSS (AliCloud Object Storage Service), and then uses DLA to directly point to the file or folder for which the table is created to query. Then the user may use BI tools to analyze and visualize business data analytics.\n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/case1.png)\n\n2. The cloud user can directly copy and upload data to OSS, and then use DLA to directly point files or folders for which the table is created to query. Common data formats like Parquet, ORC, RCFile, Avro, and others are supported as well. Then the user may use BI tools for business data analysis and visualization. \n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/case2.png)\n\n3. In order to provide better query performance and low storage cost for subsequent data analysis on OSS, data can be converted into Parquet or ORC format to improve the cost performance of repeated data analysis.\n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/case3.png)\n\n\n### Alibaba Use Case Studies: \n\n#### Customer Case #1: Log Data Analytics\n\nCloud customers generate log data which is stored on OSS. The challenge is that those log data are unstructured and typically on the order of Petabytes of data size. \n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/case1_logAnalysis.png)\n\nSo in order to make it available to analyze, DLA’s massive parallel processing engine is the key to business success. It allows users to do full-volume, personalized log data analysis by using DLA’s Hive connector. Additionally log data can be synchronized to OSS with a max delay of 5 minutes with Alibaba Cloud SLS that can feed data to DLA in near real-time.\n\n\n#### Customer Case #2: Genetic Data Analytics\n\n\nOne of Alicloud’s data vendors is in the Genetic engineering industry. The data volume is gigantic and the data is stored in various data sources. This is raw bioinformatics data storing gene sequence variations in the Variant Call Format(VCF). The challenge is that Genetic datasets are huge, but still need to be analyzed promptly. \n\n![](/img/blog/2020-06-30-data-lake-analytics-blog/case2_gene_data_analytics.png)\n\nThe solution is to place a huge volume gene data on OSS where storage costs are low and materialize indexing data to a Key value store system. Aliyun Open Table Service (OTS) is a distributed NoSQL database that works well for this propose. DLA's cross data source analytic capabilities can then be used to join the data together across services making it possible to quickly access and analyze the data even on large scale datasets.\n\n\n\n# Conclusion\n\nAlibaba Data Lake Analytics embraces Presto’s federated query engine capability and has empowered many Alibaba cloud users to experience large scale serverless cloud service. DLA is working to make the cloud user experience more transparent by improving ease of use and reducing infrastructure management overhead. The next blog will discuss some key technical developments on top of the Presto codebase."
        },
        {
          "id": "/2020/06/16/alluxio-datacaching",
          "metadata": {
            "permalink": "/blog/2020/06/16/alluxio-datacaching",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-06-16-alluxio-datacaching.md",
            "source": "@site/blog/2020-06-16-alluxio-datacaching.md",
            "title": "Improving Presto Latencies with Alluxio Data Caching",
            "description": "Facebook: Rohit Jain, James Sun, Ke Wang, Shixuan Fan, Biswapesh Chattopadhyay, Baldeep Hira",
            "date": "2020-06-16T00:00:00.000Z",
            "formattedDate": "June 16, 2020",
            "tags": [],
            "readingTime": 9.365,
            "truncated": true,
            "authors": [
              {
                "name": "Rohit Jain",
                "url": "https://www.linkedin.com/in/jain-rohit/"
              }
            ],
            "frontMatter": {
              "title": "Improving Presto Latencies with Alluxio Data Caching",
              "author": "Rohit Jain",
              "authorURL": "https://www.linkedin.com/in/jain-rohit/",
              "authorFBID": 514987722
            },
            "prevItem": {
              "title": "Data Lake Analytics: Alibaba's Federated Cloud Strategy",
              "permalink": "/blog/2020/06/30/data-lake-analytics-blog"
            },
            "nextItem": {
              "title": "Spatial Joins 1: Local Spatial Joins",
              "permalink": "/blog/2020/05/07/local-spatial-joins"
            }
          },
          "content": "**Facebook:** Rohit Jain, James Sun, Ke Wang, Shixuan Fan, Biswapesh Chattopadhyay, Baldeep Hira\n\n**Alluxio:** Bin Fan, Calvin Jia, Haoyuan Li\n\n\nThe Facebook Presto team has been collaborating with [Alluxio](https://www.alluxio.io/) on an open source data caching solution for Presto. \nThis is required for multiple Facebook use-cases to improve query latency for queries that scan data from remote sources such as HDFS. \nWe have observed significant improvements in query latencies and IO scans in our experiments. \n\n\n<!-- truncate -->\n\nWe found Alluxio data caching to be useful for multiple use-cases in the Facebook environment. \nFor one of the Facebook internal use cases we observed query latencies improved by **33%** (P50), **54%** (P75), and **48%** (P95). \nWe also recorded **57%** improvement in IO for remote data source scans.\n\n\n\n## Presto Architecture\n\nPresto's architecture allows storage and computation to scale independently. However, scanning data from remote storage can be a costly operation, \nand it makes achieving interactive query latency a challenge.\n\nPresto workers are responsible for executing query plan fragments on the data scanned from the independent and typically remote data sources. \nPresto workers do not store any data for remote data sources which enables the computation to grow elastically.\n\nThe architecture diagram below highlights the data read paths from a remote HDFS source. \nEach worker independently reads data from the remote data source. In this blog we will be only talking about optimizations done in the read \noperations from the remote data source.\n\n\n\n![](/img/blog/2020-06-16-alluxio-datacaching/presto_worker.jpg)\n\n\n\n\n\n## Presto + Data Caching Architecture\n\nTo solve sub-second latency use cases, we decided to implement various optimizations.  One important optimization was to implement a data cache. \nData caching has been a traditional optimization technique to bring the working dataset closer to the compute nodes and reduce trips to remote \nstorage to save latencies and IO.\n\nThe challenge was to make the data caching effective when petabytes of data get scanned from the remote data sources with no fixed pattern. \nAnother requirement for data caching to be effective was to achieve data affinity in a distributed environment like Presto.\n\nWith the addition of data caching, the Presto architecture looks like the following:\n\n\n![](/img/blog/2020-06-16-alluxio-datacaching/presto_worker_datacache.jpg)\n\n\nMore on this is covered in later sections.\n\n### Soft Affinity scheduling\n\nPresto’s current scheduler takes the worker load into account when distributing the splits, such scheduling strategy keeps the workload distribution uniform among workers. \nBut from the data locality perspective, it distributes splits randomly and not necessarily guarantees any affinity, which is required for any meaningful data caching effectiveness. \nIt is critical for the coordinator to leverage the same worker for a split which may contain the data for it in its cache.\n\n\n![](/img/blog/2020-06-16-alluxio-datacaching/presto_affinity_scheduler.jpg)\n\n\nThe above diagram illustrates how affinity scheduling distributes various splits among the workers.\n\n\nSoft affinity scheduling makes the best attempt to assign the same split to the same worker when doing the scheduling. The soft affinity scheduler uses the hash of a split to \nchoose a preferred worker for the split. Soft affinity scheduler:\n1. Computes a preferred worker for a split. If the preferred worker has resources available then it is assigned the split.\n2. If the preferred worker is busy then the coordinator chooses a secondary preferred worker, and assigns the split if resources are available.\n3. If the secondary preferred worker is also busy then the coordinator assigns the split to the least busy worker.\n\n![](/img/blog/2020-06-16-alluxio-datacaching/presto_affinity_scheduler_algo.jpg)\n\nThe definition of a busy node is defined by two configs: \n1. Max splits per node:  *node-scheduler.max-splits-per-node*\n2. Max pending splits per task: *node-scheduler.max-pending-splits-per-task*\n\nOnce the number of splits on one node exceeds one of the above configured limitations, this node would be treated as a busy node.\n\nAs it can be observed, node affinity is absolutely critical for cache effectiveness. Without node affinity, the same split may be processed by \ndifferent workers at different times, which can make caching the split data redundant. \n\nDue to this, if the affinity scheduler fails to assign the split to a preferred worker (because it was busy), it signals the assigned worker \nto not cache the split data. It means the worker would only cache the split data if it is the primary or secondary preferred worker for the split. \n\n### Alluxio data cache\n\nAlluxio file system is an open-source data orchestration system that is often used as a distributed caching service to Presto. \nTo achieve sub-second query latencies in our architecture, we want to further reduce the communication overhead between Presto and Alluxio. \nAs a result, core teams from Alluxio and Presto collaborated to carve out a single-node, embedded cache library from the Alluxio service.\n\nIn particular, a Presto worker queries this Alluxio local cache inside the same JVM through a standard HDFS interface. \nOn a cache hit, Alluxio local cache directly reads data from the local disk and returns the cached data to Presto; \notherwise, it retrieves data from the remote data source, and caches the data on the local disk for followup queries. \nThis cache is completely transparent to Presto. In case the cache runs into issues (e.g., local disk failures), the Presto reads \nfall back to the remote data source. This workflow is shown as the figure below.\n\n\n![](/img/blog/2020-06-16-alluxio-datacaching/presto_alluxio_caching.jpg)\n\n\n### Cache internals and configuration\nOur Alluxio data cache is a library residing in the Presto worker. It provides an HDFS-compatible interface “AlluxioCachingFileSystem” as the \nmain interface to Presto workers for all data access operations. \n\nThese are some design choices under the hood:\n\n#### Basic Caching Unit\nBoth Alluxio experience and earlier experiments from the Facebook team suggested that reading, writing and evicting data in a fixed block size is most efficient. \nIn the Alluxio system the default caching block size is 64MB. This is fairly large mostly to reduce the storage and service pressure on the metadata service.\nWe significantly reduce the caching granularity because our adaptation of the Alluxio data cache keep track of data and metadata locally. \nWe default the cache granularity to units of 1MB \"pages\".\n\n#### Cache location and hierarchy\nBy default, Alluxio local cache stores data into the local filesystem. Each caching page is stored as a separate file under a directory structure as \nfollows:\n\n*`<BASE_DIR>/LOCAL/1048576/<BUCKET>/<PAGE>`*\n\nHere:\n\n1. BASE_DIR is the root directory of the cache storage and is set by Presto configuration “cache.base-directory”.\n2. LOCAL means the cache storage type is LOCAL. Alluxio also supports RocksDB as the cache storage.\n3. 1048576: represents the 1MB block size.\n4. BUCKET represents a directory serving as buckets for various page files. They are created to make sure one single directory does not have too many \nfiles which often leads to really bad performance. \n5. PAGE represents the file named after the page ID. In presto the ID is the md5 hash of the filename. \n\n#### Thread Concurrency\n\nEach Presto worker keeps a set of threads, each executing different query tasks, but sharing the same data cache. Thus this Alluxio data cache \nis required to be highly concurrent across threads to deliver high throughput. Namely, this data cache allows multiple threads to fetch the \nsame page concurrently, while still ensuring thread-safety for evictions. \n\n#### Cache Recovery\n\nAlluxio local cache attempts to reuse cache data present in the local cache directory when a worker starts up (or restarts). \nIf the cache directory structure is compatible, it reuses the cache data. \n\n\n#### Monitoring\n\nAlluxio exports various JMX metrics while performing various caching related operations. System admins can also monitor the cache usage across the cluster easily.\n\n## Presto+Alluxio Benchmark\nWe benchmarked with queries from one of our production clusters, which was shadowed to the test cluster.\n\n**Query Count**: 17320\n\n**Cluster size**: 600 nodes\n\n**Max cache capacity per node**: 460GB\n\n**Eviction policy**: LRU\n\n**Cache data block size**: 1MB, meaning data is read, stored, and evicted in the 1 MB size.\n\n\n**Query Execution time improvement (in milliseconds)**:\n\n![](/img/blog/2020-06-16-alluxio-datacaching/query_latency.jpg)\n\nAs you can see, we observed significant improvements in the query latencies. \nWe observed 33% improvement in P50, 54% improvement in P75, and 48% improvement in P95.\n\n\n**IO Savings**\n\nData Size read for master branch run: **582 T Bytes**\n\nData Size read for caching branch run: **251 T Bytes**\n\nSavings in Scans: **57%**\n\n\n\n    \n**Cache hit rate**:\n\n![](/img/blog/2020-06-16-alluxio-datacaching/cache_hitrate.jpg)\n\nCache hit rate was pretty consistent and good during the experiment run. It remained mostly between 0.9 and 1. \nThere were a few dips that could be noticed, these can be the result of a new query scanning lots of new data. \nWe need to implement additional algorithms to prevent less frequent data blocks to get \ncached over more frequent data.\n\n## How to use it?\nIn order to use data caching the first thing we need to do is to enable soft affinity. \nData caching is not supported with random node scheduling.\n\nSet following configuration in the coordinator to enable soft affinity:\n\n*`\"hive.node-selection-strategy\", \"SOFT_AFFINITY”`*\n\nTo use the default (random) node scheduling, set it to \n\n*`\"hive.node-selection-strategy\", \"NO_PREFERENCE”`*\n\nUse the following configuration in the workers to enable Alluxio data caching\n1. Enable data caching in the worker => \"cache.enabled\", \"true\"\n2. Set the data caching type to Alluxio => \"cache.type\", \"ALLUXIO\"\n3. Set the base directory where the cache data would be stored => \"cache.base-directory\", \"file:///cache\"\n4. Set the max data capacity to be used by the cache per worker: \"cache.alluxio.max-cache-size\", \"500GB\"\n\nHere are some other configurations which can useful:\n\nCoordinator configuration (useful to configure the definition of a busy worker):\n1. Set max pending splits per task: node-scheduler.max-pending-splits-per-task\n2. Set max splits per node: node-scheduler.max-splits-per-node\n\n\nWorker configuration:\n1. Enable metrics for alluxio caching(default: true): cache.alluxio.metrics-enabled\n2. JMX class name used by the alluxio caching for metrics(default: alluxio.metrics.sink.JmxSink): cache.alluxio.metrics-enabled\n3. Metrics domain name used by the alluxio caching (default: com.facebook.alluxio): cache.alluxio.metrics-domain \n4. If alluxio caching should write to cache asynchronously(default: false): cache.alluxio.async-write-enabled \n5. If the alluxio caching should validate the provided configuration(default: false): cache.alluxio.config-validation-enabled \n\n\nAlluxio data caching exports various JMX metrics for its caching operations. A full list of metrics names can be found \n[here](https://github.com/Alluxio/alluxio/blob/e4adac3f5ca402760da757921b168b9846d2a280/core/common/src/main/java/alluxio/metrics/MetricKey.java#L1065).\n\n## What is next?\n1. Implement rate limiter to control cache write operations to avoid flash endurance issues.\n2. Implement semantic aware caching for better efficiency.\n3. Mechanism to clean cache directories for maintenance or a clean start.\n4. Ability to execute in dry run mode.\n5. Ability to enforce various capacity specifications, e.g. cache quota limit per table, cache quota limit per partition or cache quota limit per schema.\n6. More robust worker node scheduling mechanism. \n7. Implement additional algorithms to prevent less frequent data blocks to get cached over more frequent data.\n8. Fault tolerance: The current hash based node scheduling algorithm can run into issues when node count changes in a cluster. We are working on building more robust algorithms, such as consistent hashing.\n9. Better load balancing: When we take other more factors into account like split size, node resources, then we can better define a “busy” node and thus make more comprehensive decisions when it comes to load balancing.\n10. Affinity Criteria: Current affinity granularity is file level inside one presto cluster. If we are not able to achieve optimal performance under such a granularity standard, we might adjust our affinity criteria to be more fine-grained and find the balance between load balancing and good cache hit rate to achieve better overall performance.\n11. Improving resource utilization of Alluxio cache library."
        },
        {
          "id": "/2020/05/07/local-spatial-joins",
          "metadata": {
            "permalink": "/blog/2020/05/07/local-spatial-joins",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-05-07-local-spatial-joins.md",
            "source": "@site/blog/2020-05-07-local-spatial-joins.md",
            "title": "Spatial Joins 1: Local Spatial Joins",
            "description": "A common type of spatial query involves relating one table of geometric",
            "date": "2020-05-07T00:00:00.000Z",
            "formattedDate": "May 7, 2020",
            "tags": [],
            "readingTime": 8.775,
            "truncated": true,
            "authors": [
              {
                "name": "James Gill",
                "url": "https://www.linkedin.com/in/jagill/"
              }
            ],
            "frontMatter": {
              "title": "Spatial Joins 1: Local Spatial Joins",
              "author": "James Gill",
              "authorURL": "https://www.linkedin.com/in/jagill/"
            },
            "prevItem": {
              "title": "Improving Presto Latencies with Alluxio Data Caching",
              "permalink": "/blog/2020/06/16/alluxio-datacaching"
            },
            "nextItem": {
              "title": "Engineering SQL Support on Apache Pinot at Uber",
              "permalink": "/blog/2020/03/18/uber-pinot"
            }
          },
          "content": "A common type of spatial query involves relating one table of geometric\nobjects (e.g., a table `population_centers` with columns\n`population, latitude, longitude`) with another such table (e.g., a table\n`counties` with columns `county_name, boundary_wkt`), such as calculating\nfor each county the population sum of all population centers contained\nwithin it. These kinds of calculations are called _spatial joins_. While\ndoing it for a single row each from `population_centers` and `counties` is\nmanageable, doing it efficiently for two large tables is challenging. In this\npost, we'll talk about the machinery that Presto has built to make these\nqueries blazingly fast.\n\n<!-- truncate -->\n\n## Prologue: Point in Polygon\n\nHow do you test if a point is inside a polygon? A classic algorithm is the\n[winding-number test][winding-number-rule] which tests if a ray from the\npoint to `x = +infinity` intersects the polygon an even or odd number of\ntimes. For example, the ray from a point inside a circle would intersect a\ncircle once, while a ray from a point outside would intersect either 0 or 2\ntimes. The even-odd rule holds for more complex polygons as\nwell.[^even-odd-rule]\n\n![Winding number rule][winding-number-rule-image]\n\nWe'll skip the details of the algorithm, but the runtime complexity is\nimportant. Without a preprocessing step, we need to check if each side of the\npolygon intersects the ray. Since the number of sides is equal to the number\nof vertices, the algorithm runs in `O(V)` time where `V` is the number of\nvertices in the polygon.\n\n[^even-odd-rule]: There are many edge cases -- such as a ray that's tangent to\n  a polygon -- as well as important concepts of validity and simplicity in\n  polygons, as well as more robust algorithms, that we are omitting for\n  brevity.\n\n## Take 1: Double Loop\n\nA correct but very inefficient way of calculating a spatial join is a nested\n`for` loop, checking each row of `population_centers` if it is contained\nin each row of `county`. The algorithm will look something like this:\n\n```python\ndef spatial_join(population_centers, counties):\n  results = {}\n  for pop in population_centers:\n    for county in counties:\n      if not county.boundary.contains(pop.latitude, pop.longitude):\n        continue\n      if county.county_name not in results:\n        results[county.county_name] = 0.0\n      results[county.county_name] += p.population\n  return results\n```\n\nIf `P` is the number of points, and `C` is the number of counties, then this\nalgorithm will run in `O(P * C * V)` time, where `V` is the maximum number of\nvertices of any polygon. This is very expensive! Detailed polygons can easily\ncontain millions of vertices, some polygon sets (e.g., neighborhoods) can have\nmillions of entries, and some geospatial datasets have billions of points.\n\nBelow are the polygons for all 3108 counties in the continental United States.\nThey are comprised of almost 70k vertices.\n\n![US Counties][us-counties]\n\nRunning this on [some test data][github-repo]\ntakes 652.1 seconds to check ~1.5 million points against 3481 county polygons\n(some counties have multiple polygons).\n\n## Take 2: Envelopes\n\nAs humans, often we can quickly look at a point and see it is outside a\npolygon. For example, consider this point and the polygon of Beaverhead County:\n\n![Point and County][point-and-county]\n\nIt is far outside the polygon, so we don't have to check each side of the\npolygon. In fact, the polygon can be arbitrarily complex: as long as the\npoint is far away from the polygon, we can discard it as a possibility\nquickly. We can teach the computer to do this by using an _envelope_, which\nis an axis-oriented rectangle specified by minimum and maximum `x` and `y`\nvalues:\n\n![Point and County with envelope][point-and-county-envelope]\n\nThe envelope can be calculated in `O(V)` time, and can be done almost for\nfree when you are deserializing a geometry. Checking if a point is in an\nenvelope is `O(1)`:\n\n```\nenvelope.contains(point) ==\n                envelope.min_x <= point.x <= envelope.max_x\n                and envelope.min_y <= point.y <= envelope.max_y\n```\n\nWe can modify our algorithm above to take advantage of this fact:\n\n```python\ndef spatial_join(population_centers, counties):\n  results = {}\n  for pop in population_centers:\n    for county in counties:\n      if not county.envelope.contains(pop.latitude, pop.longitude):\n        # Bail quickly!\n        continue\n      if not county.boundary.contains(pop.latitude, pop.longitude):\n        continue\n      if county.county_name not in results:\n        results[county.county_name] = 0.0\n      results[county.county_name] += p.population\n  return results\n```\n\nWhile this does not change our worst-case runtime, it drastically reduces the\naverage runtime (since it removes the dependence on `V` for most checks).\nUsing county envelopes for our test data reduces the time to 13.8 seconds, an\nalmost 50x improvement!\n\nUsing an envelope is so cheap and effective that almost all geometric\nlibraries do an envelope pre-check before any relation check (containment,\nintersection, etc).\n\n## Take 3: Hierarchical Envelopes\n\nUsing envelopes makes the check for a given polygon cheaper on average, but\nwe still need to check each polygon. We can do better.\n\nAs humans, if we have polygons for each county, and a point in Massachusetts,\nwe know immediately that the point won't be in any county in California,\nOhio, or Florida. A computer version of this is to have a super-envelope for\neach of the states: for each state, find the maximum and minimum `x` and `y`.\nSince each county already has its envelope, the super-envelope can just be\nthe envelope of the envelopes. For our point in Massachusetts, we can first\ncheck the envelope for each state. If only the Massachusetts envelope\ncontains the point, we can then do an envelope check for each county in\nMassachusetts. Only for those counties that pass their envelope check do we\nneed to do the full containment check. This reduces our total envelope checks\nfrom 3108 (counties in the continental US) to 50 (states) plus 14 (counties\nin Massachusetts).\n\n![US States and Counties with Envelope][us-counties-and-states-with-boxes]\n\nAdding a check for states improves our performance to 3.4 seconds, about\n4x better than using county envelopes alone (and ~200x better than the\nbrute force calculation).\n\nWhile this is a great improvement, it leads to more questions. Since\nMassachussetts is in the far northeast of the USA, so why do we have to check\neach western and southern state? Why not also have envelopes for southern,\nwestern, central, eastern, and northern USA? Why stop at three levels? Maybe\nwe can even make envelopes for regions in Massachussetts. What about other\ndata sets without concepts like states and countries? Is there a programmatic\nway to generate the groups and levels?\n\nR-trees provide an answer for these questions.\n\n## Take 4: R-trees\n\nGiven a set of geometries, R-trees (Rectangle Trees) provide a programmatic\nway to construct an efficient set of hierarchical envelopes. R-trees start\nwith an envelope for each polygon in the data set. It groups sets of\nneighboring polygons, constructing an envelope for each group. The original\npolygons are leaf nodes in the tree, and the groups are their parent nodes.\nThe maximum group size depends on a parameter called the _branching factor_.\nThe grouping algorithm then recurses, making parent groups of child groups,\nconstructing an envelope for each, and so on until there is only one node,\nwhich encompasses all of our original geometries.\n\nIn the case of our counties, first we group them into groups of 9,\ncalculating the bounding box:\n\n![US Counties with level 1 Rtree boxes][usCountiesWithRtreeBoxes1]\n\nThen we group each of these level 1 boxes into groups of 9, calculating\n_their_ bounding box:\n\n![US Counties with level 2 Rtree boxes][usCountiesWithRtreeBoxes12]\n\nFinally, we repeat this again to create the level 3 boxes:\n\n![US Counties with level 3 Rtree boxes][usCountiesWithRtreeBoxes23]\n\nThe final node contains the bounding box for the whole continental USA.\n\nIn the example above, if we make an R-tree of the counties of the USA, we'd\nfirst check if the point is in the envelope of the root node. If not, the\npoint can't possibly be in any county, and we're done. If it is contained, we\ncan iterate through that node's children, recursing into any whose envelope\ncontains the point. Eventually we will have a (perhaps empty) subset of leaf\nnodes whose envelopes contain the point, and we can check those counties for\nproper containment.\n\nWhile the worst-case time complexity is actually worse than a linear scan\nthrough the geometries' envelopes, average complexity is `O(log C + M)`,\nwhere `C` is again the number of counties and `M` is the number of matching\ncounties (ie, counties whose envelopes contain the point). Then the time\ncomplexity for all `P` points is `O(P * log C + M * V)`, where `M` is total\nnumber of point-envelope matches, and `V` is the maximum number of vertices\nper polygon. This is a significant improvement when `C` is large.\n\nUsing an R-tree on the counties in our test data reduces the calculation to\njust 1.3s, which is ~2.5x better than the state envelopes, and about 500x\nfaster than the brute force calculation!\n\nR-trees can also help many other spatial queries about proximity. For example,\nif you want to check which polygons are within a certain distance of a point,\ninstead of querying the R-tree with a point, you can expand the point to an\nenvelope of the appropriate radius, and query the R-tree for envelope-envelope\nintersections, instead of point-envelope containment.\n\n## Local Spatial Joins\n\nWhen Presto executes a query like\n\n```sql\nSELECT county_name, SUM(population) AS total_population\nFROM population_centers\nJOIN counties\nON ST_Contains(ST_GeometryFromWkt(boundary_wkt), ST_Point(longitude, latitude))\nGROUP BY county_name\n```\n\nit will create an R-tree of the `counties` boundary geometries, and stream\nthe rows from `population_center`. For each row, it will query the R-tree for\ncounties whose envelopes contain the row's point, and for those candidates\nit will do a proper containment check against the boundary geometries. It\nwill then emit a row `county_name, population` for each match, to be later\naggregated over `county_name`.\n\nThis procedure works on a single machine, but how do we parallelize spatial\njoins? We'll examine that in a separate post on distributed spatial joins.\n\n## Acknowledgements\n\nThese visualizations were done in collaboration with\n[Jason Sundram](https://about.me/jsundram) in Facebook Boston's World AI team.\nThe starting point for our visualizations was\n[Mike Bostock](https://bost.ocks.org/mike/)'s\n[D3 US map](https://observablehq.com/@d3/u-s-map).\nFor the R-tree visualizations, I used\n[Vladimir Agafonkin](https://agafonkin.com/)'s \n[RBush](https://github.com/mourner/rbush),\nusing colors from [Carto](https://carto.com/carto-colors/).\nSpatial joins in Presto were primarily implemented by\n[Maria Basmanova](https://github.com/mbasmanova).\n\nCode for the performance profiling can be [found on GitHub][github-repo].\n\n<!-- Links -->\n\n[spatial-joins-distributed]: spatial-joins-distributed.html\n[winding-number-rule]: https://en.wikipedia.org/wiki/Point_in_polygon\n[github-repo]: https://github.com/jagill/presto_spatial_join_blog\n\n<!-- Images -->\n\n[winding-number-rule-image]: /img/blog/2020-05-07-local-spatial-joins/windingNumberTest.svg \"Illustration of winding number rule\"\n[point-and-county]: /img/blog/2020-05-07-local-spatial-joins/complexCountyMap.svg \"Beaverhead County\"\n[point-and-county-envelope]: /img/blog/2020-05-07-local-spatial-joins/complexCountyMapWithBox.svg \"Beaverhead County with Envelope\"\n[us-counties]: /img/blog/2020-05-07-local-spatial-joins/usCounties.svg \"US Counties\"\n[us-counties-with-boxes]: /img/blog/2020-05-07-local-spatial-joins/usCountiesWithBoxes.svg \"US Counties with Envelopes\"\n[us-counties-and-states-with-boxes]: /img/blog/2020-05-07-local-spatial-joins/usCountiesAndStatesWithBoxes.svg \"US Counties and States with Envelopes\"\n[usCountiesWithRtreeBoxes1]: /img/blog/2020-05-07-local-spatial-joins/usCountiesWithRtreeBoxes1.svg \"US Counties with level 1 Rtree boxes\"\n[usCountiesWithRtreeBoxes12]: /img/blog/2020-05-07-local-spatial-joins/usCountiesWithRtreeBoxes12.svg \"US Counties with level 2 Rtree boxes\"\n[usCountiesWithRtreeBoxes23]: /img/blog/2020-05-07-local-spatial-joins/usCountiesWithRtreeBoxes23.svg \"US Counties with level 3 Rtree boxes\""
        },
        {
          "id": "/2020/03/18/uber-pinot",
          "metadata": {
            "permalink": "/blog/2020/03/18/uber-pinot",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-03-18-uber-pinot.md",
            "source": "@site/blog/2020-03-18-uber-pinot.md",
            "title": "Engineering SQL Support on Apache Pinot at Uber",
            "description": "The article,  Engineering SQL Support on Apache Pinot at Uber, was originally published by Uber on the Uber Engineering Blog on January 15, 2020. Check out eng.uber.com for more articles about Uber's engineering work and follow Uber Engineering at @UberEng and Uber Open Source at @UberOpenSouce on Twitter for updates from our teams.",
            "date": "2020-03-18T00:00:00.000Z",
            "formattedDate": "March 18, 2020",
            "tags": [],
            "readingTime": 16.435,
            "truncated": true,
            "authors": [
              {
                "name": "Haibo Wang",
                "url": "https://www.linkedin.com/in/haibowangcmu/"
              }
            ],
            "frontMatter": {
              "title": "Engineering SQL Support on Apache Pinot at Uber",
              "author": "Haibo Wang",
              "authorURL": "https://www.linkedin.com/in/haibowangcmu/"
            },
            "prevItem": {
              "title": "Spatial Joins 1: Local Spatial Joins",
              "permalink": "/blog/2020/05/07/local-spatial-joins"
            },
            "nextItem": {
              "title": "Querying Nested Data with Lambda Functions",
              "permalink": "/blog/2020/03/02/presto-lambda"
            }
          },
          "content": "The article,  [Engineering SQL Support on Apache Pinot at Uber](https://eng.uber.com/engineering-sql-support-on-apache-pinot/), was originally published by Uber on the Uber Engineering Blog on January 15, 2020. Check out [eng.uber.com](https://eng.uber.com/) for more articles about Uber's engineering work and follow Uber Engineering at [@UberEng](https://twitter.com/UberEng) and Uber Open Source at [@UberOpenSouce](https://twitter.com/UberOpenSource) on Twitter for updates from our teams.\n\n![](/img/blog/2020-03-18-uber-pinot/shiny-thing.png)\n\nUber leverages real-time analytics on aggregate data to improve the user experience across our products, from [fighting fraudulent behavior](https://eng.uber.com/uber-eats-risk-team/) on Uber Eats to [forecasting demand](https://eng.uber.com/forecasting-introduction/) on our platform.\n\nAs Uber’s operations became more complex and we offered additional features and services through our platform, we needed a way to generate more timely analytics on our aggregated marketplace data to better understand how our products were being used. Specifically, we needed our Big Data stack to support cross-table queries as well as nested queries, both requirements that would enable us to write more flexible ad hoc queries to keep up with the growth of our business.\n\nTo resolve these issues, we built a solution that linked [Presto](http://prestodb.github.io/), a query engine that supports full ANSI SQL, and [Pinot](https://pinot.apache.org/), a real-time OLAP (online analytical processing) datastore. This married solution allows users to write ad-hoc SQL queries, empowering teams to unlock significant analysis capabilities.\n\nBy engineering full SQL support on Apache Pinot, users of our Big Data stack can now write complex SQL queries as well as join different tables in Pinot with those in other datastores at Uber. This new solution enables operations teams with basic SQL knowledge to build dashboards for quick analysis and reporting on aggregated data without having to spend extra time working with engineers on data modelling or building data pipelines, leading to efficiency gains and resource savings across the company.\n\n<!-- truncate -->\n\n# Challenges\n\nTimely, ad-hoc data analytics provide data scientists and operations teams at Uber with valuable information to make intelligent, data-driven decisions that benefit our users in real-time. Additionally, when operations teams request a metric that requires data across tables or other types of joins, engineers need to manually build a new metrics dashboard to satisfy this type of query. To facilitate this functionality in our Big Data stack, we needed a solution that could support querying near-real-time data with ad hoc ANSI SQL queries in our Apache Pinot datastores.\n\nPresto, which is widely used at Uber, is a distributed query engine that allows users to write SQL queries to access various underlying data stores. Presto offers full SQL support, but it generally doesn’t support real-time analytical datastores in an efficient manner, instead primarily querying tables in Hadoop, where data freshness is usually several hours old.\n\nTeams across Uber use Pinot to answer analytical queries with low query latency. However, the Pinot Query Language (PQL) lacks key functionalities, including nested queries, joins, and versatile UDF (e.g., regular expressions and geospatial functions). If users wanted to do anything more complicated, they had to spend time (upwards of several hours to a week) modeling data. Through our experience using these technologies separately, we realized that they actually complement each other quite well for conducting and storing ad-hoc data analytics. While Presto supports SQL, users cannot use it to access fresh aggregated data, and though Pinot can provide second-level data freshness, it lacks flexible query support. These discoveries are outlined in Figure 1, below:\n\n| Query Engine   | **Presto**                                                    | **Pinot**                      |\n| -------------- | ------------------------------------------------------------- | ------------------------------ |\n| Query latency  | Seconds to minutes                                            | **Millisec to seconds**        |\n| Query syntax   | **Flexible (ANSI-SQL)**                                       | Limited (no join, Limited UDF) |\n| Data freshness | Hours for derivative tables 10 min to > 1 hour for raw tables | **Seconds**                    |\n\n**Figure 1.** *Comparing Presto and Pinot’s query latency, query syntax, and data freshness reveals that these two query engines have compatible strengths. In terms of query latency, Presto lags behind at seconds to minutes, while Pinot excels, providing answers within milliseconds to seconds. On the other hand, Presto’s ANSI SQL is much more flexible, while Pinot’s query syntax is restricted by its lack of joins and limited UDF. Presto may return data that’s over an hour old to queries, while Pinot’s data refreshes in seconds.*\n\n## Our solution\n\nWe engineered a solution that allows Presto’s engine to query Pinot’s data stores in real time, optimized for low query latency. Our new system utilizes the versatile Presto query syntax to allow joins, geo-spatial queries, and nested queries, among other requests. In addition, it enables queries of data in Pinot with a freshness of seconds. With this solution, we further optimized query performance by enabling aggregate pushdown, predicates pushdown, and limit pushdown, which reduces unnecessary data transfer and improves query latency by more than 10x.\n\nThis solution enabled greater analytical capabilities for operations teams across Uber. Now, users can fully utilize the flexibility of SQL to represent more complex business metrics, and render query results into a dashboard using in-house tools. This capability has improved our operations efficiency and reduced operations cost.\n\n## Architecture\n\nWhile designing our new system, we first had to consider how we would modify Presto’s engine. A Presto cluster has two types of components: a coordinator and its workers. The coordinator is in charge of query parsing, planning, task scheduling, and distributing tasks to its group of workers. When the coordinator gives its workers an assignment, the workers fetch data from data sources through connectors and return the final result to the client.\n\n![](/img/blog/2020-03-18-uber-pinot/figure-2.jpg)\n\n**Figure 2.** *Uber’s Presto architecture incorporates one coordinator node and several worker nodes. After the coordinator receives and processes the query, it generates a query plan and distributes the tasks to its workers. Each worker scans a table scan from the underlying storage and sends the aggregated insights back to the user.*\n\nAs shown in Figure 2, above, Presto supports plugging in different storage engines, and allows the connector in each worker to fetch data from the underlying storage. Then, since Pinot can be used as storage, we can write a Pinot connector that supports fetching Pinot data through Presto workers.This functionality makes it possible to query Pinot data through Presto.\n\nBefore building the Pinot connector, it’s important to understand how Pinot works. A Pinot cluster has three main components: controllers, brokers, and servers. While controllers are in charge of node and task management, servers store and serve data. Each server contains a list of segments (in other words, shards) and each segment is a set of rows. Brokers receive queries, fetch data from servers, and return the final results to clients. Pinot servers can ingest data from Apache Kafka, a distributed real-time streaming platform, and the data can be queried as it is ingested, so the data freshness can be on the order of seconds.\n\n![](/img/blog/2020-03-18-uber-pinot/figure-3.jpg)\n\n**Figure 3.** *The Pinot architecture incorporates controllers, brokers, and servers. When the broker receives the query from the user, it receives a routing table from the controller. The routing table informs the broker where different segments are stored. The broker then fetches data from different servers in a scatter-gather manner, and finally returns the merged result.*\n\nAs shown in Figure 3, above, Pinot servers store different partitions of the data, and after getting data from each server, the broker merges the data and returns the final result. This workflow is similar to the Presto architecture, in which different workers fetch data before sending it to the aggregation worker. Based on Pinot’s data processing flow, building a Pinot connector in Presto seemed like a viable option for us.\n\n## Marrying Presto with Pinot\n\nTo merge the intuitive interface of Presto with the swift power of Pinot, we built a novel Pinot connector in Presto that allows Presto to query data with minimal latency, facilitating the complex queries of Presto's SQL support.\n\n![](/img/blog/2020-03-18-uber-pinot/figure-4.jpg)\n\n**Figure 4.** *Architecture of Presto-Pinot Connector. After the Coordinator receives the query from the user, it gets the routing table from the Pinot broker to find where each Pinot segment is stored. Then, it generates splits for each Presto worker to fetch Pinot data from the corresponding Pinot segments. Another Presto worker would aggregate the fetched data and return the final result to the user.*\n\nAs depicted in Figure 4, above, we combined Pinot’s scatter-gather query mode with Presto’s coordinator-worker architecture. When a user sends a Presto query for Pinot data with this new solution, Presto’s coordinator queries the Pinot broker to access Pinot’s routing table. The routing table contains information about which Pinot segments are stored on which Pinot servers. Next, Presto creates splits based on the routing table. The splits tell each worker a list of Pinot segments from which it should fetch data. Subsequently, each Presto worker simultaneously queries the underlying Pinot data in its assigned split, enabling aggregation and predicate pushdown when applicable. Finally, the aggregation worker aggregates the results from each split and returns the final result back to the user.\n\n### Our initial Pinot connector\n\nOur initial version of the Pinot connector treated Pinot as a database. Other open source Presto connectors, such as Presto-Cassandra, a Cassandra connector that allows querying Apache Cassandra through Presto, and Presto-Hive, a Hive connector that allows querying data in HDFS through Presto, operate this way, too.\n\n### Improving query performance\n\nAfter implementing the initial workflow, we discovered that our system was spending most of its query execution time on data transfer, especially as the data volume of the Pinot tables grew. A lot of data transferred by workers was discarded by the aggregation worker, and fetching unnecessary data both increased query latency and added extra workload for Presto workers and Pinot servers.\n\nTo address these issues and improve query performance, we implemented the following updates to our system:\n\n#### Predicate pushdown\n\nPredicates are Boolean-valued functions in the WHERE clause of the query. Predicates represent the allowed value range for specific columns. We implemented predicate pushdown, which means that the Presto coordinator would push the predicate down to Presto workers to do best-effort filtering when fetching data from Pinot. When Presto workers fetch records from Pinot servers, our system preserves the predicates of the query the workers are operating with. By applying the predicates in the user’s query in the Presto workers, our system fetches only the necessary data from Pinot. For example, if the predicate of the Presto query is WHERE city_id = 1, utilizing predicate pushdown would ensure that workers only fetch records from Pinot segments where city_id = 1. Without predicate pushdown, they will fetch all data from Pinot.\n\n#### Limit pushdown\n\nWe also implemented a limit pushdown for our system in order to further prevent unnecessary data transfer. Often, users do not need to query all rows of data in a given table, and this new functionality enables users to explore data on a much more limited (and less resource-intensive) scale. For instance, a user may only want to view the first ten rows of Pinot data; with this feature, the user can add LIMIT 10 in the query to sample just ten rows of the data. By applying limit pushdown, we ensure that when there are limit clauses (e.g., LIMIT 10) in the Presto query, we can apply the same limit when Presto workers fetch data from Pinot, preventing them from fetching all records.\n\n#### Aggregate pushdown\n\nSince many users apply aggregates like SUM/COUNT in their analytics queries, our new system facilitates aggregate pushdowns when relevant, allowing Pinot to perform various aggregations, including COUNT, MIN, MAX, and SUM.\n\nThe queries that users send to Presto coordinators already include aggregation requests. In order to provide aggregate pushdown, we pass this information to connectors with what we call aggregation hints. These are generated after query parsing and indicate the aggregations requested in each column. Then, when Presto workers fetch data from Pinot, they directly request the aggregated values and process them accordingly.\n\nDue to aggregate pushdown, our current system can:\n\n-   Utilize the functionality of Pinot to support aggregational queries with low query latency using Star-Tree.\n\n-   Reduce the number of rows needed from thousands to just one when passing aggregated results like COUNT and SUM from the Pinot server to Presto workers as one entry, greatly reducing query latency.\n\n-   Dramatically improve query performance by more than 10x, due to the reduction of the amount of data transferred between Presto workers and Pinot servers.\n\nWith the benefits of aggregate pushdown to reduce query latency fresh in our minds, let’s take a deeper look at how we engineered our system to enable aggregate pushdown for common aggregate functions.\n\n##### Pushing down MIN/MAX/SUM\n\nAggregations like MIN, MAX, and SUM are relatively straightforward to push down: we simply rewrite the Pinot query with the actual aggregation, so instead of fetching records, we can just request the MIN/MAX/SUM value in each split and get the result in one single row. In Presto’s architecture, each split returns a page which represents the data fetched from underlying storage. When the Presto aggregation worker processes this page, it treats each row in it as a record.\n\nFor example, imagine the Presto worker queries a Pinot segment with three records: 1, 10, and 100. Suppose the user wants to query the MAX of those records. When aggregate pushdown is not enabled, the Presto worker returns a page with three records: 1, 10, and 100. The aggregation worker computes the MAX of 1, 10, and 100, and returns 100 to the user. With aggregate pushdown, the Presto worker requests the MAX value directly from Pinot, and returns a page with one record of 100. The aggregation computes the max of 100 and returns the result to the user.\n\nIn Figure 5, below, we depict the workflow of the original Pinot connector, and in Figure 6, below, we compare it to our updated version of the tool:\n\n![](/img/blog/2020-03-18-uber-pinot/figure-5.png)\n\n**Figure 5.** *The original Pinot connector without aggregate pushdown received the query with aggregate functions (MAX and SUM). Each Presto worker fetches data from Pinot and constructs a page with all matching rows. The Presto aggregation worker then returns the aggregated results to the user.*\n\n![](/img/blog/2020-03-18-uber-pinot/figure-6.png)\n\n**Figure 6.** *The Pinot connector that supports aggregate pushdown (MIN, MAX, and SUM) passes the query with aggregate functions (MAX and SUM) to workers regarding which columns to aggregate on. Each worker will directly fetch the aggregated values (MAX and SUM) from Pinot, and construct a page with one value per aggregated column. The Presto aggregation worker then aggregates the returned rows and returns the final result to the user.*\n\nAs depicted in Figure 6, Presto workers in the revised Presto workflow now only fetch one row per segment instead of thousands of rows by utilizing more information from the original query about the requested aggregate functions. As a result, network transfers are significantly reduced between Presto workers and Pinot servers.\n\n##### Pushing down COUNT\n\nPushing down COUNT was not as simple as pushing down MIN, MAX, and SUM queries. Our initial architecture for the solution would not facilitate this query, and would pull inaccurate results. For instance, if our Pinot segment contained three values, 1, 10, and 100, pushing down COUNT in Pinot would return one row with a value of 3, indicating that there are three rows matching the original query. When Presto’s aggregation worker processed this page, it ignored whatever value was in that row, treated it as one row, and performed the COUNT, so the final result would be 1 instead of 3, the correct answer.\n\nIn order to solve this problem, we refactored the Presto page so that it can represent an aggregated page, and then refactored the page construction and processing flow accordingly. The refactored architecture not only gave Presto workers the flexibility to directly construct an aggregated page, but also enabled us to push down COUNT aggregation and support other more complex aggregations (like GROUP BY) in Presto as well.\n\n![](/img/blog/2020-03-18-uber-pinot/figure-7.png)\n\n**Figure 7.** *When the Presto connector receives the query with aggregate functions (COUNT/SUM), it will pass the information to workers on which columns to aggregate on. Each worker will directly fetch the aggregated values (COUNT/SUM) from Pinot, and construct a page with the one value per aggregated column, indicating the value is aggregated and should be directly used. The Presto aggregation worker would then directly merge each page and return the final result to the user.*\n\nWe have seen tremendous query latency improvements after introducing aggregate pushdown, which greatly reduced the time users waited for their query results, thus improving developer efficiency.\n\n### How our Presto-Pinot connector performs\n\nTo evaluate how well our new system works, we benchmarked query performance on Pinot data. We generated about 100 million rows in Parquet, ORC, and Pinot segments.\n\nWe set up Presto and Pinot clusters on the same SSD box (32 core Intel Xeon CPU E5-2620 v4 @ 2.10GHz, 256GB memory). Then we ran Presto queries to request data from Parquet and ORC on the local disk through our Presto-Hive connector, as well as querying Pinot data through our new Presto-Pinot connector. We also queried Pinot data through the Pinot broker directly.\n\nQuerying Pinot directly achieved the best query latency, as we expected. When querying through Presto, we found no significant performance differences between different data sources. We saw sporadic latency spikes when querying Parquet files, and other than that, querying Pinot had similar query latencies compared with querying Parquet and ORC files through the Hive connector.\n\n![](/img/blog/2020-03-18-uber-pinot/figure-8.png)\n\n**Figure 8.** *Query performance of querying Pinot directly vs. using Presto to query local Parquet/ORC files and Pinot segments. Querying Pinot directly achieved the lowest query latency. We observed no significant differences between querying Pinot through the Pinot connector and querying local Parquet and ORC files through the Presto connector. Note that the Pinot connector benchmarked here did not enable any pushdown optimization.*\n\nWe also benchmarked how aggregate pushdown performance improved by sending aggregation queries on several Pinot tables with different sizes. As shown in Figure 9, below, our efficiency gains from aggregate pushdown grow as the total number of documents increases in the Pinot table.\n\n![](/img/blog/2020-03-18-uber-pinot/figure-9.png)\n\n**Figure 9.** *Query performance of Presto-Pinot connector, before and after enabling aggregate pushdown. As the total number of documents increased in the Pinot table, our efficiency gains from aggregate pushdown grew.*\n\nAs shown in Figures 8 and 9, the Presto-Pinot connector had similar query performance with the existing Hive connector, while providing improved data freshness over Parquet or ORC files in HDFS. After further introducing aggregate pushdown in the Pinot connector, we were able to utilize the analytical capability of Pinot to do certain commonly used aggregates, which enhanced query latency. By allowing users to access fresh data in Pinot with SQL queries in Presto, the Pinot connector unlocks even more precise and data-driven business decisions. In turn, these decisions allow us to deliver a better user experience across our suite of products.\n\n## Looking ahead\n\nWith the success of our Presto-Pinot connector, we’ve seen just how valuable it is to access fresh data with standard SQL. Without having to learn different SQL dialects for different real-time data storage systems, users can access the fresh insights they need and make informed decisions. To this end, we are currently building the next generation of our analytics platform by consolidating storage solutions and using Presto as our unified query layer.\n\n## Acknowledgements\n\nWe’d like to give a special thanks to Xiang Fu, Zhenxiao Luo and Chinmay Soman for their valuable contribution to this project.\n\n\nLearn more about how we engineer real-time analytics at Uber:\n\n-   [Building a Better Big Data Architecture: Meet Uber’s Presto Team](https://eng.uber.com/presto-team-profile/)\n\n-   [Introducing AresDB: Uber’s GPU-Powered Open Source, Real-time Analytics Engine](https://eng.uber.com/aresdb/)\n\n-   [Turbocharging Analytics at Uber with our Data Science Workbench](https://eng.uber.com/dsw/)\n\n-   [Engineering Data Analytics with Presto and Apache Parquet at Uber](https://eng.uber.com/presto/)"
        },
        {
          "id": "/2020/03/02/presto-lambda",
          "metadata": {
            "permalink": "/blog/2020/03/02/presto-lambda",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-03-02-presto-lambda.md",
            "source": "@site/blog/2020-03-02-presto-lambda.md",
            "title": "Querying Nested Data with Lambda Functions",
            "description": "Denormalized data with nested values (e.g. array/map) have become omnipresent in this Big Data era, as a lot of data naturally conforms to a nested representation [1, 2]. As a result it is important to provide an efficient and convenient way to query nested data. SQL traditionally does not include support for this.",
            "date": "2020-03-02T00:00:00.000Z",
            "formattedDate": "March 2, 2020",
            "tags": [],
            "readingTime": 7.49,
            "truncated": true,
            "authors": [
              {
                "name": "Wenlei Xie",
                "url": "https://www.linkedin.com/in/wenleix/"
              }
            ],
            "frontMatter": {
              "title": "Querying Nested Data with Lambda Functions",
              "author": "Wenlei Xie",
              "authorURL": "https://www.linkedin.com/in/wenleix/",
              "authorFBID": 681470066
            },
            "prevItem": {
              "title": "Engineering SQL Support on Apache Pinot at Uber",
              "permalink": "/blog/2020/03/18/uber-pinot"
            },
            "nextItem": {
              "title": "Announcing PrestoCon 2020: Advancing the Big Data Ecosystem with Presto ",
              "permalink": "/blog/2020/02/13/prestocon-announcement"
            }
          },
          "content": "Denormalized data with nested values (e.g. array/map) have become omnipresent in this Big Data era, as a lot of data naturally conforms to a nested representation [1, 2]. As a result it is important to provide an efficient and convenient way to query nested data. SQL traditionally does not include support for this.\n\nThe pioneering work of Dremel proposed an extension to SQL based on recursive relational algebra to allow querying nested records [1], and is now available in BigQuery and the SQL:2016 standard. The following example shows how to transform array elements with this (adapted from [BigQuery Docs](https://cloud.google.com/bigquery/docs/reference/standard-sql/arrays#creating-arrays-from-subqueries)):\n\n```sql\nSELECT elements,\n    ARRAY(SELECT v * 2\n          FROM UNNEST(elements) AS v) AS multiplied_elements\nFROM (\n    VALUES\n        (ARRAY[1, 2]),\n        (ARRAY[1, 3, 9]),\n        (ARRAY[1, 4, 16, 64])\n) AS t(elements)\n\n    elements    | multiplied_elements\n----------------+---------------------\n [1, 2]         | [2, 4]\n [1, 3, 9]      | [2, 6, 18]\n [1, 4, 16, 64] | [2, 8, 32, 128]\n(3 rows)\n```\n\nWhile nested relational algebra provides an elegant and unified approach to query nested data, we found it could be challenging for users to track the “unnest stack” in mind when writing the query. In our experience, users are more comfortable to apply a given function (e.g lambda) to each element in the collection. This motivates us to introduce lambda expressions into Presto to help query nested data, as illustrated below:\n\n```sql\nSELECT elements, \ntransform(elements, v -> v * 2) as multiplied_elements\nFROM (\n    VALUES\n        (ARRAY[1, 2]),\n        (ARRAY[1, 3, 9]),\n        (ARRAY[1, 4, 16, 64])\n) AS t(elements)\n```\n\nIn Presto, a lambda expression consists of an argument list and lambda body, separated by `->`:\n\n```sql\nx -> x + 1\n(x, y) -> x + y\nx -> regexp_like(x, 'a+')\nx -> x[1] / x[2]\nx -> IF(x > 0, x, -x)\nx -> COALESCE(x, 0)\nx -> CAST(x AS JSON)\nx -> x + TRY(1 / 0)\n```\n\nNote there is no type annotation in a lambda expression. The type of a lambda expression (e.g. `function(integer, integer)`) thus has to be inferred from the context of function call. As a result, standalone lambda expressions are not allowed since their types cannot be determined. \n\n<!--truncate-->\n\n## Lambda Type Inference\n\nThe initial lambda support in Presto was added in [Presto#6198](https://github.com/prestodb/presto/pull/6198) with basic compilation and execution. One of the major challenges this pull request addressed was type inference for lambda, as there is no type annotation in lambda expressions. Consider the following expression contains lambda:\n\n```sql\ntransform(elements, v -> v * 2)\n```\n\nwhere `elements` is with type `array(integer)`.\n\nPresto allows function overloading, and the exact function match is resolved by looking up the function name and argument types. This is infeasible for higher-order functions as the type for `v -> v * 2` cannot be resolved without context. \n\n![Lambda Type Inference](/img/blog/2020-03-02-presto-lambda/lambda-type-inference.png)\n\nAs a result, Presto doesn’t allow function overloading for higher-order functions, thus it can still resolve the function `transform` just by the name. `ExpresionAnalyzer` will have the following information:\n\n- The generic type for `transform` is `(array(T), function(T, U)) -> array(U)`.\n- The first argument type is `array(integer)`.\n- The exact type for the second argument `v -> v * 2` is unknown since it’s a lambda expression. However, its type can be uniquely determined once the input parameter type is bound. This is done by the [TypeSignatureProvider](https://github.com/prestodb/presto/blob/5833338c127c380f505873614d25862921437e75/presto-main/src/main/java/com/facebook/presto/sql/analyzer/TypeSignatureProvider.java#L27) class.\n\nThe type parameter `T` and `U` needs to be determined to resolve the expression type. This is done by [SignaturerBinder#bind](https://github.com/prestodb/presto/blob/5833338c127c380f505873614d25862921437e75/presto-main/src/main/java/com/facebook/presto/metadata/SignatureBinder.java#L92) method. [SignaturerBinder#appendConstraintSolvers](https://github.com/prestodb/presto/blob/5833338c127c380f505873614d25862921437e75/presto-main/src/main/java/com/facebook/presto/metadata/SignatureBinder.java#L289-L300) is called under the hood to iteratively solve this constraint satisfaction problem. A new [TypeConstraintSolver](https://github.com/prestodb/presto/blob/5833338c127c380f505873614d25862921437e75/presto-main/src/main/java/com/facebook/presto/metadata/SignatureBinder.java#L541-L544) called [FunctionSolver](https://github.com/prestodb/presto/blob/5833338c127c380f505873614d25862921437e75/presto-main/src/main/java/com/facebook/presto/metadata/SignatureBinder.java#L720-L721) was added for updating type constraints related to lambda expressions.\n\n## Lambda Capture\n\nLambda capture allows users to refer to other columns in the lambda function, for example: \n\n```sql\nSELECT elements, \n    transform(elements, v -> v * factor) as multiplied_elements\nFROM (\n    VALUES\n        (ARRAY[1, 2], 2),\n        (ARRAY[1, 3, 9], 3),\n        (ARRAY[1, 4, 16, 64], 4)\n) AS t(elements, factor)\n\n    elements    | multiplied_elements\n----------------+---------------------\n [1, 2]         | [2, 4]\n [1, 3, 9]      | [3, 9, 27]\n [1, 4, 16, 64] | [4, 16, 64, 256]\n(3 rows)\n```\n\nLambda capture supported is added in [Presto#7210](https://github.com/prestodb/presto/pull/7210). It rewrites the captured lambda into non-capture lambda via [partial function application](https://en.wikipedia.org/wiki/Partial_application). A special internal higher order function `BIND` is introduced to partially apply captured arguments to the lambda.\n\nTake the above example, the captured lambda call\n\n```sql\ntransform(elements, v -> v * factor)\n```\n\nis rewritten to\n\n```sql\ntransform(\n    elements, \n    BIND(factor, (captured_factor, e) -> e * captured_factor)\n)\n```\n\nThe original unary lambda with capture `e -> e * factor` is rewritten into a binary lambda without capture: `(captured_factor, e) -> e * captured_factor`. The `BIND` call takes `factor` and this binary lambda as input, returns the partially applied function that multiplies the input by `captured_factor` (Note the `captured_factor` will be different for each row!). This partially applied function is a unary function and is provided as the second parameter to `transform` call.\n\n## Lambda Execution\n\nIn this section we are going to discuss how a lambda is executed during runtime. The original implementation used `MethodHandle` objects to represent lambdas on the stack. Consider the same example:\n\n```sql\ntransform(\n    elements, \n    BIND(factor, (captured_factor, v) -> v * captured_factor)\n)\n```\n\nEach invocation of `transform` works in the following way: \n1. Push the Java object representing elements on the stack. When `elements` is `array(integer)`, the corresponding Java stack type is `IntArrayBlock`.\n2. Push the `MethodHandle` object representing captured lambda to the stack, i.e. binding `factor` to `(captured_factor, v) -> v * captured_factor`. To this end: \n    1. Push the `MethodHandle` object represents `v -> v * captured_factor` onto the stack.\n    2. Push `captured_factor` on the stack.\n    3. Invoke `MethodHandle#bindTo` to get a `BoundMethodHandle` representing captured lambda on the top of stack. \n3. Invoke `transform`. \n\nUnfortunately, this implementation causes Java to generate a separate customized LambdaForm class for every `MethodHandle#bindTo` call (i.e. per each row). Such excessive runtime class generation quickly fills the Metaspace and causes full GC, see [Presto#7935](https://github.com/prestodb/presto/issues/7935) for reproduction and details. JDK developers have confirmed that each `BoundMethodHandle` should be customized independently, and `MethodHandle#bindTo` [is not a good fit for implementing lambda capturing](http://mail.openjdk.java.net/pipermail/mlvm-dev/2017-May/006755.html).\n\nTo fix this, we redesigned Presto lambda execution via [Presto #8031](https://github.com/prestodb/presto/pull/8031). The key observations are:\n- Lambda capture has to be performed per invocation, as different value will be captured for each row.\n- However, we should use the same class representing captured lambda for every `BIND` call, otherwise we will generate too many classes.\n\nWe use the same approach as Java uses to handle lambda and capture [3, 4]:\n- A lambda is represented as an object whose type is a functional interface (a.k.a. Single Abstract Method class)\n- The `invokedynamic` instruction is used to perform lambda capture:\n    * During the first `invokedynamic` call, the class representing the captured lambda (which is a functional interface) is created and a method to perform the capture is generated in it. This step is also called linkage and will only be done once.\n    * Every `invokedynamic` call performs capture and returns an instance of the desired functional interface. \n\nWith this design, `BIND` function will always be fused together with the lambda generation step to generate a captured lambda in a single step -- we cannot first generate an object representing the uncaptured lambda, and then perform a separate partial application step. Note this implementation also doesn't allow more general higher-order functions that return a function as result.\n\n## Lambda in Aggregation \n\n\nWhile lambda was originally introduced to help query nested data with scalar functions, we also noted it can be used in aggregation functions to allow more flexible analytics. The initial support for lambda in aggregation was added in [Presto#12084](https://github.com/prestodb/presto/pull/12084), with a [reduce_agg](https://prestodb.io/docs/current/functions/aggregate.html#reduce_agg) function for demonstration purposes. `reduce_agg` *conceputally* allows the creation of User-Defined Aggregation Function (UDAF) by making the input and combine functions lambdas. The following example shows how to use `reduce_agg` to compute group-wise product (instead of sum):\n\n```sql\nSELECT id, \nreduce_agg(value, 1, (a, b) -> a * b, (a, b) -> a * b) prod\nFROM (\n    VALUES\n        (1, 2),\n        (1, 3),\n        (1, 4),\n        (2, 20),\n        (2, 30),\n        (2, 40)\n) AS t(id, value)\nGROUP BY id;\n\n id | prod\n----+-------\n  2 | 24000\n  1 |    24\n(2 rows)\n```\n\nUnfortunately, due to [JDK-8017163](https://bugs.openjdk.java.net/browse/JDK-8017163), aggregation state with `Slice` or `Block` as a native container type is intentionally not supported yet. It can result in excessive JVM remembered set memory usage. This is because aggregation state requires updates in unpredictable order, resulting in a huge amount of cross-region references when each state is a separate object. This issue is also reported in [Presto#9553](https://github.com/prestodb/presto/issues/9553). This makes this function not yet practically useful. Once [JDK-8017163](https://bugs.openjdk.java.net/browse/JDK-8017163) is fixed in later versions of the JVM, we are looking forward to enabling it with more general types to allow more flexible analytics in aggregations!\n\n## Reference\n\n[1] [Dremel: Interactive Analysis of Web-Scale Datasets](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/36632.pdf)\n\n[2] [Everything You Always Wanted To Do in Table Scan](https://prestodb.io/blog/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan)\n\n[3] [Lambda: A peek under the hood](http://chariotsolutions.com/wp-content/uploads/presentation/2014/04/Brian-Goetz-Lambda-Under-The-Hood.pdf)\n\n[4] [State of the Lambda](https://cr.openjdk.java.net/~briangoetz/lambda/lambda-state-final.html)"
        },
        {
          "id": "/2020/02/13/prestocon-announcement",
          "metadata": {
            "permalink": "/blog/2020/02/13/prestocon-announcement",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2020-02-13-prestocon-announcement.md",
            "source": "@site/blog/2020-02-13-prestocon-announcement.md",
            "title": "Announcing PrestoCon 2020: Advancing the Big Data Ecosystem with Presto ",
            "description": "On March 24, 2020 in San Mateo, the Presto Foundation, in partnership with The Linux Foundation, will be hosting the organization’s first-ever PrestoCon. The event, one of the first Presto-focused full-day conferences ever held, will feature speakers from Uber, Facebook, and Twitter, as well as tech talks from other major Presto contributors and enthusiasts.",
            "date": "2020-02-13T00:00:00.000Z",
            "formattedDate": "February 13, 2020",
            "tags": [],
            "readingTime": 1.075,
            "truncated": false,
            "authors": [
              {
                "name": "Nezih Yigitbasi",
                "url": "https://www.linkedin.com/in/nezihyigitbasi/"
              }
            ],
            "frontMatter": {
              "title": "Announcing PrestoCon 2020: Advancing the Big Data Ecosystem with Presto ",
              "author": "Nezih Yigitbasi",
              "authorURL": "https://www.linkedin.com/in/nezihyigitbasi/",
              "authorFBID": 100000082666878
            },
            "prevItem": {
              "title": "Querying Nested Data with Lambda Functions",
              "permalink": "/blog/2020/03/02/presto-lambda"
            },
            "nextItem": {
              "title": "Improving the Presto planner for better push down and data federation",
              "permalink": "/blog/2019/12/23/improve-presto-planner"
            }
          },
          "content": "On March 24, 2020 in San Mateo, the Presto Foundation, in partnership with The Linux Foundation, will be hosting the organization’s first-ever [PrestoCon](https://events.linuxfoundation.org/prestocon/). The event, one of the first Presto-focused full-day conferences ever held, will feature speakers from Uber, Facebook, and Twitter, as well as tech talks from other major Presto contributors and enthusiasts. \n\nPresto, a high performance, distributed SQL query engine for Big Data, was originally developed at Facebook to power large-scale data analytics on Apache Hadoop. Open sourced in 2013, the [PrestoDB](https://github.com/prestodb/presto) project was contributed to the Linux Foundation in 2019 to facilitate an [open and neutral governance model](https://prestodb.io/blog/2019/12/16/growing-the-presto-foundation) that has enabled the software to grow and diversify its community of contributors.  \n\nIn addition to keynotes and tech talks, PrestoCon will bring together Big Data and analytics practitioners from across the world for a day of networking and collaboration to advance the use of Presto. We will close out the event with a special happy hour for all attendees, providing an opportunity for discussion and socializing.  \n\nFor those interested in speaking about their Presto experiences or best practices, we encourage you to [submit a proposal to speak](https://events.linuxfoundation.org/prestocon/program/cfp/).\n\nDon’t forget to [register](https://events.linuxfoundation.org/prestocon/Register/) for the event as spaces are limited. We look forward to seeing you there!   \n\nLearn more at [the official PrestoCon event site](https://events.linuxfoundation.org/prestocon/)."
        },
        {
          "id": "/2019/12/23/improve-presto-planner",
          "metadata": {
            "permalink": "/blog/2019/12/23/improve-presto-planner",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-12-23-improve-presto-planner.md",
            "source": "@site/blog/2019-12-23-improve-presto-planner.md",
            "title": "Improving the Presto planner for better push down and data federation",
            "description": "Alibaba: Yuan Mei",
            "date": "2019-12-23T00:00:00.000Z",
            "formattedDate": "December 23, 2019",
            "tags": [],
            "readingTime": 8.965,
            "truncated": true,
            "authors": [
              {
                "name": "Yi He",
                "url": "https://www.linkedin.com/in/yi-he-69aa9723/"
              }
            ],
            "frontMatter": {
              "title": "Improving the Presto planner for better push down and data federation",
              "author": "Yi He",
              "authorURL": "https://www.linkedin.com/in/yi-he-69aa9723/"
            },
            "prevItem": {
              "title": "Announcing PrestoCon 2020: Advancing the Big Data Ecosystem with Presto ",
              "permalink": "/blog/2020/02/13/prestocon-announcement"
            },
            "nextItem": {
              "title": "5 design choices—and 1 weird trick — to get 2x efficiency gains in Presto repartitioning",
              "permalink": "/blog/2019/12/20/repartition"
            }
          },
          "content": "**Alibaba:** Yuan Mei\n\n**Facebook:** James Sun, Maria Basmanova, Rongrong Zhong, Jiexi Lin, Saksham Sachdev\n\n**Pinterest:** Yi He\n\n**University of Waterloo:** Akshay Pall\n\nPresto defines a connector API that allows Presto to query any data source that has a connector implementation. The existing connector API provides basic predicate pushdown functionality allowing connectors to perform filtering at the underlying data source.\n\nHowever, there are certain limitations with the existing predicate pushdown functionality that limits what connectors can do. The expressiveness of what can be pushed down is limited and the connectors can't change the structure of the plan at all. \n\n<!-- truncate -->\n\nThis image shows what the planner and connector interaction used to look like:\n![Remote Exchange](/img/blog/2019-12-19-improve-presto-planner.md/image_01.png)\n\nFirst, Presto only supports predicate push down through connector provided methods. If Presto needs to push down a set of operations (for example, `projection`/`filter`/`aggregation`/`limit`), then the connectors need to support several methods:\n\n```java\npushDownFilterLimitScan\npushDownProjectFilterScan\n...\n```\n\nThis increases the complexity of creating and maintaining a connectors. Also, as we will show later, we not only want to push down operations, but also add new operations into a query plan. The current planner model does support many useful connector driven plan changes.\n\nSecond, the range of predicates and operators that can be pushed down is limited. Only predicates that can be represented by a data structure called TupleDomain, can be pushed down. This data structure only supports ANDed predicates that determine whether a variable/column is in a value set (ranged or equitable). There is thus no way to describe complex predicates like `'A[1] IN (1, 2, 3)'` or `‘A like 'A Varchar %'’`.\n\nA more flexible approach would be to push down the entire expression which is currently represented as an Abstract Syntax Tree (AST). One problem with this approach is that the AST evolves over time such as when new language features are added. Additionally the AST does not contain type information as well as enough information to perform function resolution.\n\nFunctions can now resolve to different implementations thanks to the recently added dynamic function registration. Dynamic function registration allows users to write their own SQL functions. For example, a user can update the definition of a SQL function in another session while queries using the function are still running. If we were to perform function resolution at invocation time then we could end up using different implementations within the same session and query. If we are going to support materialized views, we also need to make sure the function version is consistent between the data reader and writer.\n\nWe resolve this by storing function resolution information in the expression representation itself as a serializable `functionHandle`. This makes it possible to consistently reference a function when we reuse the expressions containing the function.\n\nTypes present a similar issue. Connectors can’t safely rely on metadata to know the type of a variable. The metadata describing the variable might be unavailable or have changed during execution.\n\nWe have gradually improved the Presto planner’s ability to push down more expressive operations between version 0.217 and 0.229. We also made corresponding updates to connectors allowing them to understand and operate on plan sub-trees.\n\n## Exposing plan sub-trees to connector\nPresto executes a SQL query by first parsing it to an Abstract Syntax Tree (AST). The AST is then converted to logical plan tree, which represents the relational algebra contained in the query. The relational algebra representation is not optimized and lacks sufficient physical layout information for query execution.\n\nPresto uses a list of optimizers to transform the logical plan to an optimized physical plan. Each plan optimizer can operate on sub-trees of the whole plan tree and replace them with more optimized sub-trees based on heuristic or statistics. Optimizers can save the physical information of execution on a set of connector provided handles (for example, `ConnectorTableHandle`, `ConnectorTableLayoutHandle`, `ConnectorPartitionHandle`, …).\n\n![Remote Exchange](/img/blog/2019-12-19-improve-presto-planner.md/image_02.png)\n\nUnlike some other SQL engines, Presto does not explicitly set the boundary between the logical plan and physical plan. Instead, there are a few crucial optimizers that transform the logical plan into a physical one.\n\n`PickTableLayout` and `AddExchanges` are two of the more important optimizers.\n\n`PickTableLayout` plans predicate push down into a table scan by calling the connector provided API method `getTableLayout`. It is also used to obtain physical layout information from the connector. `getTableLayout` returns a `LayoutHandle` that the connector populates with information on the structure of the data that will be returned by the scan. Presto will later use the `LayoutHandle` to plan, optimize, and execute the query.\n\n`AddExchanges` adds the data shuffling (data exchange) operators to a query execution. This important step determines how query execution is parallelized, and how data is redistributed for processing at each stage of a query. A stage of execution in Presto is generally the shuffling of data on the partition key that is required for processing the next part of a query plan. `AddExchanges` relies on the Handle returned from the connector to decide on the appropriate places and kinds of exchanges to add to the plan.\n\nRelying on `PickTableLayout` to do both predicate push down and physical planning is very restrictive because there is no way for connectors to modify the plan beyond basic predicate pushdown.\n\nPresto now allows connectors to provide optimization rules to the Presto engine, which allows connectors to introduce arbitrary optimizations. There are restrictions to prevent connector provided optimizers from accidentally changing another connector’s sub plan:\n1.  PlanNodes that are exposed to presto-spi module.\n2.  PlanNodes that belong to the connector.\n\nA sub max tree that satisfies the above rules will be transformed to more optimized form picked by a connector provided optimization rule:\n```java\npublic  interface  ConnectorPlanOptimizer\n{\n\tPlanNode optimize(\n\t\tPlanNode maxSubplan,\n\t\tConnectorSession session,\n\t\tVariableAllocator variableAllocator,\n\t\tPlanNodeIdAllocator idAllocator);\n}\n```\n![Remote Exchange](/img/blog/2019-12-19-improve-presto-planner.md/image_03.png)\n\nNote that, the above rule only applies to the `maxSubPlan` optimizer taken as input. It is quite possible a connector provided optimizer can generate a new plan contains nodes belonging to another connector (usually during a view expansion). In such cases, `TableScan` might be reading from a virtual table combining data from multiple different data sources. The `TableScan` on the virtual table can be expanded to a new sub tree that unions TableScans from both sides. Once expanded, the optimization of newly generated plan nodes can then be handled by the connectors they belong to, so that most optimized subplan can be achieved.\n\nThe connector rule will transform the sub max plan tree. In the case of predicate pushdown (taking `MySQLConnector` as an example), the connector can save the predicates that MySQL can handle as a SQL expression inside `MySQLConnectorLayoutHandle` and return a `TableScan` node. \n\nThe engine will apply the connector optimization rules at critical plan transformation checkpoints:\n\nAll rules that are operating on logical plans will be applied once before `AddExchange` to start the transformation into physical plan. At this point, we can expand views and push down many operations.\n\nSome optimizations that rely on physical information need to be applied later at the end of optimization cycle. For example, we may want to only push down part of an aggregation into the connector in order to still benefit from parallel execution. Splitting up the aggregation stages happens after the exchange nodes are added.\n\n## More descriptive expression language\nWe also replaced AST-based expression representation with a new representation called `RowExpression`. `RowExpression` is completely self-contained and can be shared across multiple systems. The new representation has several sub-types:\n\n|ExpressionType|Represents|\n|-|-|\n|`ConstantExpression`|Literal values such as `(1L, BIGINT)`, `(\"string\", VARCHAR)` ...|\n|`VariableReferenceExpression`|Reference to an input column and a field of the output from previous relation expression.|\n|`CallExpression`|Function calls, which includes all arithmetic operations, casts, UDFs, … with function handle resolved.|\n|`SpecialFormExpression`|Special built-in function calls that is generic to any types or can only have single type(Boolean) thus function handle is not necessary. Examples are: `IN` `IF`, `IS_NULL`, `AND`,`OR`, `COALESCE`, `DEREFERENCE`, `ROW_CONSTRUCTOR`, `BIND`, ...|\n|`LambdaDefinitionExpression`|Definition of anonymous (lambda) functions. For example: `(x:BIGINT,y:BIGINT):BIGINT -> x+y`|\n\n## How are we using it so far:\n\n### Aria Scan filter pushdown\n[Project Aria Scan](https://engineering.fb.com/data-infrastructure/aria-presto/) aims to improve the CPU efficiency of table scan by pushing filters into the scan. The new planner provides native support for the filter pushdown required here.\n\n### Uber Pinot connector\nUber uses AresDB and Pinot to serve their real time analytics([[1]](https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber)[[2]](https://eng.uber.com/restaurant-manager/)[[3]](https://eng.uber.com/aresdb/)). These systems are very fast, but have limited SQL support. Presto can provide full SQL support on top of these systems to satisfy the growing need for complex analytics.\n\nAresDB and Pinot can handle certain subsets of localized relational algebra. Being able to push down those operations means better efficiency and lower latency. Recently, Uber has contributed the Pinot connector to [PrestoDB](https://prestodb.io/) which leverages the new connector architecture to push down aggregations and filters into Pinot.\n\nUber is actively working on a Presto-AresDB connector to do the same.\n\n### Scuba\nAt Facebook, we use Scuba for analytics on real time data. The new connector architecture allows pushing down filters and aggregations into these systems to achieve better efficiency and latency.\n\n[Scuba](https://research.fb.com/publications/scuba-diving-into-data-at-facebook/) is a Facebook internal system offering real time ingestion with limited retention. There are many use cases where recent data, from the last hour or day, is stored in Scuba and older data is stored in Hive. We built a Scuba connector for Presto to allow users to query data from both Hive and Scuba in the same query.\n\nWe also built views that combine Scuba tables with their Hive counterparts to allow for seamless querying. The support for views spanning two connectors was made possible by the new planner architecture. The connector expands the table scan node referring to the view into a union of table scans: one from Scuba, and one from Hive.\n\n![Remote Exchange](/img/blog/2019-12-19-improve-presto-planner.md/image_04.png)\n\n### Row level security\nSometimes we want to add filters dynamically, e.g. based on who is querying the data. For example, an employee of Coca Cola shouldn’t see records from Pepsi. We built an optimizer rule that conditionally adds Filter node on top of the TableScan node based on the query user and table structure.\n\n![Remote Exchange](/img/blog/2019-12-19-improve-presto-planner.md/image_05.png)\n\n## What’s next?\nEven though the planner changes are already delivery benefits, we are still only half way there:\n\nNot all optimization rules are using `RowExpression` at the moment. We are actively migrating all optimization rules to use RowExpression. In today’s version of Presto, we still rely on both connector provided optimization rules and the old API to plan for different data sources. Over time we will unify them.\n\nWe also want to add support for [traits](https://docs.google.com/presentation/d/10wCmZEp5NnRSb_4oPsdiKkmYy4Hy5BgddEA8brPuVis/edit#slide=id.g1fce08ab98_3_5) to simplify the mechanism for obtaining data layout information. In the long run, we are hoping our planner can be exploratory which means we can find the lowest cost (most optimized) plan through many different optimization combinations.\n\n\n\n\n[[1]](https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber)https://www.slideshare.net/XIANGFU3/pinot-near-realtime-analytics-uber\n\n[[2]](https://eng.uber.com/restaurant-manager/)https://eng.uber.com/restaurant-manager\n\n[[3]](https://eng.uber.com/aresdb/)https://eng.uber.com/aresdb"
        },
        {
          "id": "/2019/12/20/repartition",
          "metadata": {
            "permalink": "/blog/2019/12/20/repartition",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-12-20-repartition.md",
            "source": "@site/blog/2019-12-20-repartition.md",
            "title": "5 design choices—and 1 weird trick — to get 2x efficiency gains in Presto repartitioning",
            "description": "Ying Su, Masha Basmanova, Orri Erling, Tim Meehan, Sahar Massachi, Bhavani Hari",
            "date": "2019-12-20T00:00:00.000Z",
            "formattedDate": "December 20, 2019",
            "tags": [],
            "readingTime": 13.555,
            "truncated": true,
            "authors": [
              {
                "name": "Ying Su",
                "url": "https://www.linkedin.com/in/ying-su-b00b81107/"
              }
            ],
            "frontMatter": {
              "title": "5 design choices—and 1 weird trick — to get 2x efficiency gains in Presto repartitioning",
              "author": "Ying Su",
              "authorURL": "https://www.linkedin.com/in/ying-su-b00b81107/",
              "authorFBID": 656599427
            },
            "prevItem": {
              "title": "Improving the Presto planner for better push down and data federation",
              "permalink": "/blog/2019/12/23/improve-presto-planner"
            },
            "nextItem": {
              "title": "Join Us! Growing the Presto Foundation in 2020 and Beyond",
              "permalink": "/blog/2019/12/16/growing-the-presto-foundation"
            }
          },
          "content": "Ying Su, Masha Basmanova, Orri Erling, Tim Meehan, Sahar Massachi, Bhavani Hari\n\nWe like Presto. We like it a lot — so much we want to make it better in every way. Here's an example: we just optimized the PartitionedOutputOperator. It's now 2-3x more CPU efficient, which, when measured against Facebook's production workload, translates to 6% gains overall. That's huge.\n\nThe optimized repartitioning is in use on some production Presto clusters right now, and available for use as of release 0.229.\n\nIn this note, let's go over how we did it, what optimizations we unlocked specifically, and a case study of how we approached opportunity sizing whether this was worth doing at all.\n\n<!--truncate-->\n\n## What is the Partitioned Output Operator, anyway?\n\nIn a distributed query engine data needs to be shuffled between workers so that each worker only has to process a fraction of the total data. Because rows are usually not pre-ordered based on the hash of the partition key for an operation (for example join columns, or group by columns), repartitioning is needed to send the rows to the right workers. PartitionedOutputOperator is responsible for this process: it takes a stream of data that is not partitioned, and divide the stream into a series of output data ready to be sent to other workers.\n\nThe PartitionedOutputOperator takes about 10% of the total CPU of all Facebook warehouse workload. That's a lot! We can cut it down to 3-5%.\n\nThe legacy PartionedOutputOperator works as follows:\n\n1. Building step: Each destination partition has a PageBuilder. When a page comes in, the destination of each row is calculated using a hash function (xxHash64) on the partitioning columns which may be pre-computed. The rows are appended to each destination’s PageBuilder.\n2. Serialization step: If any PageBuilder's size is larger than configured max page size then it will be split into several pages that fit into the limit.  Then each of these pages will be serialized to a SerializedPage which is enqueued to the OutputBuffer for the destination.\n\nIn the new implementation we removed the build step and directly append the data to buffers. Then we concatenate these buffers to form a SerializedPage, and then send it out. The new one looks like this:\n\n```text\nFor each incoming page\n    Populate the top level row numbers for each destination partition\n    Decode the blocks by peeling off the Dictionary or RLE wrappings\n        For each partition\n            Populate nested level row numbers for nested blocks like ArrayBlock\n            Calculate row sizes\n            Calculate how many rows can fit the buffers before reaching the limit. This is based on the result of step 1 and 2.\n            Append the rows to the buffers block by block\n            If the size limit is reached (e.g. 1MB for a destination), flush the buffer\n```\n\n## Results: Things are easier, faster, stronger, better\n\nOptimized repartitioning has been enabled on several Facebook clusters. A substantial improvement in CPU utilization for PartitionedOutputOperator has been observed in all regions. The percentage of CPU consumed by PartitionedOutputOperator dropped from approximately 10% to about 4%. TPCH SF3000 benchmark on 22 read only queries show an overall 13% gain in CPU reduction. Certain queries improved by over 30%:\n\n![Remote Exchange](/img/blog/2019-12-20-repartition.md/tpch_sf3000_repartition.png)\n\n## Opportunity Sizing: How we decided to make these changes\n\nA key part of our work is making sure we choose the right projects. To butcher a saying, there will always be fruit on the tree. How did we choose this low hanging fruit? Here's an example of how we started:\n\nFirst, we looked at all the operators in production, and realized that PartionedOutputOperator took up a nice chunk of total CPU. We took the CPU profiles for some of the queries with high PartionedOutputOperator cost, and found the cost of the PageBuilder is on top of the profiles. This step can be skipped in fact, and we just need to write the data directly into buffers that conforms to the serialized format and concatenate them before putting them on the wire. Now we need to find an efficient way to do this memory copying and serialization.\n\nWe performed a few experiments to game out the performance differences between different options for reading data from the pages and blocks, and writing data into memory.\n\n### Experiment 1: Reading blocks\n\nOur first experiment helped us decide between two different ways of reading data. Which did we prefer?\n\n1. Directly access the raw arrays at specified positions\n2. Access the values through the Block.getXXX() interface\n\nDirectly accessing arrays would in theory be faster. Compilers can do all sorts of tricks like loop unrolling and auto-vectorization, but, in Presto, the Block interface does not expose the raw arrays, just the getXXX() methods to access a single value. To access the raw arrays directly, the Block interface would have to be changed — and we generally want to avoid that. Block.getXXX() methods are virtual interface functions. In C++, virtual calls are mostly expensive, because it’s AOT compilation and cannot devirtualize the virtual function calls at run time. Each call involves a vtable lookup and a jump.\n\nHow well can JVM optimize the code? This is the first experiment we needed to do. Can we achieve similar performance of accessing raw arrays in Java without modifying the Block interface? Theoretically yes, if the number of types is not more than 2.\n\nIn the first experiment, we read one type (BIGINT) from a LongArrayBlock. We compared it to reading from a raw array. The destination for both cases are byte arrays with same size. The raw array was up to 33% faster. Was it due to virtual function dispatch or something else?\n\nWe verified the functions were being optimized by C2 in level 4 and were inlined properly. We then got the async-profiler and perf-asm results and found the difference was coming from the boundary check in the getLong() implementation:\n\n```java\npublic long getLong(int position)\n{\n    checkReadablePosition(position);\n    return getLongUnchecked(position + arrayOffset);\n}\n\nprivate void checkReadablePosition(int position)\n{\n    if (position < 0 || position >= getPositionCount()) {\n        throw new IllegalArgumentException(\"position is not valid\");\n    }\n}\n```\n\nThe code in checkReadablePosition() was compiled to two tests and jumps. Applying this to every row has a negative impact on performance. By removing this boundary check the performance of the getXXX() loop is as fast as accessing the raw arrays!\n\nIn fact, for some operators like the PartitionedOutputOperator, the positions for a given batch of rows are known in advance and this range check can be hoisted out and performed only once per batch. We introduced UncheckedBlock with getXxxUnchecked methods that don’t include the boundary checks to allow this approach to be used.\n\nThere were no virtual function dispatch costs, and the generated assembly from the two tests are the same, both in  a tight loop, inlined,  and unrolled. This is because Java uses JIT compilation and has complete information about the classes loaded that implement an interface. So for a given call site, if only one class implements a given interface (monomorphic), then the calls can be de-virtualized to direct calls, and inlined if the function is small enough.\n\nWe also expect most calls to be monomorphic for this operator because we copy the values one block at a time in a tight loop and there is only one implementation invoked at each call site.\n\nNext, we verified that the addition of arrayOffset in the getLong call didn’t incur additional cost. We checked how it was compiled. Instead of a standalone add instruction, It was a mov instruction with indirect addressing with displacement and scaled-index as follows:\n\n```text\ngetByteUnchecked(position + *arrayOffset*) -> mov 0x20(%r8,%r10,8),%rax  ;*laload\n```\n\nOn Intel and AMD CPUs the different varieties of mov instructions have similar cost in terms of CPU cycles. It seems the JVM did an awesome job in optimizing this loop so we decided to go through the UncheckedBlock getters.\n\n\n### Experiment 2: Writing to buffers\n\nOur next experiment helped us decide between a few different ways of *writing*:\n\n1. Raw byte array\n2. SliceOutput in Airlift (BasicSliceOutput or DynamicSliceOutput)\n3. A custom SliceOutput that wraps a raw byte array\n\nWe tried 3 different ways of writing buffers: A byte array, basic slice, and dynamic slice. (All were patched with the \"cmov\" fix we'll talk about later).\n\nLong story short, all SliceOutput implementations were much slower than raw byte arrays. That is because SliceOutput contains lots of sanity checks like boundary checks and null checks. If we used byte arrays to avoid those checks, we could get a 1.5x to 3x win on writes.\n\n### Experiment 3: Partitions first or columns first?\n\nWe also studied the performance for two different ways to add values to the buffer:\n\n1. Loop over columns, and then partitions\n2. Loop over partitions, and then columns.\n\nFor 1) the reads are local i.e. reading from the same array/block over and over while the writes are scattered. For 2) the reads are scattered but the writes are local. We didn't see enough difference to make us favor one over another.\n\nAfter all that impact scoping, we were ready to make the changes.\n\n## Our 5 design choices (and 1 weird trick) that got this working:\n\n1. Process data column-by-column, not row-by-row\n2. Use unchecked blocks and unchecked getters (for speed)\n3. Avoid SliceOutput, use byte arrays as destination buffers\n4. Avoid branches and jumps by optimizing the if checks for the null case\n5. Avoid copying in page serialization / deserialization\n\n### 1. Read columns, not rows\n\nYou can think of the operator as a pipeline that takes in pages of input, does a hash on each row of input, and then writes to output.\n\nIn the legacy implementation, the input was read row by row. Kind of like this:\n\n```text\nfor each row\n    for each column\n        call the type.appendTo to write the value into a BlockBuilder\n```\n\nThis is inefficient for a few reasons:\n\n1. Type is megamorphic. That means that the Type.appendTo() call and Block.getLong(), getDouble(), etc could be implemented by many different subclasses. (RowBlock, IntArrayBlock, MapBlock, etc). So each time we call getXXX the JVM has to search for the right method.\n2. You can't unroll this loop. Relatedly, since each column in a row might be different, the compiler can't unroll or parallelize this loop.\n\n\nIn our new implementation, we do something like this:\n\n```text\nfor each column\n    cast to the correct subclass of block\n    for each row\n        call XXXBlock.getYYY\n```\n\nWinning!\n\n ### 2. Arrays are better than SliceOutput\n\n*See the discussion around [Opportunity Sizing: How we decided to make these changes](http://localhost:3000/blog/2019/12/20/repartition#opportunity-sizing-how-we-decided-to-make-these-changes)*.\n\nWe need a thing we'll call buffers. These will be used to, well, buffer the data after we calculate its destination partition. We used to use SliceOutput. But now, we use a thin wrapper around byte arrays instead. This wrapper has fewer checks. But with careful coding, we don't need them.\n\nHere's an example: We have to check the buffer's size ourselves, and deal with problems if the data we write is too large for the buffer. There're two ways for checking the buffer size and make sure they're not over the limit. One way is to calculate the row sizes in advance, and add to the buffer only for the rows that fit. The other way is to check if the buffers need to be flushed for every row it adds. We chose the first method because 1) the second way requires us to do a size check for every value added inside of the loop. 2) calculating row sizes can be done fairly fast. For fixed length types this can be simplified to a simple division. If all columns are fixed length, we can get the size really fast. For variable width columns, we need to calculate the row sizes. To do this efficiently, we pass in an int array to the block in recursive manner, so that no new memory is allocated in each nested block.\n\n\n### 3. UncheckedBlock is best block\n\nReading blocks is slow. Why? Because of all those pesky checks. Null checks. Boundary checks. etc.\n\nUncheckedBlock is a new superclass of Block. It gives us a set of getXXXUnchecked methods. (Like getLongUnchecked()). These methods don't check to see if you're writing to an index outside the size of the array. That small change gives us an ~10% speed boost -- comparable to raw array handling.\n\nUncheckedBlock exists right now, and Presto developers can feel free to use it in the future for their code.\n\n### 4. Rewrite if statements to avoid jumps/branches\n\nLook at this code:\n\n```java\nfor (int j = 0; j < positionCount; j++) {\n    int position = positions[j];\n        if (!block.isNull(position)) {\n               long longValue = block.getLong(position);\n               ByteArrayUtils.setLong(longValueBuffer, longBufferIndex, longValue);\n               longBufferIndex += ARRAY_LONG_INDEX_SCALE;\n        }\n    }\n```\n\nThere's a problem here. Can you see it? That if statement is pretty hefty, and that means that it compiles down to a `jump` or `jmp` command. The condition contains several statements, and it's necessary for the complier to create different branches. This forces the CPU to speculate and potentially throw away work if the branch is mispredicted.\n\nIf only we could do an atomic if statement. This would allow us to avoid the whole mess of a jump and branch. Could such a thing be possible?\n\nYes! The assembly call we want is cmov or cmovne. We can induce it through careful rewriting:\n\n```java\nfor (int j = 0; j < positionCount; j++) {\n    int position = positions[j];\n    long longValue = block.getLong(position);\n    ByteArrayUtils.setLong(longValueBuffer, longBufferIndex, longValue);\n    if (!block.isNull(position)) {\n           longBufferIndex += ARRAY_LONG_INDEX_SCALE;\n    }\n```\n\nThat gives us an up to 2.6x performance improvement. Nice!\n\n\n### 5. Avoid unnecessary copying in PagesSerde\n\nContext: PagesSerde stands for Pages Serialization / Deserialization. The method wrapSlice is what we care about right now.\n\nWe did the following things to make the wrapSlice method better:\n\n1. Avoid copying a buffer when the slice is already compact. Added a check -- If the slice you're using as input is already compact, don't bother compacting/copying it.\n2. Materialize a compression buffer in this class instead of creating it every time.\n\nOur version of a Slice is always compact, so that's nice. (We skip the copy!). How?\n\n1. We estimate the size of the buffer beforehand, and we only write that much for each batch\n2. We only allocate that number of bytes. How can we estimate the size of a slice? Type size * num rows.\n3. Bonus -- we don't need to check that the buffer is full after adding data to it!\n\n### Bonus: One weird trick — bitshift for range reduction when calculating partitions\n\nThis part is really good. Think about the basic concept of the operator: we take pages of data, look at the hash of the partitioning columns of that data, and then output data to different places depending on the modulus of that hash.\n\nModulus, the method, is pretty expensive. Luckily, there's a faster way.\n\nWe can use bitwise arithmetic to quickly implement the method that takes a hash and outputs a destination. This, by itself, improves CPU by 35% for the operator from end to end. And it can be easily used in other parts of the code.\n\nCurious? Here’s all it takes:\n\n```java\n// This function reduces the 64 bit hashcode to [0, hashTableSize) uniformly. It first reduces the hashcode to 32 bit\n// integer x then normalize it to x / 2^32 * hashSize to reduce the range of x from [0, 2^32) to [0, hashTableSize)\nstatic int computePosition(long hashcode, int hashTableSize)\n{\n    return (int) ((Integer.toUnsignedLong(Long.hashCode(hashcode)) * hashTableSize) >> 32);\n}\n```\n\nNote that the >> operator can be replaced by direct division of 2^32, the JVM would optimize it to bit shifting anyways.\n\nSee this PR for details: https://github.com/prestodb/presto/pull/11832\n\n## Try Optimized Repartitioning\n\nThe optimization is available in mainline Presto and can be enabled using the `optimized_repartitioning` session property or the `experimental.optimized-repartitioning` configuration property. You are welcome to try it out and give us feedback.\n\n## Further reading\n\n* Here is the original issue explaining the plan https://github.com/prestodb/presto/issues/13015\n* Here is an (internal) note going into benchmarking and wins https://fb.workplace.com/notes/ying-su/how-fast-can-we-serialize-blocks/471975263562084/\n* Here is the main pull request that made it all happen https://github.com/prestodb/presto/pull/13183\n* Here’s the commit for huge improvements in hashing dispatch by using modular arithmetic. https://github.com/prestodb/presto/pull/11832"
        },
        {
          "id": "/2019/12/16/growing-the-presto-foundation",
          "metadata": {
            "permalink": "/blog/2019/12/16/growing-the-presto-foundation",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-12-16-growing-the-presto-foundation.md",
            "source": "@site/blog/2019-12-16-growing-the-presto-foundation.md",
            "title": "Join Us! Growing the Presto Foundation in 2020 and Beyond",
            "description": "The Presto Foundation (PF) was established in September 2019 as an openly governed and vendor-neutral body dedicated to scaling and diversifying the Presto community. Hosted by the Linux Foundation, PF and its Governing Board are in a unique position to make Presto the fastest and the most reliable SQL engine for massively distributed data processing.",
            "date": "2019-12-16T00:00:00.000Z",
            "formattedDate": "December 16, 2019",
            "tags": [],
            "readingTime": 5.3,
            "truncated": false,
            "authors": [
              {
                "name": "Brian Hsieh",
                "url": "https://www.linkedin.com/in/briankhsieh/"
              }
            ],
            "frontMatter": {
              "title": "Join Us! Growing the Presto Foundation in 2020 and Beyond",
              "author": "Brian Hsieh",
              "authorURL": "https://www.linkedin.com/in/briankhsieh/"
            },
            "prevItem": {
              "title": "5 design choices—and 1 weird trick — to get 2x efficiency gains in Presto repartitioning",
              "permalink": "/blog/2019/12/20/repartition"
            },
            "nextItem": {
              "title": "Table Scan: Doing The Right Thing With Structured Types",
              "permalink": "/blog/2019/09/26/tablescan-structs"
            }
          },
          "content": "The Presto Foundation (PF) was [established in September 2019](https://www.linuxfoundation.org/press-release/2019/09/facebook-uber-twitter-and-alibaba-form-presto-foundation-to-tackle-distributed-data-processing-at-scale/) as an openly governed and vendor-neutral body dedicated to scaling and diversifying the [Presto](https://prestodb.io/) community. Hosted by the Linux Foundation, PF and its Governing Board are in a unique position to make Presto the fastest and the most reliable SQL engine for massively distributed data processing.\n\nAfter many discussions, the Governing Board agreed and ratified the PF’s guiding principles and 2020 strategic goals.\n\n## Presto Foundation Guiding Principles\n\nOur guiding [principles](https://github.com/prestodb/foundation/pull/16) reflect our commitment to open governance, neutrality, and community building, listed below: \n\n### One open, neutral and united Presto community\n\nOne of the main reasons the founding members came together and created the Presto Foundation was to unite the Presto community. The Presto community and its contributors have been working on many great capabilities and connectors, and many of these haven’t made their way back into the repo, and we truly believe that these can benefit many other users. We invite all Presto community members to embrace [PrestoDB](https://github.com/prestodb/presto) and contribute back to move the project faster, reduce duplication of efforts and create wider impact for the entire community.\n\n### Open, transparent technical leadership and direction\n\nAt the end of the day, technology and technical direction is what keeps a community interested and the innovation going. We want the project to be inclusive and the technical roadmap to be fairly representative of the community’s view. In fact, the [Technical Steering C](https://github.com/prestodb/tsc)ommittee has its own open [charter](https://github.com/prestodb/tsc/blob/master/CHARTER.md) and the meetings are open for everyone to attend. We invite you to join the Technical Steering Committee meetings. We believe under the open and transparent governance of the Technical Steering Committee, the PrestoDB project will continue to grow, gain contributors, and solve some hard technical challenges.\n\n### No one individual or company is greater than the project and its community\n\n[The Linux Foundation](https://www.linuxfoundation.org/) is dedicated to building sustainable ecosystems around open source projects to accelerate technology development and commercial adoption. It is the home of Linux creator Linus Torvalds, and most importantly provides a neutral home to Linux and other projects where their development can be protected and accelerated for years to come. This important principle of neutrality has been the core of Linux Foundation as well as Apache Software Foundation so that no one individual or company agenda becomes that of the projects. Neutrality will drive the approach to governance that we have adopted so that no single company or individual can take undue influence.\n\n## 2020 Strategic Goals\n\nWith our guiding principles in mind, the PF Governing Board developed six strategic goals to guide our growth in 2020. The goals, listed in no particular order, below, are fluid and meant to evolve over time with [input from the community](https://github.com/prestodb/foundation):\n\n### Unify Presto under one neutral and [openly governed](https://github.com/prestodb/foundation/blob/master/Presto%20Technical%20Charter%2020191015.pdf) community\n\nBy establishing a neutral and openly governed body to help incorporate new contributions to Presto, we can facilitate the open sourcing and maintenance of these updates to the software, and in turn, provide great benefit to Presto’s users. Additionally, unifying Presto under one community will enable greater adoption and collaboration with benefits beyond the foundation itself.\n\n### Cultivate adoption of Presto across industries\n\nWith a history of adopting and supporting the growth of ecosystems for such projects as [Envoy](https://www.envoyproxy.io/), [Kubernetes](https://kubernetes.io/), [LetsEncrypt](https://letsencrypt.org/), [Prometheus](https://prometheus.io/), [Node.js](https://nodejs.org/), we are confident that the Linux Foundation will provide an open, collaborative, and supportive environment for the growth of the PF and the broader Presto community. Through the foundation, we invite all Presto users to embrace PrestoDB and contribute back to the project faster, allowing us to reduce duplication and create true impact.\n\n### Create value for all PF developers and members\n\nThe foundation’s neutral and open governance body is core to the PF’s ability to ensure that all members benefit from the organization, and no single company or individual can wield undue influence. Additionally, the PF’s Technical Steering Committee (TSC) is responsible for ensuring that project development is on track, software contributions are high quality, and all developer opinions and perspectives are welcomed and considered as it relates to the future of the ecosystem. We invite you to join the TSC meetings and we believe under the leadership of our TSC Chairperson Nezih Yigitbasi (PrestoDB Lead at Facebook), the PrestoDB project will continue to grow, gain contributors, and solve hard technical challenges applicable to organizations across the industry.\n\n### Grow PF membership\n\nThe PF’s success is dependent on the growth of our community. In 2020, we intend to increase PF membership through our support from the Linux Foundation, our neutral and open Governing Board, the dedication of our Technical Steering Committee, and various other initiatives that will contribute to a robust and diverse community that represents the scope and scale of Presto usage by organizations worldwide.\n\n### Champion diversity and inclusion in the community\n\nIn addition to growing our membership, the foundation is committed to broadening the diversity of our community, and making sure that all members are supported. To this end, our Governing Board ensures that all members feel welcomed regardless of their background, and our Technical Steering Committee’s [open charter](https://github.com/prestodb/tsc/blob/master/CHARTER.md) is reflective of the diverse perspectives and opinions of our members, with TSC meetings open to everyone in the community.\n\n### Create community events to foster collaboration\n\nIn 2020, we intend to host neutral events that support discussion and open collaboration between all members of our community. Through these initiatives, we can bring about unprecedented growth of the Presto ecosystem that will open up new partnership opportunities for both members and end users alike.\n\n## Moving forward: PrestoCon 2020\n\nThe journey to make PrestoDB into a successful project is just beginning. As Chairperson of the Governing Board, it gives me great pleasure to announce a key milestone in this journey that brings together our core principles and strategic goals: in early 2020, the Presto Foundation will be hosting the first-ever PrestoCon. The details are still in the works and a date will be announced shortly. We look forward to new contributions, new memberships, and new milestones.\n\nWe invite everyone who has built upon Presto to [join our community](https://prestodb.io/join.html). We truly believe that having all users and contributors working together under one neutral organization will accelerate the growth of the [Presto ecosystem](https://prestodb.io/).\n\nSincerely,\n\nBrian Hsieh, Uber\nHead of Open Source Program Office and Presto Foundation Governing Board Chairperson"
        },
        {
          "id": "/2019/09/26/tablescan-structs",
          "metadata": {
            "permalink": "/blog/2019/09/26/tablescan-structs",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-09-26-tablescan-structs.md",
            "source": "@site/blog/2019-09-26-tablescan-structs.md",
            "title": "Table Scan: Doing The Right Thing With Structured Types",
            "description": "In the previous article we saw what gains are possible when filtering early and in the right order. In this article we look at how we do this with nested and structured types.",
            "date": "2019-09-26T00:00:00.000Z",
            "formattedDate": "September 26, 2019",
            "tags": [],
            "readingTime": 10.295,
            "truncated": true,
            "authors": [
              {
                "name": "Orri Erling",
                "url": "https://www.linkedin.com/in/orrierling/"
              }
            ],
            "frontMatter": {
              "title": "Table Scan: Doing The Right Thing With Structured Types",
              "author": "Orri Erling",
              "authorURL": "https://www.linkedin.com/in/orrierling/",
              "authorFBID": 100026224749124
            },
            "prevItem": {
              "title": "Join Us! Growing the Presto Foundation in 2020 and Beyond",
              "permalink": "/blog/2019/12/16/growing-the-presto-foundation"
            },
            "nextItem": {
              "title": "Presto now hosted under the Linux Foundation",
              "permalink": "/blog/2019/09/23/linux-foundation"
            }
          },
          "content": "In the previous article we saw what gains are possible when filtering early and in the right order. In this article we look at how we do this with nested and structured types.\n\n<!--truncate-->\n\nWe use the 100G TPC-H dataset, but now we group top level columns into structs or maps.\n\nMaps, lists and structs are very common with big data because ETL jobs tend to put all data of interest in a single fact table. If the data involves any schema variability or over 100 or 200 of columns, maps tend to be used instead of top level columns. These can be copied as a unit and adding keys does not require top level schema change with its complications. In this article we mimick these practices by reshaping the TPC-H data.\n\n\nThe tables are defined as follows:\n\n```sql\nCREATE TABLE exportlineitem  (\n    orderkey BIGINT,\n    linenumber INTEGER,\n    shipment row(\n        partkey BIGINT,\n        suppkey BIGINT,\n        extendedprice DOUBLE,\n        discount DOUBLE,\n        quantity DOUBLE,\n        shipdate DATE,\n        receiptdate DATE,\n        commitdate DATE,\n        comment VARCHAR,\n    export row(\n        s_nation BIGINT,\n        c_nation BIGINT,\n        is_inside_eu INTEGER,\n        is_restricted INTEGER,\n        license row(\n            applydate DATE,\n            grantdate DATE,\n            filing_no BIGINT,\n            comment VARCHAR))\n)\nWITH (\n    format = 'ORC'\n);\n```\n\nThe `shipment` struct has the non-null top-level columns we all have come to know and love. The `export` struct is null if the customer and supplier nations are the same and present otherwise. A fraction of the rows have an additional nested export `license` struct.\n\n```sql\nCREATE TABLE lineitem_map (\n    orderkey BIGINT,\n    linenumber INTEGER,\n    ints map(INTEGER, BIGINT),\n    strs map(VARCHAR, VARCHAR)\n)\nWITH (\n    format = 'ORC'\n);\n```\n\nThis table has a map of 12 integers in `ints` and 5 strings in `strs`. The key is the column ordinal number in the original `lineitem` table as an integer in `ints` and as a string in `strs`.\n\nLike before, the tables are in ORC V2 and are compressed with Snappy. We show the Aria and baseline times as wall time seconds / CPU seconds, labeled with Aria: and Baseline: respectively. The queries were run on a desktop machine with two sockets and four hyperthreaded Skylake cores per socket clocked at 3.5GHz.\n\nFirst we compare performance of top level columns to performance of columns embedded in a non-null struct:\n```sql\nSELECT COUNT(*)\nFROM lineitem\nWHERE partkey BETWEEN 1000000 AND 2000000 AND suppkey BETWEEN 100000 AND 200000 AND extendedprice > 0;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 35                 | 1.0                     |\n| Baseline | 7                   | 80                 | 2.28                    |\n\n```sql\nSELECT COUNT(*)\nFROM exportlineitem\nWHERE shipment.partkey BETWEEN 1000000 AND 2000000 AND shipment.suppkey BETWEEN 100000 AND 200000 AND shipment.extendedprice > 0 ;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 35                 | 1.0                     |\n| Baseline | 16                  | 227                | 6.5                     |\n\nWe notice that for Aria it makes no difference whether the filtered columns are top level or in a non-null struct. We also note that none of the columns are materialized by Aria, since these are only filtered on, but they are materialized by baseline Presto.\n\n```sql\nSELECT COUNT(*), SUM(shipment.extendedprice)\nFROM exportlineitem\nWHERE shipment.partkey BETWEEN 1000000 AND 2000000 AND shipment.suppkey BETWEEN 100000 AND 200000;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 34                 | 1.0                     |\n| Baseline | 18                  | 253                | 7.44                    |\n\nNow, instead of having a filter that is always true, we retrieve the value of `extendedprice` and materialize a struct. We only materialize 1% of the structs with this predicate, and the struct only has the `extendedprice` column filled in. The cost of materialization for Aria is within the margin of error. We could of course, since we only access fields of the struct and not the whole struct, elide materializing the struct and only materialize the component columns. But the gain of this last optimization will not yield much improvement unless a larger percentage of the values are materialized and/or the struct to materialize has many fields.\n\n```sql\nSELECT COUNT(*), SUM(shipment.extendedprice), COUNT(export.license.filing_no)\nFROM exportlineitem\nWHERE shipment.suppkey BETWEEN 200000 AND 400000 AND shipment.quantity < 10;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 10                  | 58                 | 1.0                     |\n| Baseline | 30                  | 330                | 5.68                    |\n\nHere we add a second struct to the mix. We filter on members of one struct and return a field, `filing_no`, which is wrapped inside two structs. Both the `export` struct and the `license` struct inside it are nullable, i.e. not all shipments are international and not all export shipments need a license.\nThis takes longer because 4x more rows are returned in order to highlight the cost of handling null flags. We must read two levels of null flags, one for `export` and the other for the `license` substruct. Then we read the filing number for the positions where there is a `license` and fill in a null for the case where either `license` or the enclosing `export` struct is null.\n\n# Experiments\n\n```sql\nSELECT SUM(shipment.extendedprice), COUNT(export.license.filing_no)\nFROM exportlineitem\nWHERE shipment.partkey BETWEEN 200000 AND 400000 AND shipment.quantity < 10 AND export.s_nation IN (1, 3, 6) AND export.c_nation = 11;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 5.6                 | 49.8               | 1.0                     |\n| Baseline | 30                  | 309                | 6.20                    |\n\nHere we add a filter on `export`. The filters within structs are reorderable as well as the top level structs. We find that the new filter does not negatively impact running time and in fact can improve it. The best filter is evaluated first and after this all column access is sparse and only the data at interesting positions gets touched.\n\n```sql\nSELECT COUNT(*)\nFROM lineitem_map\nWHERE ints[2] BETWEEN 1000000 AND 2000000 AND ints[3] BETWEEN 100000 AND 200000;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 35                  | 422                | 1.0                     |\n| Baseline | 50                  | 706                | 1.67                    |\n\nThis case corresponds to a `lineitem` table represented as a map. This would be common if this were the feature vector for a machine learning use case, except that we would typically have hundreds of keys per map instead of the 12 here. we do the same thing as in the first query but use a direct encoded map instead. This is noticeably slower than the struct case because we must read through all the keys even if we do not look at them. However only the values for keys that are accessed need to be read. Thus we do not save in decompression but do save in materialization and filtering. Internally, this makes a filter `ints.key IN (2, 3)`. This selects the positions in the values column that we look at. Then we make a list of filters to apply to these positions. Different positions have a different filter.  There is an extra trick in this: If there are `n` filters, e.g. 2 for 2 values that we look at out of a total of 12 values in each map and the `ith` filter is false, then we can fail the next `n - i` filters without even looking at the data.\n\nBecause the map is only filtered on, we do not create any `MapBlock` at any point in the query.\n\nProcessing the positional filter is only around 5% of the query CPU. The bulk goes into decoding and skipping over the ORC columns for keys and values.\n\nThe Facebook DWRF addition of flat map brings this again into the struct range. A flat map is a columnar representation where we have a separate column for each key that occurs at least once within a stripe. This is much like the representation for a struct, except that nested columns have an extra flag that tells whether they are present in each of the maps.\n\n```sql\nSELECT COUNT(*), SUM(ints[6])\nFROM lineitem_map\nWHERE ints[2] BETWEEN 1000000 AND 2000000 AND ints[3] BETWEEN 100000 AND 200000;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 37                  | 476                | 1.0                     |\n| Baseline | 55                  | 710                | 1.49                    |\n\nWhen we use values in a map outside of simple filters on these values, we need to actually construct a map  column and return it from table scan to the next operator. But here we know that only key 6 is actually used by the query, so we can leave out the 11 other values that would be in the map. This takes a little longer than the previous query but the extra cost is not very high because the resulting map only has the entry for key 6 filled in. The `MapBlock` and its hash tables are thus only 1/12th of what they would otherwise be.\n\n\n```sql\nSELECT COUNT(*), SUM(ints[6])\nFROM lineitem_map\nWHERE ints[2] BETWEEN 1000000 AND 2000000 AND ints[3] BETWEEN 100000 AND 200000 AND strs['13'] = 'AIR';\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 45                  | 572                | 1.0                     |\n| Baseline | 76                  | 1080               | 1.88                    |\n\nHere we add another map to look at. The relative gain against `LazyBlock` is now higher because baseline materializes the `strs` map for all rows and all keys while the access is fairly sparse.  Also, making the hash table for all the string keys is very expensive and generates a lot of garbage in single-use `Slices`. Here we do not materialize any string map but just look at the appropriate places in the value column. The value column must still be uncompressed, which takes time since this contains the `comment` column of the original table. The high cardinality of `comment` also prevents dictionary encoding for the values. The keys on the other hand are encoded as a string dictionary.\n\n```sql\nSELECT COUNT(*), SUM(ints[6])\nFROM lineitem_map\nWHERE ints[2] + 1 BETWEEN 1000000 AND 2000000 AND ints[3] + 1 BETWEEN 100000 AND 200000 AND strs['15'] LIKE '%theodol%';\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 51                  | 703                | 1.0                     |\n| Baseline | 77                  | 1083               | 1.54                    |\n\nHere we evaluate code against the maps, hence we cannot elide materializing these. We still win because the `strs` map is sparsely accessed and only the keys that are actually needed get extracted into `MapBlock` instances.\n`\n\n## Try Aria\nThe prototype of Aria is [available](https://github.com/aweisberg/presto/tree/tablescan-structs) to experiment with along with [instructions](https://github.com/aweisberg/presto/blob/tablescan-structs/BENCHMARK.md) on how to try these queries yourself.\n\nThe ideas presented here are currently being integrated into mainline Presto.\n\n# Conclusions\n\nWe see that pruning subfields and map keys produces solid value and as one would expect never loses. But since we must still uncompress and skip over all the map keys and values, the gains are less for maps as opposed to structs. Maps are especially common with machine learning applications where these encode the feature vector for model training.  We commonly see maps of several thousand keys, of which a handful are accessed. Thus the gains in these cases tend to be higher than seen here with small maps.\n\nThe 'schemaless struct', i.e. flat map will equalize the situation.\n\nIn the next installment we will look at the experience gained in building and testing this functionality.  We will see where the complexities and pitfalls lie and talk about some of the surprises and catches we met when testing this on production workload."
        },
        {
          "id": "/2019/09/23/linux-foundation",
          "metadata": {
            "permalink": "/blog/2019/09/23/linux-foundation",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-09-23-linux-foundation.md",
            "source": "@site/blog/2019-09-23-linux-foundation.md",
            "title": "Presto now hosted under the Linux Foundation",
            "description": "We are excited to announce today, in partnership with Alibaba, Facebook, Twitter, and Uber, the launch of the Presto Foundation, a non-profit organization under the umbrella of the Linux Foundation.",
            "date": "2019-09-23T00:00:00.000Z",
            "formattedDate": "September 23, 2019",
            "tags": [],
            "readingTime": 0.685,
            "truncated": false,
            "authors": [
              {
                "name": "Ariel Weisberg",
                "url": "https://www.linkedin.com/in/ariel-weisberg-a5b6899"
              }
            ],
            "frontMatter": {
              "title": "Presto now hosted under the Linux Foundation",
              "author": "Ariel Weisberg",
              "authorURL": "https://www.linkedin.com/in/ariel-weisberg-a5b6899",
              "authorFBID": 100006183888902
            },
            "prevItem": {
              "title": "Table Scan: Doing The Right Thing With Structured Types",
              "permalink": "/blog/2019/09/26/tablescan-structs"
            },
            "nextItem": {
              "title": "Memory Management in Presto",
              "permalink": "/blog/2019/08/19/memory-tracking"
            }
          },
          "content": "We are excited to announce today, in partnership with [Alibaba](https://www.alibaba.com/), [Facebook](https://www.facebook.com/), [Twitter](https://twitter.com/home), and [Uber](https://www.uber.com), the launch of the Presto Foundation, a non-profit organization under the umbrella of the [Linux Foundation](https://www.linuxfoundation.org/).\n\nHosting by the Linux Foundation opens up the Presto community to a broader ecosystem of users and contributors. The Presto Foundation's open and neutral governance enables the community to influence Presto's future, which will also make it more attractive to developers. Together, we will raise Presto's performance, scalability, and reliability to new heights that could never have been reached alone.\n\nThis is a new chapter for the Presto open source project. We are very excited for what lies ahead!\n\n[Full announcement on the Linux Foundation’s website.](https://www.linuxfoundation.org/uncategorized/2019/09/facebook-uber-twitter-and-alibaba-form-presto-foundation-to-tackle-distributed-data-processing-at-scale/)\n\n[Uber's announcement of their participation.](https://eng.uber.com/presto-foundation/)\n\n**Media coverage**\n* [Yahoo Finance (Release Syndication)](https://finance.yahoo.com/news/facebook-uber-twitter-alibaba-form-presto-foundation-tackle-130000947.html)\n* [SD Times](https://sdtimes.com/data/the-presto-foundation-launches-under-the-linux-foundation/)\n* [Silicon Angle](https://siliconangle.com/2019/09/23/presto-establishes-formal-community-linux-foundation/)\n* [Fierce Telecom](https://www.fiercetelecom.com/telecom/presto-chango-data-query-engine-now-hosted-by-linux-foundation)"
        },
        {
          "id": "/2019/08/19/memory-tracking",
          "metadata": {
            "permalink": "/blog/2019/08/19/memory-tracking",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-08-19-memory-tracking.md",
            "source": "@site/blog/2019-08-19-memory-tracking.md",
            "title": "Memory Management in Presto",
            "description": "In a multi-tenant system like Presto careful memory management is required to keep the system stable and prevent individual queries from taking over all the resources. However, tracking the memory usage of data structures in an application (Presto) running on the Java Virtual Machine (JVM) requires a significant amount of work. In addition, Presto is a distributed system, which makes the problem more complicated. This post provides an overview of how memory management works in Presto, and provides info about the various memory management related JMX counters/endpoints that can be used for monitoring production clusters.",
            "date": "2019-08-19T00:00:00.000Z",
            "formattedDate": "August 19, 2019",
            "tags": [],
            "readingTime": 8.925,
            "truncated": true,
            "authors": [
              {
                "name": "Nezih Yigitbasi",
                "url": "https://www.linkedin.com/in/nezihyigitbasi/"
              }
            ],
            "frontMatter": {
              "title": "Memory Management in Presto",
              "author": "Nezih Yigitbasi",
              "authorURL": "https://www.linkedin.com/in/nezihyigitbasi/",
              "authorFBID": 100000082666878
            },
            "prevItem": {
              "title": "Presto now hosted under the Linux Foundation",
              "permalink": "/blog/2019/09/23/linux-foundation"
            },
            "nextItem": {
              "title": "Presto Unlimited: MPP SQL Engine at Scale",
              "permalink": "/blog/2019/08/05/presto-unlimited-mpp-database-at-scale"
            }
          },
          "content": "In a multi-tenant system like Presto careful memory management is required to keep the system stable and prevent individual queries from taking over all the resources. However, tracking the memory usage of data structures in an application (Presto) running on the Java Virtual Machine (JVM) requires a significant amount of work. In addition, Presto is a distributed system, which makes the problem more complicated. This post provides an overview of how memory management works in Presto, and provides info about the various memory management related JMX counters/endpoints that can be used for monitoring production clusters.\n\n<!--truncate-->\n\n## Memory Pools\n\nTo understand the details described in the rest of the article it would be instructive to first take a look at different types of memory allocations and memory pools in Presto.\n\nIn the Presto engine there are two types of memory allocations: user and system. User memory is the type of memory that's easier for the users to reason about given the input data (e.g., the memory usage of an aggregation is proportional to its\ncardinality). System memory, however, is the type of memory that's a byproduct of the execution (e.g., table scan and write buffers), and doesn't necessarily have a strong correlation with the query input/shape.\n\nThroughout the execution of a query, operator implementations allocate user/system memory from memory pools on the workers. Presto has two memory pools: general and reserved (historically it also had a system memory pool, and it has been removed for various reasons, but that's another story). The general pool serves the user and system memory allocations in \"normal\" mode of operation of the system. However, when there is a worker that has exhausted its general pool the reserved pool comes into the play. In that state, the coordinator selects the query with the largest total (user + system) memory reservation across the cluster, and assigns that query to the reserved pool on all workers. This assignment guarantees the completion of that particular query and also guarantees forward progress in the system. \n\nIt is worth noting that the reserved pool is set aside on startup and it's as large as the largest query that the cluster is configured to execute with the `query.max-total-memory-per-node` config property. However, this is not efficient as the reserved pool is unused in normal mode of operation. Therefore, the engine also supports disabling the reserved pool with a config (`experimental.reserved-pool-enabled`). To guarantee forward progress in the system when the reserved pool is disabled, the OOM killer should be enabled (`query.low-memory-killer.policy=total-reservation-on-blocked-nodes`). The OOM killer, which is running on the coordinator, will trigger when the cluster goes into the OOM state, and will kill some query to free up space on the workers guaranteeing the progress of other blocked queries that are waiting for memory.\n\n## Memory Limits\n\nThe Presto engine has two main mechanisms to keep itself stable under high memory pressure. One of these mechanisms is the configured local (worker level) and distributed memory limits. When a query hits either of these limits it gets killed by the Presto engine with a special error code (`EXCEEDED_LOCAL_MEMORY_LIMIT`/`EXCEEDED_GLOBAL_MEMORY_LIMIT`). The local memory limit can be configured with the `query.max-memory-per-node` and `query.max-total-memory-per-node` config parameters. The former configures the worker level user memory limit while the latter configures the worker level total (user + system) memory limit. Similarly, `query.max-memory` and `query.max-total-memory` can be used to configure the distributed user and total memory limits, respectively. For a detailed description of these properties please refer to [1]. The other mechanism to keep the system stable under high memory pressure is the cooperative blocking mechanism built into the memory tracking framework. When the general memory pool gets full the operators will block until memory is available in the general pool. This mechanism prevents aggressive queries from filling up the JVM heap and cause reliability issues.\n\n\n## How Does Presto Track Memory?\n\nEach Presto operator (e.g., `ScanFilterAndProjectOperator`, `HashAggregationOperator`, etc.) has an `OperatorContext` that has a bunch of info about the operator, counters, and the methods to get/create memory contexts. Memory context instances are used to  account for memory in the Presto engine. A common pattern in operator implementations is to get/create a memory context from the operator context, and then call `setBytes(N)` on the memory context instance to account for `N` bytes of memory for this particular operator. It is worth noting that calculating `N` is not always trivial as the engine has complex data structures and we need to properly account for the Java object overhead and we need to make sure that we don't account for a piece of memory multiple times if there are multiple references to it (e.g., multiple references pointing to a single physical copy). The JOL (Java Object Layout) library [2] helps solving the first problem by providing the APIs to get the retained size of Java objects in an easy way. However, the latter requires careful accounting of the data structures throughout the engine.\n\nThe memory contexts are organized in a tree hierarchy that reflects the hierarchy of the operators, drivers, pipelines, tasks, and the query. The memory accounted by all operators running for a particular task and a query are summed all the way up the tree, and eventually gets accounted for in the memory pool. Through this tracking mechanism the memory pools can track the memory used by every operator and every query running on that worker, which is exposed via a REST endpoint mentioned in the last section below.\n\nThe engine also sets aside some headroom (`memory.heap-headroom-per-node`) to account for the allocations that it cannot track, for example, due to allocations in the 3rd party dependencies, local/stack allocations during execution, etc. Without enough headroom it's possible to fill up the JVM heap as the general pool gets full, and that may cause reliability problems.\n\nFor more details about memory management in Presto please refer to our ICDE'19 paper [3].\n\n## How About the Coordinator?\n\nSo far we have mostly looked at the worker-side of memory management. The coordinator also has various responsibilities to help with the memory management across the cluster. \n\nThe coordinator collects the memory pool information from all the workers periodically and builds the global state of all memory pools in the cluster. This state is used for taking decisions (e.g., which query to kill when cluster is in OOM state or to kill a query if it exceeds the distributed memory limit) and for monitoring.\n\nCoordinator has multiple responsibilities related to memory management: \n\n- __Enforce distributed memory limits:__ If a query reserves more than the configured distributed user/total memory limits, the coordinator kills the query with a special error code (`EXCEEDED_GLOBAL_MEMORY_LIMIT`).\n- __Assign queries to reserved pool__: If any worker in the cluster exhausts its general pool, the coordinator assigns the largest query to the reserved pool on all workers.\n- __Kill queries when the cluster is in the OOM state (a.k.a. the OOM killer)__: When the cluster goes into the OOM state the coordinator uses the configured heuristic (`query.low-memory-killer.policy`) to select a query to kill. For a cluster to go into the OOM state one or more workers should exhaust their general pool and the reserved pool should have a query assigned to it (if reserved pool is enabled).\n- __Detect memory accounting leaks:__ Life is not perfect. So, it's possible that there are memory accounting bugs in the engine causing accounting leaks, that is, a query has non-zero memory reservation in the general pool even after it completes. Such leaks have a bunch of side effects, such as causing premature exhaustion of the general pool and preventing OOM killer from kicking in. The reason OOM killer cannot kick in when there are leaks is that it waits for the previously killed query to leave the system, however when  there are leaks previously killed query will still have a non-zero reservation in the memory pool state (and hence will not leave the system). This is a critical problem, because preventing OOM killer from kicking in may cause the cluster to get stuck in the OOM state, which will significantly reduce the cluster throughput. Presto addresses this problem by running a cluster memory leak detector on the coordinator to mark a query as \"possibly leaked\" if the query has finished 1m ago, but it still has non-zero memory reservation on the workers. With that the OOM killer can just coordinate with the leak detector to continue functioning properly.\n\nMost of this functionality is implemented in Presto's cluster memory manager that runs on the coordinator, please see [4] for the implementation.\n\n## Getting Visibility Into the Memory Management Framework\n\nFinally, let's take a look at some of the important JMX counters and REST endpoints that may help with getting more visibility into the memory management framework, and help with monitoring production clusters.\n\nThe memory pools exports various counters for monitoring the used/free/max memory in the pools. For example, the free memory in the general pool on a worker can be monitored with the `presto.com.facebook.presto.memory:type=MemoryPool:name=general:FreeBytes` JXM counter. Similarly, the amount of memory allocated in the reserved pool can be monitored on a worker with the `presto.com.facebook.presto.memory:type=MemoryPool:name=reserved:ReservedBytes` JMX counter.\n\nThe coordinator exports similar counters for monitoring the memory pools, but at the cluster level. For example, `presto.com.facebook.presto.memory:type=ClusterMemoryPool:name=reserved:AssignedQueries` can be used to track the number of active queries in the reserved pools across all workers in the cluster. Another interesting counter is `presto.com.facebook.presto.memory:type=ClusterMemoryPool:name=general:BlockedNodes`, which can be used to monitor the number of \"blocked\" workers, that is, the number of workers that have exhausted their general pool. These two counters can be handy to understand whether the cluster is in the OOM state. Another useful counter is `presto.com.facebook.presto.memory:name=ClusterMemoryManager:QueriesKilledDueToOutOfMemory`, which is for monitoring the number of queries killed by the OOM killer.\n\nThe workers provide the REST endpoint `/v1/memory/{poolName}` to expose detailed memory tracking information at the query and the operator level where `{poolName}` is the name of the memory pool (general or reserved). This info can be useful to get deep visibility into the allocation information per operator per query. Simiarly, this info is rolled up at the cluster level and exposed via the `/v1/cluster/memory` endpoint on the coordinator.\n\nWhen debugging reliability problems in production deployments one usually requires these counters plus the JVM's memory-related [5] and garbage collection-related [6] JMX counters. Using both Presto's view of the memory and the JVM's view of the memory and garbage collection activity provides a comprehensive coverage of the state of the system.\n\nIf you have questions about Presto internals including memory management please join [the Presto Slack community](https://prestodb.github.io/community.html).\n\n## References\n[1] [Presto Configuration Reference for Memory Management](https://prestodb.github.io/docs/current/admin/properties.html#memory-management-properties)\n\n[2] [Java Object Layout library](https://openjdk.java.net/projects/code-tools/jol/)\n\n[3] [Presto: SQL on Everything ICDE'19 paper](https://research.fb.com/publications/presto-sql-on-everything/)\n\n[4] [ClusterMemoryManager implementation](https://github.com/prestodb/presto/blob/master/presto-main/src/main/java/com/facebook/presto/memory/ClusterMemoryManager.java)\n\n[5] [Java MemoryMXBean Reference](https://docs.oracle.com/javase/10/docs/api/java/lang/management/MemoryMXBean.html)\n\n[6] [Java GarbageCollectorMXBean Reference](https://docs.oracle.com/javase/10/docs/api/com/sun/management/GarbageCollectorMXBean.html)"
        },
        {
          "id": "/2019/08/05/presto-unlimited-mpp-database-at-scale",
          "metadata": {
            "permalink": "/blog/2019/08/05/presto-unlimited-mpp-database-at-scale",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-08-05-presto-unlimited-mpp-database-at-scale.md",
            "source": "@site/blog/2019-08-05-presto-unlimited-mpp-database-at-scale.md",
            "title": "Presto Unlimited: MPP SQL Engine at Scale",
            "description": "Wenlei Xie, Andrii Rosa, Shixuan Fan, Rebecca Schlussel, Tim Meehan",
            "date": "2019-08-05T00:00:00.000Z",
            "formattedDate": "August 5, 2019",
            "tags": [],
            "readingTime": 5.505,
            "truncated": true,
            "authors": [
              {
                "name": "Wenlei Xie",
                "url": "https://www.linkedin.com/in/wenleix/"
              }
            ],
            "frontMatter": {
              "title": "Presto Unlimited: MPP SQL Engine at Scale",
              "author": "Wenlei Xie",
              "authorURL": "https://www.linkedin.com/in/wenleix/",
              "authorFBID": 681470066
            },
            "prevItem": {
              "title": "Memory Management in Presto",
              "permalink": "/blog/2019/08/19/memory-tracking"
            },
            "nextItem": {
              "title": "Complete Table Scan: A Quantitative Assessment",
              "permalink": "/blog/2019/07/23/complete-table-scan"
            }
          },
          "content": "Wenlei Xie, Andrii Rosa, Shixuan Fan, Rebecca Schlussel, Tim Meehan\n\nPresto is an open source distributed SQL query engine for running analytic queries against data sources of all sizes ranging from gigabytes to petabytes.\n\nPresto was originally designed for interactive use cases, however, after seeing the merit in having a single interface for both batch and interactive, it is now also used heavily for processing batch workloads [6]. As a concrete example, more than 80% of new warehouse batch workloads at Facebook are developed on Presto. Its flexible “connector” design makes it possible to run queries against heterogeneous data sources — such as joining together Hive and MySQL tables without preloading the data.\n\nHowever, memory-intensive (many TBs) and long-running (multiple hours) queries have been major pain points for Presto users. It is difficult to reason how much memory queries will use and when it will hit memory limit, and failures in long-running queries cause retries which create landing time variance. To improve user experience and scale MPP Database to large ETL workloads, we started this Presto Unlimited project.\n\n<!--truncate-->\n\n## Grouped Execution\n\n[Grouped execution](https://github.com/prestodb/presto/pull/8951) was developed to scale Presto to run memory-intensive queries by leveraging table partitioning. See [Stage and Source Scheduler and Grouped Execution](https://github.com/prestodb/presto/wiki/Stage-and-Source-Scheduler-and-Grouped-Execution) for information about how it works.\n\nConsider the following query, where table `customer` and `orders` are already bucketed on `custkey`:\n\n```sql\nSELECT ...\nFROM customer JOIN orders\nUSING custkey\n```\n\nWithout grouped execution, the workers will build hash table using all the data on the build side (Table `orders`):\n\n![Ungrouped Execution](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/ungrouped.png)\n\nHowever, since `customer` and `orders` are already bucketed, Presto can schedule the query execution in a more intelligent way to reduce peak memory consumption: for each bucket `i`, joining the bucket `i` on table `customer` and `orders` can be done independently! In Presto engine, we call this computation unit a “lifespan”:\n\n![Grouped Execution](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/grouped.png)\n\nGrouped execution has been enabled in Facebook's production environment for over a year, and supports queries that would otherwise require tens of TBs, in some cases over 100 TB, of distributed memory.\n\n## Presto Unlimited\n\nIn this section we are going to introduce the two projects we worked on in the last half, aimed at the two pain points discussed in the introduction section:\n\n- Exchange materialization for memory-intensive queries (many TBs)\n- Recoverable grouped execution for long-running queries (multiple hours)\n\nA few initial production pipelines in Facebook warehouse are already benefiting from these two features.\n\n\n### Exchange Materialization\n\nWhile grouped execution serves as the foundation for scaling Presto to large batch queries, it doesn't work for non-bucketed tables. In order to make grouped execution work for such cases, we could materialize the exchange by writing data into intermediate bucketed tables.\n\nConsider the same example query, the original simplified plan will be like the following:\n\n![Remote Exchange](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/remote_exchange.png)\n\nWhile the default remote network exchange is efficient and fast, it requires all the join workers to run concurrently.\nMaterializing intermediate exchange data to disk opens opportunities for more flexible job scheduling, and using less memory by scheduling a group of lifespans at the same time.\n\nWhen exchanges are materialized, the plan will first be “sectioned”, and `ExchangeNode` will be replaced by `TableWriterNode`/`TableFinishNode` and `TableScanNode`:\n\n![Remote Exchange](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/materialized_exchange.png)\n\nAs a starting point, we introduced a new configuration to allow a query to materialize all exchanges (`exchange_materialization_strategy`). In the future, whether to materialize exchanges can be decided by Cost-Based Optimizer, or even user hints, in order to seek a better trade-off between reliability and efficiency.\n\nFor more details, please see the design doc at [#12387](https://github.com/prestodb/presto/issues/12387).\n\n### Recoverable Grouped Execution\n\nGrouped execution also enables partial query failure recovery, as now each lifespan of the query is independent and can be retried independently. As illustrated in the following figure:\n\n![Remote Exchange](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/grouped_recovery.png)\n\nFor more details, please see the design doc at [#12124](https://github.com/prestodb/presto/issues/12124)\n\n## Future Work\n\nWe are also thinking about the following future work:\n- Fault-Tolerant Exchange Execution\n    * With exchange materialization and recoverable grouped execution, the only reason that a single worker failure can fail the query is during the exchange stage.\n    * The long term, the solution is to implement MapReduce-style shuffle, or integrate with a fault-tolerant distributed shuffle service such as [Cosco](https://databricks.com/session/cosco-an-efficient-facebook-scale-shuffle-service) or [Crail](https://crail.incubator.apache.org/).\n    * The short term plan is to support recoverability for each execution section. Thus the query can restart from the last checkpoint instead of the beginning.\n\n- Reliable and Scalable Coordinator\n    * The coordinator becomes a single point of failure once query execution can survive worker failures.\n    * Coordinator high-availability and scalability is a crucial future work for Presto Unlimited project.\n\n- Resource Management for Presto Unlimited\n    * With more larger queries onboard we need to actively monitor whether the current resource management needs improvement and adapt to the potential workload shifts introduced by Presto Unlimited.\n    * Lifespan provides a unit for resource management at fine granularity in addition to being a smaller unit of retry. This opens opportunities for fine-grained resource management.\n\n\n## Parallel Databases Meet MapReduce\n\nWith Presto Unlimited, Presto executes large ETL queries in a similar way to MapReduce. Consider the following simple aggregation query:\n\n```sql\nSELECT custkey, SUM(totalprice)\nFROM orders\nGROUP BY custkey\n```\n\nThe following figure illustrates how the query will be executed with Presto Unlimited:\n\n![Presto Unlimited Execution](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/presto_unlimited_exec.png)\n\nIn comparison, here is how the query will be executed without Presto Unlimited:\n\n![Presto Normal Execution](/img/blog/2019-08-05-presto-unlimited-mpp-database-at-scale/presto_normal_exec.png)\n\nSpilling is used in parallel databases to support memory-intensive queries [5, 7]. Note spilling and Presto Unlimited leverage the same fundamental idea: first partition data into intermediate result, and operate on a small chunk of data at a time to reduce peak memory usage. Spilling does this in a *lazy* fashion: it only writes intermediate data when the join/aggregate is running out of memory, while Presto Unlimited *eagerly* materializing intermediate results prior to the join/aggregation execution. This MapReduce-style execution also allows much easier fault-tolerance implementation, as each partition can be retried independently.\n\nDuring the last decade, the standard approach to support SQL at scale is to build parallel database on a MapReduce-like runtime: Tenzing [1], Hive [2], SCOPE [3], SparkSQL [4], F1 Query [5], etc.\n\nTo the best of our knowledge, Presto Unlimited is the first attempt to meet parallel database and MapReduce in a different direction, as it brings MapReduce-style execution to the parallel database.\n\n## Reference\n\n[1] [Tenzing A SQL Implementation On The MapReduce Framework](https://ai.google/research/pubs/pub37200)\n\n[2] [Hive - A Petabyte Scale Data Warehouse using Hadoop](https://www.facebook.com/notes/facebook-engineering/hive-a-petabyte-scale-data-warehouse-using-hadoop/89508453919/)\n\n[3] [SCOPE: parallel databases meet MapReduce](https://dl.acm.org/citation.cfm?id=2387351)\n\n[4] [Spark SQL: Relational Data Processing in Spark](https://dl.acm.org/citation.cfm?id=2742797)\n\n[5] [F1 Query: Declarative Querying at Scale](https://ai.google/research/pubs/pub47224)\n\n[6] [Presto: SQL on Everything](https://research.fb.com/publications/presto-sql-on-everything/)\n\n[7] [HAWQ: A Massively Parallel Processing SQL Engine in Hadoop](https://dl.acm.org/citation.cfm?id=2595636)"
        },
        {
          "id": "/2019/07/23/complete-table-scan",
          "metadata": {
            "permalink": "/blog/2019/07/23/complete-table-scan",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-07-23-complete-table-scan.md",
            "source": "@site/blog/2019-07-23-complete-table-scan.md",
            "title": "Complete Table Scan: A Quantitative Assessment",
            "description": "In the previous article we looked at the abstract problem statement and possibilities inherent in scanning tables. In this piece we look at the quantitative upside with Presto. We look at a number of queries and explain the findings.",
            "date": "2019-07-23T00:00:00.000Z",
            "formattedDate": "July 23, 2019",
            "tags": [],
            "readingTime": 9.735,
            "truncated": true,
            "authors": [
              {
                "name": "Orri Erling",
                "url": "https://www.linkedin.com/in/orrierling/"
              }
            ],
            "frontMatter": {
              "title": "Complete Table Scan: A Quantitative Assessment",
              "author": "Orri Erling",
              "authorURL": "https://www.linkedin.com/in/orrierling/",
              "authorFBID": 100026224749124
            },
            "prevItem": {
              "title": "Presto Unlimited: MPP SQL Engine at Scale",
              "permalink": "/blog/2019/08/05/presto-unlimited-mpp-database-at-scale"
            },
            "nextItem": {
              "title": "Everything You Always Wanted To Do in Table Scan",
              "permalink": "/blog/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan"
            }
          },
          "content": "In the previous article we looked at the abstract problem statement and possibilities inherent in scanning tables. In this piece we look at the quantitative upside with Presto. We look at a number of queries and explain the findings.\n\nThe initial impulse motivating this work is the observation that table scan is by far the #1 operator in Presto workloads I have seen. This is a little over half of all Presto CPU, with repartitioning a distant second, at around 1/10 of the total. The other half of the motivation is ready opportunity: Presto in its pre-Aria state does almost none of the things that are common in table scan.\n\n<!--truncate-->\n\nFor easy reproducibility and staying away from proprietary material, we use a TPC-H dataset at scale factor 100 running on a desktop machine with two sockets and four hyperthreaded Skylake cores per socket clocked at 3.5GHz. The data is compressed with Snappy and we are running with warm OS cache. The Presto is a modified 0.221 where the Aria functionality can be switched on and off. Not to worry, we will talk about disaggregated storage and IO in due time but the basics will come first.\n\n# Simple scan\n\nThe base case for scan optimization is the simplest possible query:\n\n```sql\nSELECT SUM(extendedprice)\nFROM lineitem\nWHERE suppkey = 12345;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 2                   | 14.2               | 1.0                     |\n| Baseline | 3                   | 29.0               | 2.04                    |\n\n\nThis is a 2x win for the most basic case. This selects approximately 600 rows from 600 million rows and spends all its time in scan. It is here worthwhile to step through exactly how each implementation does this, as we need to understand this when analyzing the more complex cases.\n\n## Mechanics of a scan\n\nBaseline Presto does this as follows: The `OrcPageSource` produces consecutive `Page` instances that contain a `LazyBlock` for each column. This operation as such takes no time since the `LazyBlock` instances are just promises. The actual work takes place when evaluating the generated code for the comparison. This sees that the column is not loaded, loads all the values in the range of the `LazyBlock`, typically 1024 values and then does the operation and produces a set of passing row numbers. This set is empty for all but 1/100k of the cases. If this is empty, the `LazyBlock` for `extendedprice` is not touched. If there are hits, the `extendedprice` `LazyBlock` is loaded and the values for the selected rows are copied out. When this happens, 1024 values are decoded from the column and most often one of them is accessed. Loading a `LazyBlock` allocates memory for each value. In the present case this becomes garbage immediately after first use. The same applies to the values in extended price, of which only one is copied to a `Block` of output. This is handled by a special buffering stage that accumulates rows from multiple loaded `LazyBlock` instances until there is a minimum batch worth of rows to pass to the next operator.\n\nThe Aria implementation works as follows: The `OrcRecordReader` has an adaptable column order. It reads up to a full row group worth of data for the first column. When decoding the encoded values, it applies a callback to each value. The callback will either test the value and record the row number for passing values or copy out the value into a results buffer of the column or both. In the case of `suppkey = 12345` we only record the row number. Since for virtually all row groups there is no hit we do not touch the second column. If there is a hit, we touch the hit rows of the second column. This has the effect of copying the decoded value into the column's result buffer. We keep going from row group to row group until enough data has been accumulated, at which point we return it as a `Page`. The `Block` instances in the `Page` are loaded and may either be views into the column reader's buffers or copies of the data, depending on what kind of downstream operator we have. If the downstream operator does not keep references to memory of processed `Block` instances, we can reuse the memory, which is owned by the column readers.\n\n## Allocation overhead\n\nWe execute the query three times with both baseline and Aria and consider the top 5 memory allocations:\n\nAria:\n```java\n12378062280 54.03% 3261 byte[] (out)\n\n1137340888 4.96% 1881 java.lang.Object[]\n\n1058280112 4.62% 764 byte[]\n\n941224808 4.11% 280 com.facebook.presto.orc.metadata.statistics.ColumnStatistics\n\n908822304 3.97% 272 com.facebook.presto.orc.proto.OrcProto$ColumnStatistics\n```\n\nBaseline:\n\n```java\n12370936336 28.75% 3146 byte[] (out)\n\n9689400112 22.52% 2399 long[]\n\n4661750784 10.84% 1128 com.facebook.presto.spi.block.LongArrayBlock\n\n2551827928 5.93% 766 boolean[]\n\n1289650912 3.00% 1982 java.lang.Object[]\n```\n\nIn both cases, the top item is `byte[]`, which comes from most often allocating new memory for raw data read from the ORC file. There is no reason whatever to do this. There is another Aria fix that saves another 10% by fixing this but this is not in scope here. We will talk about what to do with this allocation when we cover smart IO and buffering in later articles.\n\nThe main difference is that the `long[]` allocation for `suppkey` values that are tested once and then dropped on the floor are gone. The other observation is that there is a lot of memory allocated for column row group statistics. Again there is no need for allocation and even if these were allocated, there is no need to read stats for columns that have no filters. The use of the stats is here to eliminate row groups based on column min/max. The current implementation reads stats for all columns of the table in any case. For larger queries this is not that significant, but this does stand out in this minimal case.\n\n# Complex queries\n## Reduced materialization\nWe have now covered the core basics. Let's see how these play out with more complex queries.\n```sql\nSELECT COUNT(*), SUM(extendedprice), SUM(quantity)\nFROM lineitem\nWHERE partkey BETWEEN 10000000 AND 10200000 AND suppkey BETWEEN 500000 AND 510000;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 41.5               | 1.0                     |\n| Baseline | 6                   | 72                 | 1.73                    |\n\nIn this example there is a 1/10K selection that consists of two 1/100 filters. With `LazyBlock`, the two first columns are materialized in their entirety and 1/10th of the last two columns is materialized because we have on the average one hit in a row group and 1K rows get materialized for each load. In Aria we materialize none of this, which is the principal difference, much as in the previous example.\n\n```sql\nSELECT sum (partkey)\nFROM lineitem\nWHERE quantity < 10;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 2                   | 23.4               | 1.0                     |\n| Baseline | 5                   | 49.5               | 2.11                    |\n\nHere we read with less selectivity, taking 1/5 of the rows.\n\n```sql\nSELECT MAX(orderkey), MAX(partkey), MAX(suppkey)\nFROM lineitem\nWHERE suppkey > 0;\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 37.8               | 1.0                     |\n| Baseline | 4                   | 54.9               | 1.45                    |\n\nHere we read all rows. The difference is not very large since there is only so much that can be done when having to decode and copy everything. The principal inefficiency of baseline is `BlockBuilder` and again the fact of always allocating new memory. The main point of this query is to show that the techniques used here are never worse than baseline.\n\n## Filter reordering\n```sql\nSELECT count (*)\nFROM lineitem\nWHERE partkey < 19000000 AND suppkey < 900000 AND quantity < 45 AND extendedprice < 9000;\n```\n| Version              | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------------------- | ------------------- | ------------------ | ----------------------- |\n| Aria                 | 5                   | 52                 | 1.0                     |\n| Aria w/no reordering | 5                   | 57                 | 1.09                    |\n| Baseline             | 7                   | 96                 | 1.84                    |\n\nHere we have a conjunction of low selectivity filters, each true approximately 9/10 of the time. We are running Aria with and without filter reordering. We see about 10% gain from reordering even in a situation where the difference between different filter orders is small. We discussed the basic principle of reordering in [Everything You Always Wanted To Do in Table Scan](/blog/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan). Measure the time per dropped row and put the fastest first.\n\n```sql\nSELECT COUNT(*), MAX(partkey)\nFROM hive.tpch.lineitem\nWHERE comment LIKE '%fur%' AND partkey + 1 < 19000000 AND suppkey + 1 < 100000;\n```\n| Version              | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------------------- | ------------------- | ------------------ | ----------------------- |\n| Aria                 | 7                   | 98.7               | 1.0                     |\n| Aria w/no reordering | 24                  | 339                | 3.43                    |\n| Baseline             | 25                  | 361                | 3.65                    |\n\nHere we have a case where adaptivity makes a large difference with expensive filter expressions. The key to the puzzle is that the conjunct on `partkey` is cheap and selects 1/10. The like is 1/5 and expensive. The `partkey` is cheap and 19/20. Putting `suppkey` first wins the race. This happens without any reliance on statistics or pre-execution optimization and will adapt at run time if the data properties change.\n\n## More flexible filters\n\n```sql\nSELECT COUNT(*), SUM(extendedprice)\nFROM lineitem\nWHERE shipmode LIKE '%AIR%' AND shipinstruct LIKE '%PERSON';\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 44.2               | 1.0                     |\n| Baseline | 21                  | 271                | 6.13                    |\n\nThe filtered columns are of low cardinality and are encoded as dictionaries. This is an example of evaluating an expensive predicate on only distinct values. Baseline Presto misses the opportunity because all filters are generated into a monolithic code block. Aria generates filter expressions for each distinct set of required columns. In this case the filters are independent and reorderable.\n\n```sql\nSELECT COUNT(*), SUM(extendedprice)\nFROM lineitem\nWHERE shipmode LIKE '%AIR%';\n```\n| Version  | Wall time (seconds) | CPU time (seconds) | Baseline CPU / Aria CPU |\n| -------- | ------------------- | ------------------ | ----------------------- |\n| Aria     | 4                   | 38.5               | 1.0                     |\n| Baseline | 5                   | 52.9               | 1.37                    |\n\nWhen we select on only one column, baseline Presto can use its dictionary aware filter and the difference drops to the usual magnitude.\n\n## Try Aria\nThe prototype of Aria is [available](https://github.com/aweisberg/presto/tree/aria-scan-prototype) to experiment with along with [instructions](https://github.com/aweisberg/presto/blob/aria-scan-prototype/BENCHMARK.md) on how to try these queries yourself.\n\nThe ideas presented here are currently being integrated into mainline Presto.\n\n# Conclusions and Next Steps\nWe have so far had a look at the low-hanging fruits for scanning flat tables. These techniques are widely known and their use in Presto are a straightforward way to improve it.\n\nIn the next installment we will look at more complex cases having to do with operating on variously nested lists, structs and maps. After this we will talk about experiences of implementation and considerations of efficient use of Java."
        },
        {
          "id": "/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan",
          "metadata": {
            "permalink": "/blog/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-06-29-everything-you-always-wanted-to-do-in-a-table-scan.md",
            "source": "@site/blog/2019-06-29-everything-you-always-wanted-to-do-in-a-table-scan.md",
            "title": "Everything You Always Wanted To Do in Table Scan",
            "description": "Orri Erling, Maria Basmanova, Ying Su, Timothy Meehan, Elon Azoulay",
            "date": "2019-06-29T00:00:00.000Z",
            "formattedDate": "June 29, 2019",
            "tags": [],
            "readingTime": 13.445,
            "truncated": true,
            "authors": [
              {
                "name": "Orri Erling",
                "url": "https://www.linkedin.com/in/orrierling/"
              }
            ],
            "frontMatter": {
              "title": "Everything You Always Wanted To Do in Table Scan",
              "author": "Orri Erling",
              "authorURL": "https://www.linkedin.com/in/orrierling/",
              "authorFBID": 100026224749124
            },
            "prevItem": {
              "title": "Complete Table Scan: A Quantitative Assessment",
              "permalink": "/blog/2019/07/23/complete-table-scan"
            },
            "nextItem": {
              "title": "Introducing the Presto blog",
              "permalink": "/blog/2019/06/28/introducing-the-presto-blog"
            }
          },
          "content": "Orri Erling, Maria Basmanova, Ying Su, Timothy Meehan, Elon Azoulay\n\nTable scan, on the face of it, sounds trivial and boring. What’s there in just reading a long bunch of records from first to last? Aren’t indexing and other kinds of physical design more interesting?\n\nAs data has gotten bigger, the columnar table scan has only gotten more prominent. The columnar scan is a fairly safe baseline operation: The cost of writing data is low, the cost of reading it is predictable.\n\nAnother factor that makes the table scan the main operation is the omnipresent denormalization in data warehouse. This only goes further as a result of ubiquitous use of lists and maps and other non-first normal form data.\n\nThe aim of this series of articles is to lay out the full theory and practice of table scan with all angles covered. We will see that this is mostly a matter of common sense and systematic application of a few principles: Do not do extra work and do the work that you do always in bulk. Many systems like Google’s BigQuery do some subset of the optimizations outlined here. Doing all of these is however far from universal in the big data world, so there is a point in laying this all out and making a model implementation on top of Presto. We are here talking about the ORC format, but the same things apply equally to Parquet or JSON shredded into columns.\n\n<!--truncate-->\n\nWe divide the presentation into several parts:\n\n* The logical structure of the data and the operations to apply to it. What are the strictly necessary steps for producing a result?\n* What are the performance gains as opposed to a naive implementation?\n* How does the implementation work and what are the difficulties and tradeoffs?\n* Physical aspects of execution. When are we CPU bound and when IO bound? How can we schedule IO and what do we gain from it in a world that is increasingly about disaggregated storage?\n* What can we say about file formats? What are the evolutionary pressures from use cases? What about metadata?\n\nThe ideal table scan can be described in terms of first principles as follows:\n\n* Do not materialize data that is not part of the result\n* Only read the data that is strictly necessary for producing the result\n* Filter early, run the most efficient filters first.\n* Produce output in predictable sized chunks and run in fixed memory.\n\nThis is the logical aspect. The physical aspect is about making the best use of the IO, reading the right block sizes and doing so ahead of demand, so that the data has arrived before it is accessed.\n\nNext, we look at this at the level of principles. In subsequent articles we look at the physical reality and what it takes to implement this.\n\n# Structure of ORC\n\nORC divides data into stripes and row groups, often 10K rows per group but sometimes less, if the rows are wide. The usual case for wide rows comes from maps and lists, which may have up to a few thousand nested values per row. A stripe is a few row groups. The encoding of any particular column is set at the stripe level. For example, a column may be encoded as a dictionary in one stripe and as a sequence of values in another, according to the properties of the data.\n\nColumns consist of streams. A stream corresponds to a contiguous area in an ORC file. Streams consist of compression chunks, which are up to 256K of raw data that may or may not be compressed with a stream compression like gzip, ZSTD, or Snappy. Different streams encode different aspects of a column, for example nullness, lengths, and the data itself.\n\nReading an ORC file involves a reader per column. The readers form a class hierarchy. Some readers in fact encapsulate a tree of readers, as we have structured types like lists, maps and structs.\n\nWe distinguish the following abstract super classes:\n\n* Column reader — Encapsulates common logic for keeping track of nulls and lengths.\n* Null wrapper — Superclass for streams with complex internal structure that applies to non-null values\n* Repeated - A common superclass for lists and maps. This deals with logic related to having multiple nested rows for one top level row\n* Variant — This encapsulates a selection of alternating readers that may be used for different stripes of one column. For example, a string reader is a variant that wraps a direct and a dictionary string reader\n\nThe actual stream readers correspond to SQL data types, like numbers and strings. The three structured types, list, map and struct wrap one or more readers of arbitrary types. A list has one nested reader for the repeated data. A struct has a reader for each member, a map has a nested reader for keys (string or number) and another for values. Combinations like lists inside maps and maps inside a struct inside a list occur frequently.\n\n# Logical Steps in Scanning a Table\n\nA Presto worker receives stripes to scan. It looks at the row group metadata to see if it can exclude row groups based on metadata, in practice min/max values of columns. Since data is seldom sorted or otherwise correlated, skipping whole row groups is rare.\n\nThe Presto table scan then seeks to the beginning of a row group.\n\nThe scan touches a number of columns. For each column, we either evaluate a filter and produce the set of row numbers for which the filter is true, or we retrieve the value at each row in the qualifying set. We may also do both, in which case the filter comes first. If the column has structure, i.e. is a list, map or struct, there may be filters on subfields and only a fraction of the subfields may be referenced by the query. A subfield is a member of a struct, a specific index in a list, a specific key in a map or it may refer to the number of elements in a list or map. More on this later.\n\nAt the beginning, we have a choice of filters. Some filters may be independent of any columns, for example in the case of random sampling. If we have such, these produce the initial set of rows to scan. Otherwise the initial set is a fraction of the row group. The size of the fraction depends on the width of the data and the target batch size. Thus, we have an initial selection of rows to look at. This is called a qualifying set and will get restricted as we evaluate filters.\n\n# Different Kinds of Filters\n\nIn practice, most filters are comparisons between a single column and a literal. The literal may be a single value, a set of ranges or an IN list. Most of the time filters are AND’ed together. Each term of a top level AND is called a top-level conjunct. Most conjuncts are comparisons of a column and literal. These are separated in their own category and can be independently evaluated when decoding the representation of a column. In the Presto context we call these tuple domain filters. The next most common filter is an expression over a column, for example a regexp match of a JSON field extraction over the column. Finally, there are cases of expressions that depend on multiple columns, for example `discount > 0.08 OR comment LIKE ‘%special price%’`. The two operands of OR (disjuncts) could plausibly be evaluated on the column itself but the OR makes this a two-column expression. A more common example of a multicolumn filter is a hash join probe pushed down into a scan with a multi-part key. We will come back to this later.\n\n# Filter and Column Order\n\nSQL does not specify any order for evaluating filters. The rows for which all conjuncts are true are the result of the selection. Most filters are defined for all values in a column; thus, errors are infrequent. However, some filters may produce errors, e.g. division by zero, and in those cases different filter orders may have different error behavior.\n\nPresto in its initial state evaluates complex filters left to right in the order of the original query and signals the leftmost error of the first row that has an error. We change this to reordering filters according to their performance and to signaling the first error on the first row that has no false filters.\n\nHaving defined this, we now have full freedom in rearranging filters so that we get the most selection as early as possible. The cost function of a filter is `time / (rows_in / rows_out)`. The lower the value, the better the filter. Thus, we can sort all filters by this score and since we can read the columns independently and, in any order, we can rearrange the column order according to the filter order. The only constraint is that a multicolumn filter may not be placed earlier than its last operand.\n\nThe theoretically complete solution to this ordering problem would be like query join order selection where we run a cost model on all possible permutations. In practice, something simpler works just as well, in this case placing all the single column filters in order of ascending cost and then placing all multicolumn filters, starting with the cheapest, after the last column this depends on. If a column the filter depends on is not in the set of columns placed so far, it is added. After all filters are placed, we place all the non-filtered columns, widest first.\n\n# Nested Structure: Lists, Maps and Structs\n\nScanning a struct is just like scanning a group of top-level columns. The reason why the columns inside a struct cannot just be treated as regular top-level columns is that they all share a struct-level null indicator. Within the struct, exactly the same filter order choices are possible as between top level columns. Reading a struct thus starts with a qualifying set of row numbers at the level of the struct. This is translated into a set of row numbers at the level of the struct members. This translation is identity if there are no null structs. The inner qualifying set is then restricted by filters over struct columns. Finally, this is converted into a set of qualifying rows at the outer level. If null structs are included in the result, the nulls are added after everything else. This last step happens only if there were no filters other than is null on struct members.\n\nA list or map is like a struct, except that now there may be zero or more nested rows per top level row. Now the inner qualifying set is a two-way mapping from top level row to the set of corresponding nested row numbers and back.\n\nFiltering over lists or maps adds the extra complication of applying a different filter on different positions of one column. Consider the case of `features[1] = 2 AND features[3] = 4`. This is in fact a very common case, especially in machine learning applications where training data is often represented as a map.\n\nThe map reader here reads the keys and filters out the positions where the value is either 1 or 3. From this, the reader knows which value positions should have `= 2` and `= 4` as a filter. The qualifying set for the values column reader is set to these rows and a position dependent filter is created so that we alternately apply one or the other condition. If, for any enclosing row, we had less than 2 filter hits, the row is discarded. If for any enclosing row, we placed less than 2 filters then a key was absent, which is either and error or discards the row. A global configuration controls the error behavior for `[]`. The `element_at(map, key)` form is always null for missing keys.\n\nThere are some more complications depending on whether all or part of the map keys are projected out but the general principle as above. Lists are like maps, except that here we do not need the key column to map a top-level row number and subscript to a position in the value column.\n\nThe Facebook DWRF format, a customized ORC V1, has an additional concept called a flat map. This has one column of values and present flags for each distinct key. When reading a subset of keys, this is much more efficient than the direct map where it is at the very least necessary to skip over values that are not wanted. The reader for this becomes a modified struct reader. This reintroduces the possibilities of filter reordering also for maps.\n\nFinally, there are cases of deeply nested maps, lists and structs. Expressions like `map[1].field.list[0] = 10 AND map[2].field2.list2[1] = 11` involve a tree of position dependent filters. These operations are composable and work as expected by just stacking multiple levels of inner to outer row numbers on top of each other. In all cases we get long tight loops over the leaf columns and need no tuple at a time processing.\n\n# Adaptation\n\nSo far, we have two kinds of adaptation: Filter order and batch size. The point of filter order is obvious: In the good case, it costs nothing and in the bad case it saves the day. It does this without relying on an optimizer cost model, which is an extra plus. Statistics are often absent or wrong and, in any case,, these do not cover data correlations. This is not so much an issue in a DBMS which writes all its data but is more of an issue with a data lake where multiple engines work on the same data and there is no centralized control over metadata.\n\nAdapting the batch size is a compromise between having long loops over consecutive values and maintaining a memory cap on intermediate results. The basic point of columnar data representation is that it is efficient to compress and loop over consecutive values of one column: The same operation applies to all and the values are all of one domain. The longer one stays with one column the less time one spends in interpreting the query, e.g. switching between columns.\n\nBut with data that is usually wide, always denormalized and has unpredictable runs of repeated elements, it is no longer practical to do for example 1K values of a column and then move to the next column.\n\nIn practice, we are always operating under memory pressure and basically all reliability incidents with Presto have to do with running out of memory. For this reason, we must enforce a memory cap in scanning a table.\n\nThis is done by keeping statistics over filter selectivity and column width. The basic formula is to start with the proposed batch size and for each column add the value size times the product of selectivity’s of filters applied before the column. This gives the estimated size of the batch. One needs to apply reasonable safety margins because data is not uniformly distributed. In this way we get an estimate that allows scaling the batch size so that we are likely to fit within budget. If a hard limit is exceeded, we retry the scan with a much smaller batch size.\n\nThis is relatively easy to implement and as long as the retry happens in under 1/1000 of the row groups we are fine.\n\nIn practice, for getting the gains from tight loops over columns, it is enough that we loop over several thousand values of the highest cardinality column. Usually this is the deepest nested column. If there is this pattern of nesting, the number of top-level rows processed in a batch is not very significant.\n\n# Conclusions and Next Steps\n\nWe have described the general operation of scanning columnar data and what choices and optimization possibilities there are. Next, we will look at results and compare with baseline Presto using TPC-H with both flat tables and nested structures."
        },
        {
          "id": "/2019/06/28/introducing-the-presto-blog",
          "metadata": {
            "permalink": "/blog/2019/06/28/introducing-the-presto-blog",
            "editUrl": "https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/blog/2019-06-28-introducing-the-presto-blog.md",
            "source": "@site/blog/2019-06-28-introducing-the-presto-blog.md",
            "title": "Introducing the Presto blog",
            "description": "Presto is a key piece of data infrastructure at many companies. The community has many ongoing projects for taking it to new levels of performance and functionality plus unique experience and insight into challenges of scale.",
            "date": "2019-06-28T00:00:00.000Z",
            "formattedDate": "June 28, 2019",
            "tags": [],
            "readingTime": 1.425,
            "truncated": false,
            "authors": [
              {
                "name": "Orri Erling",
                "url": "https://www.linkedin.com/in/orrierling/"
              }
            ],
            "frontMatter": {
              "title": "Introducing the Presto blog",
              "author": "Orri Erling",
              "authorURL": "https://www.linkedin.com/in/orrierling/",
              "authorFBID": 100026224749124
            },
            "prevItem": {
              "title": "Everything You Always Wanted To Do in Table Scan",
              "permalink": "/blog/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan"
            }
          },
          "content": "Presto is a key piece of data infrastructure at many companies. The community has many ongoing projects for taking it to new levels of performance and functionality plus unique experience and insight into challenges of scale.\n\nWe are opening this blog as an informal channel for discussing our work as well as technology trends and issues that affect the big data and data warehouse world at large. Our development continues to take place at github and can thus be followed by everybody. Here we seek to have a channel that is more concise and interesting to a broader readership than github issues and code comments would be.\n\nWe have current projects like Aria Presto for doubling CPU efficiency and Presto Unlimited for enabling fault tolerant execution of very large queries. We are running one of the world’s largest data warehouses and thus have a unique perspective on platform technologies, e.g. C++ vs. Java, data analytics usage patterns, integration of machine learning and database, data center infrastructure for supporting these and much more.\nSome of the big questions we are facing have to do with optimizing infrastructure at scale and designing the future of interoperable file formats and metadata. Today we are running ORC on Presto and Spark and system specific file formats for diverse online systems. We are constantly navigating the strait between universality and specialization and keep looking for ways to generalize while advancing functionality and performance.\n\nThe Presto user and developer community involves many of the world’s leading technology players. There is exciting work in progress around Presto at many of these companies. We look forward to tracking these too here. Articles from the Presto world are welcome.\nStay tuned for everything Presto."
        }
      ],
      "blogListPaginated": [
        {
          "items": [
            "/2022/04/15/disggregated-coordinator",
            "/2022/03/15/native-delta-lake-connector-for-presto",
            "/2022/01/28/avoid-data-silos-in-presto-in-meta",
            "/2021/11/22/common-sub-expression-optimization",
            "/2021/10/26/Scaling-with-Presto-on-Spark",
            "/2021/06/29/native-parquet-writer-for-presto",
            "/2021/06/14/Commitment-to-Presto-Open-Source-Community",
            "/2021/02/04/raptorx",
            "/2021/01/12/2020-recap-year-with-presto",
            "/2020/12/04/typedset"
          ],
          "metadata": {
            "permalink": "/blog",
            "page": 1,
            "postsPerPage": 10,
            "totalPages": 4,
            "totalCount": 33,
            "nextPage": "/blog/page/2",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2020/11/20/prestocon-and-foundation-update",
            "/2020/10/29/presto-at-drift",
            "/2020/08/20/unnest",
            "/2020/08/14/getting-started-and-aria",
            "/2020/08/06/presto-in-ea",
            "/2020/08/04/prestodb-and-hudi",
            "/2020/07/17/alluxio-hybrid-cloud",
            "/2020/06/30/data-lake-analytics-blog",
            "/2020/06/16/alluxio-datacaching",
            "/2020/05/07/local-spatial-joins"
          ],
          "metadata": {
            "permalink": "/blog/page/2",
            "page": 2,
            "postsPerPage": 10,
            "totalPages": 4,
            "totalCount": 33,
            "previousPage": "/blog",
            "nextPage": "/blog/page/3",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2020/03/18/uber-pinot",
            "/2020/03/02/presto-lambda",
            "/2020/02/13/prestocon-announcement",
            "/2019/12/23/improve-presto-planner",
            "/2019/12/20/repartition",
            "/2019/12/16/growing-the-presto-foundation",
            "/2019/09/26/tablescan-structs",
            "/2019/09/23/linux-foundation",
            "/2019/08/19/memory-tracking",
            "/2019/08/05/presto-unlimited-mpp-database-at-scale"
          ],
          "metadata": {
            "permalink": "/blog/page/3",
            "page": 3,
            "postsPerPage": 10,
            "totalPages": 4,
            "totalCount": 33,
            "previousPage": "/blog/page/2",
            "nextPage": "/blog/page/4",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        },
        {
          "items": [
            "/2019/07/23/complete-table-scan",
            "/2019/06/29/everything-you-always-wanted-to-do-in-a-table-scan",
            "/2019/06/28/introducing-the-presto-blog"
          ],
          "metadata": {
            "permalink": "/blog/page/4",
            "page": 4,
            "postsPerPage": 10,
            "totalPages": 4,
            "totalCount": 33,
            "previousPage": "/blog/page/3",
            "blogDescription": "Blog",
            "blogTitle": "Blog"
          }
        }
      ],
      "blogTags": {},
      "blogTagsListPath": null
    }
  },
  "docusaurus-plugin-content-pages": {
    "default": [
      {
        "type": "jsx",
        "permalink": "/community",
        "source": "@site/src/pages/community.js"
      },
      {
        "type": "jsx",
        "permalink": "/",
        "source": "@site/src/pages/index.js"
      },
      {
        "type": "mdx",
        "permalink": "/markdown-page",
        "source": "@site/src/pages/markdown-page.md",
        "title": "Markdown page example",
        "description": "You don't need React to write simple standalone pages.",
        "frontMatter": {
          "title": "Markdown page example"
        }
      },
      {
        "type": "jsx",
        "permalink": "/test",
        "source": "@site/src/pages/test.js"
      }
    ]
  },
  "docusaurus-plugin-debug": {},
  "docusaurus-theme-classic": {}
}